
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../../../PGM/pyro/Untitled/" rel="prev"/>
<link href="../../chap10/Pasted%20image%2020210519194435.png/" rel="next"/>
<link href="../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.1.7" name="generator"/>
<title>Chap1 - Some random ML notes</title>
<link href="../../../assets/stylesheets/main.ded33207.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.a0c5b2b5.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="cyan" data-md-color-primary="white" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#11-curve-fitting">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Some random ML notes" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="Some random ML notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Some random ML notes
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Chap1
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="cyan" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="white" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="light-blue" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="deep-purple" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Some random ML notes" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="Some random ML notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    Some random ML notes
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../..">
        Nothing special here
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          MML
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          MML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap1/">
        chap 1: Introduction and Motivation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap2/">
        chap2: Linear Algebra
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap3/">
        chap3 Analytic Geometry
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap4/">
        chap 4 - Matrix Decompositions
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap5/">
        chap5 - Vector Calculus
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../MML/chap6%20-%20Probability%20and%20Distributions/">
        Probability and Distributions
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          NLP
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          NLP
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          Lecture
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_1">
<span class="md-nav__icon md-icon"></span>
          Lecture
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_1_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          HMM
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_1_1_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_3_1_1">
<span class="md-nav__icon md-icon"></span>
          HMM
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../NLP/Lecture/HMM/HMM/">
        HMM
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          Speech Synthesis
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_2">
<span class="md-nav__icon md-icon"></span>
          Speech Synthesis
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../NLP/Speech%20Synthesis/Speech%20Papers/">
        Speech Papers
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PGM
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          PGM
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          Course
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_1">
<span class="md-nav__icon md-icon"></span>
          Course
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture01-Introduction/">
        lecture01-Introduction
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture02-MRFrepresentation/">
        lecture02-MRFrepresentation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture03-BNrepresentation/">
        lecture03-BNrepresentation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture04-ExactInference/">
        lecture04-ExactInference
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture05-ParameterEst/">
        lecture05 ParameterEst
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture06-HMMCRF/">
        lecture06 HMMCRF
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture07-VI1/">
        lecture07-VI1
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture08-VI2/">
        lecture08 VI2
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture09-MC/">
        lecture09-MC
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture10-MCMC-opt/">
        lecture10-MCMC-opt
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture11-NN/">
        lecture11-NN
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/course/lecture12-DGM1/">
        lecture12-DGM1
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          Pyro
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_2">
<span class="md-nav__icon md-icon"></span>
          Pyro
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../PGM/pyro/Untitled/">
        An Introduction to Models in Pyro
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PRML
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          PRML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_5_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          Chap1
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_5_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_1">
<span class="md-nav__icon md-icon"></span>
          Chap1
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Chap1
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Chap1
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#11-curve-fitting">
    1.1. Curve Fitting
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#12-probability-theory">
    1.2. Probability Theory
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#121-probability-densities">
    1.2.1 Probability densities
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#122-expectations-and-covariances">
    1.2.2 Expectations and covariances
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#13-model-selection">
    1.3. Model Selection
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#15-decision-theory">
    1.5. Decision Theory
  </a>
<nav aria-label="1.5. Decision Theory" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#153-the-reject-option">
    1.5.3 The reject option
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#154-inference-and-decision">
    1.5.4 Inference and decision
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#155-loss-functions-for-regression">
    1.5.5 Loss functions for regression
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#16-information-theory">
    1.6. Information Theory
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#161-relative-entropy-and-mutual-information">
    1.6.1 Relative entropy and mutual information
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          Chap10
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
          Chap10
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap10/Pasted%20image%2020210519194435.png/">
        Pasted image 20210519194435.png
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap10/chap10/">
        10. Approximate Inference
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Chap2
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
          Chap2
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap2/chap2/">
        Chap2
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
          Chap3
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_4">
<span class="md-nav__icon md-icon"></span>
          Chap3
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap3/chap3/">
        Chap3
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
          Chap8
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_5">
<span class="md-nav__icon md-icon"></span>
          Chap8
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap8/chap8/">
        8. GRAPHICAL MODELS
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
          Chap9
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_6">
<span class="md-nav__icon md-icon"></span>
          Chap9
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../chap9/chap9/">
        9. Mixture Models and EM
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#11-curve-fitting">
    1.1. Curve Fitting
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#12-probability-theory">
    1.2. Probability Theory
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#121-probability-densities">
    1.2.1 Probability densities
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#122-expectations-and-covariances">
    1.2.2 Expectations and covariances
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#13-model-selection">
    1.3. Model Selection
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#15-decision-theory">
    1.5. Decision Theory
  </a>
<nav aria-label="1.5. Decision Theory" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#153-the-reject-option">
    1.5.3 The reject option
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#154-inference-and-decision">
    1.5.4 Inference and decision
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#155-loss-functions-for-regression">
    1.5.5 Loss functions for regression
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#16-information-theory">
    1.6. Information Theory
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#161-relative-entropy-and-mutual-information">
    1.6.1 Relative entropy and mutual information
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1>Chap1</h1>
<h2 id="11-curve-fitting">1.1. Curve Fitting<a class="headerlink" href="#11-curve-fitting" title="Permanent link">¶</a></h2>
<ol>
<li>As we shall see in Chapter 3, the number of parameters is not necessarily the most appropriate measure of model complexity. 参数个数不足以衡量模型的复杂度</li>
<li>We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood (discussed in Section 1.2.5), and that the over-fitting problem can be understood as Section 3.4 a general property of maximum likelihood 最小二乘法可以看作MLE，过拟合是MLE的property</li>
</ol>
<h2 id="12-probability-theory">1.2. Probability Theory<a class="headerlink" href="#12-probability-theory" title="Permanent link">¶</a></h2>
<p><img alt="" src="../Pasted%20image%2020210324204605.png"/>
<strong>Bayes’ theorem</strong>
<img alt="" src="../Pasted%20image%2020210324204747.png"/>
<img alt="" src="../Pasted%20image%2020210324205005.png"/>
We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.</p>
<hr/>
<p>If we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is provided by the probability p(B). We call this the <strong>prior probability</strong> because it is the probability available before we observe the identity of the fruit. </p>
<p>Once we are told that the fruit is an orange, we can then use Bayes’ theorem to compute the probability p(B|F), which we shall call the <strong>posterior probability</strong> because it is the probability obtained after we have observed F.</p>
<hr/>
<p>We note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be <strong>independent</strong>. From the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X.</p>
<h2 id="121-probability-densities">1.2.1 Probability densities<a class="headerlink" href="#121-probability-densities" title="Permanent link">¶</a></h2>
<p>The probability that x will lie in an interval (a, b) is then given by
<img alt="" src="../Pasted%20image%2020210324210333.png"/>
满足
<img alt="" src="../Pasted%20image%2020210324210521.png"/></p>
<hr/>
<p>下面这里提到了一个jacobian factor，大概意思就是：
概率密度函数和普通的函数是不同的。
<strong>在pdf中，如果两个随机变量有非线性的函数关系，这两个函数的pdf不会保持这种关系。</strong>
进而可以推广出结论，pdf的最值与variable的选择有关
<img alt="" src="../Pasted%20image%2020210324214908.png"/></p>
<p><strong>下面是对于“pdf的最值与variable的选择无关”的说明：</strong>
由下面的推导，可知对于普通的非线性函数，最值的非线性关系是保持的：
<img alt="" src="../Pasted%20image%2020210324215859.png"/></p>
<p>但是
由于有这个式子：
<img alt="" src="../Pasted%20image%2020210324220810.png"/>
<img alt="" src="../Pasted%20image%2020210324220710.png"/>
在(4)中，因为有第二项，所以左边等于0时 <span class="arithmatex">\(p_{x}^{\prime} (g(y))\)</span> 不一定等于0，因此两个极值不一定同时达到。
需要注意的是，当 <span class="arithmatex">\(x=g(y)\)</span> 为线性变化时，<span class="arithmatex">\(g^{\prime\prime}(y)=0\)</span> ，因此上面式子里的第二项就没有了，此时关系保持。</p>
<p><img alt="" src="../Pasted%20image%2020210324222232.png"/>
上图中，从<span class="arithmatex">\(x\)</span> 变换到 <span class="arithmatex">\(y\)</span>经历了一个非线性变换。如果不考虑jacobian factor，应该是红线转移到绿线，最值保持函数关系。但是因为有jacobian factor，实际上转移到了紫色的线，最值并不符合函数关系</p>
<hr/>
<p>cumulative distribution function：
<img alt="" src="../Pasted%20image%2020210324222703.png"/></p>
<p>The sum and product rules
<img alt="" src="../Pasted%20image%2020210324222833.png"/></p>
<h2 id="122-expectations-and-covariances">1.2.2 Expectations and covariances<a class="headerlink" href="#122-expectations-and-covariances" title="Permanent link">¶</a></h2>
<p>Expectation of f(x)：
<img alt="" src="../Pasted%20image%2020210324223635.png"/>
<img alt="" src="../Pasted%20image%2020210324223649.png"/>
可以用有限的<span class="arithmatex">\(N\)</span>次sample近似求expectation：
<img alt="" src="../Pasted%20image%2020210324223840.png"/>
We shall make extensive use of this result when we discuss sampling methods in Chapter 11. The approximation in (1.35) becomes <strong>exact</strong> in the limit <span class="arithmatex">\(N\to\infty\)</span>.</p>
<p>用下标表示which variable is being averaged over，
在 <span class="arithmatex">\(\mathbb{E}_{x}f(x,y)\)</span> 中，是对 <span class="arithmatex">\(x\)</span> 取平均， <span class="arithmatex">\(\mathbb{E}_{x}f(x,y)\)</span> will be a function of <span class="arithmatex">\(y\)</span> .</p>
<p><strong><em>Conditional Expectation</em></strong>
<img alt="" src="../Pasted%20image%2020210324225812.png"/></p>
<p>Variance:
<img alt="" src="../Pasted%20image%2020210324230050.png"/>
<img alt="" src="../Pasted%20image%2020210324230211.png"/>
<img alt="" src="../Pasted%20image%2020210324230237.png"/>
<img alt="" src="../Pasted%20image%2020210324230247.png"/>
If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation <span class="arithmatex">\(cov[\mathrm {x}]\equiv cov[\mathrm {x}, \mathrm {x}]\)</span>.
 ## 1.2.3 Bayesian probabilities
 对于不可重复的实验，如冰川会不会融化，我们不能通过频率来描述uncertainty，这时候要通过probability来描述。此时每当我们掌握了一些新的证据，都会对原有的估计加以修正，这就是bayesian的思路。</p>
<p>prior, posterior, likelihood之间的关系，这个应该看了好多遍了：
 <img alt="" src="../Pasted%20image%2020210325150440.png"/>
<img alt="" src="../Pasted%20image%2020210325150459.png"/>
 分母是normalization term，因为如果左右两边同时对<span class="arithmatex">\(\mathrm{w}\)</span>积分：
 <img alt="" src="../Pasted%20image%2020210325150708.png"/></p>
<hr/>
<p>In a frequentist setting, <span class="arithmatex">\(\mathrm{w}\)</span> is considered to be a <strong>fixed</strong> parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets <span class="arithmatex">\(\mathcal{D}\)</span>. 
 By contrast, from the Bayesian viewpoint there is only a single data set <span class="arithmatex">\(\mathcal{D}\)</span> (namely the one that is actually observed), and the <strong>uncertainty in the parameters is expressed through a probability distribution over <span class="arithmatex">\(\mathrm{w}\)</span>.</strong></p>
<hr/>
<p>## 1.2.4 The Gaussian distribution
 <img alt="" src="../Pasted%20image%2020210325151909.png"/>
 The square root of the variance σ, is called the <em>standard deviation</em>
 β = 1/σ2, is called the <em>precision</em>
<img alt="" src="../Pasted%20image%2020210325152145.png"/>
<img alt="" src="../Pasted%20image%2020210325152155.png"/>
<img alt="" src="../Pasted%20image%2020210325152218.png"/></p>
<p><img alt="" src="../Pasted%20image%2020210325152313.png"/></p>
<p>One common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function. 
 This might seem like a strange criterion because, from our foregoing discussion of probability theory, <strong>it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters.</strong></p>
<p>MLE得到的<span class="arithmatex">\(\mu_{ML}\)</span>就是sample mean，<span class="arithmatex">\(\sigma_{ML}\)</span>就是sample variance
 <img alt="" src="../Pasted%20image%2020210325170548.png"/>
<img alt="" src="../Pasted%20image%2020210325170557.png"/>
<img alt="" src="../Pasted%20image%2020210325170606.png"/></p>
<p>maximum likelihood approach systematically underestimates the variance of the distribution.
 <img alt="" src="../Pasted%20image%2020210325171017.png"/></p>
<p>## 1.2.5 Curve fitting re-visited</p>
<p>we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve
 <img alt="" src="../Pasted%20image%2020210325193340.png"/>
 写出likelihood：
 <img alt="" src="../Pasted%20image%2020210325193715.png"/>
 求log，后两项与<span class="arithmatex">\(\mathrm{w}\)</span>无关
 <img alt="" src="../Pasted%20image%2020210325193824.png"/>
 the sum-of-squares error function has arisen as a consequence of <strong>maximizing likelihood under the assumption of a Gaussian noise distribution</strong></p>
<p>同时，对<span class="arithmatex">\(\beta\)</span>求导，可以得出
 <img alt="" src="../Pasted%20image%2020210325194125.png"/>
 此时就可以用这个分布进行预测了
 <img alt="" src="../Pasted%20image%2020210325194319.png"/></p>
<hr/>
<p>introduce a prior distribution over the polynomial coefficients <span class="arithmatex">\(\mathrm{w}\)</span>
<img alt="" src="../Pasted%20image%2020210325200842.png"/>
 此时likelihood：
 <img alt="" src="../Pasted%20image%2020210325203838.png"/>
 可以看到相当于加入了正则项：
 <img alt="" src="../Pasted%20image%2020210325203913.png"/>
 This technique is called <strong>maximum posterior</strong>, or simply <strong>MAP</strong>.</p>
<p>## 1.2.6 Bayesian curve fitting
 虽然前面求出了<span class="arithmatex">\(\mathrm{w}\)</span>的后验，但这不能算是完成的bayesian treatment。we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of <span class="arithmatex">\(\mathrm{w}\)</span>。</p>
<p>We therefore wish to evaluate the predictive distribution <span class="arithmatex">\(p(t|x, \mathbf{x}, \mathbf{t})\)</span>(这里设<span class="arithmatex">\(\alpha\)</span>和<span class="arithmatex">\(\beta\)</span>已知)
<img alt="" src="../Pasted%20image%2020210325222016.png"/>
其中
<img alt="" src="../Pasted%20image%2020210325222034.png"/>
Here p(w|x, t) is the posterior distribution，通过<span class="arithmatex">\(\frac{prior\times likelihood}{normalization\space term}\)</span>得到，section3.3中可知，对于curve fitting，posterior也是一个gaussian</p>
<p>此外更进一步。预测结果也是一个gaussian：
<img alt="" src="../Pasted%20image%2020210325223259.png"/>
The first term in (1.71) represents the uncertainty in the predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through <span class="arithmatex">\({β^{-1}_{ML}}\)</span>. However, the second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment.</p>
<h2 id="13-model-selection">1.3. Model Selection<a class="headerlink" href="#13-model-selection" title="Permanent link">¶</a></h2>
<p>Akaike information criterion, or AIC chooses the model for which the quantity <img alt="" src="../Pasted%20image%2020210325224119.png"/>
is largest. Here p(D|wML) is the best-fit log likelihood, and <strong>M is the number of adjustable parameters in the model.</strong></p>
<p>section 4.4.1中要讲到 Bayesian information criterion, or BIC
Such criteria do not take account of the uncertainty in the model parameters, 
however, and in practice they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully Bayesian approach where we shall see how complexity penalties arise in a natural and principled way.</p>
<h2 id="15-decision-theory">1.5. Decision Theory<a class="headerlink" href="#15-decision-theory" title="Permanent link">¶</a></h2>
<p>作用：Here we turn to a discussion of decision theory that, <strong>when combined with probability theory, allows us to make optimal decisions in situations involving uncertainty</strong> such as those encountered in pattern recognition.</p>
<p>Determination of p(x, t) from a set of training data is an example of <strong>inference</strong> and is typically a very difficult problem whose solution forms the subject of much of this book</p>
<p><img alt="" src="../Pasted%20image%2020210326004340.png"/></p>
<h3 id="153-the-reject-option">1.5.3 The reject option<a class="headerlink" href="#153-the-reject-option" title="Permanent link">¶</a></h3>
<p>In some applications, it will be appropriate to avoid making decisions on the difficult cases in anticipation of a lower error rate on those examples for which a classification decision is made. This is known as the <strong>reject option</strong>.</p>
<h3 id="154-inference-and-decision">1.5.4 Inference and decision<a class="headerlink" href="#154-inference-and-decision" title="Permanent link">¶</a></h3>
<p>We have broken the classification problem down into two separate stages</p>
<p><strong>inference stage</strong> in which we use training data to learn a model for p(Ck|x)
<strong>decision stage</strong> in which we use these posterior probabilities to make optimal class assignments</p>
<p>如果直接learn a function，把输入映射到决策中，就是discriminant function</p>
<p>下面这里讲了三种模型：generative, discrimitive和直接映射到0和1
<img alt="" src="../Pasted%20image%2020210326011503.png"/></p>
<h3 id="155-loss-functions-for-regression">1.5.5 Loss functions for regression<a class="headerlink" href="#155-loss-functions-for-regression" title="Permanent link">¶</a></h3>
<p>之前说的都是分类问题，对于回归问题，我们想要minimize的loss的期望可以表示为：
<img alt="" src="../Pasted%20image%2020210326081315.png"/>
比如说square loss：
<img alt="" src="../Pasted%20image%2020210326081806.png"/>
下面这一段是对上面这个式子求导，要用到variational calculus（对于函数求导），需要看一下appendix D
<img alt="" src="../Pasted%20image%2020210326082026.png"/></p>
<p>which is the conditional average of t conditioned on x and is known as the <strong>regression function</strong></p>
<p>可以看到regression function <span class="arithmatex">\(\mathbb{E}_{t}[t|x]\)</span>，可以minimize square loss的期望，是由<span class="arithmatex">\(p(t|x)\)</span>的均值得到的</p>
<p>因为<span class="arithmatex">\(\mathbb{E}_{t}[t|x]\)</span>是最优解， <span class="arithmatex">\({y(x) − \mathbb{E}[t|x]}\)</span>的期望是0，因此交叉项消失
<img alt="" src="../Pasted%20image%2020210326084639.png"/>
而由(1.90)的前半部分也可以得出之前的结论，即最优解是由conditional mean <span class="arithmatex">\(\mathbb{E}_{t}[t|x]\)</span>得出的</p>
<p>第二项则是target的variance，因此预测出的target的不同可以看作噪声。而因为这项与y(x)无关，因此这个方差是去不掉的</p>
<hr/>
<p>类比分类问题，回归问题也可以分为三类：
<img alt="" src="../Pasted%20image%2020210326085713.png"/>
<img alt="" src="../Pasted%20image%2020210326085725.png"/></p>
<h3 id="16-information-theory">1.6. Information Theory<a class="headerlink" href="#16-information-theory" title="Permanent link">¶</a></h3>
<p>引出<span class="arithmatex">\(h(x)\)</span>:
我们用<span class="arithmatex">\(h(x)\)</span>描述degree of surprise，显然<span class="arithmatex">\(h(x)\)</span>与<span class="arithmatex">\(p(x)\)</span>相关
当x和y独立时，我们观察x的surprise+观察y的surprise应该等于同时观察x和y的surprise，即<span class="arithmatex">\(h(x,y)=h(x)+h(y)\)</span>
而又有<span class="arithmatex">\(p(x,y)=p(x)\cdotp(y)\)</span>
因此可以定义information
<img alt="" src="../Pasted%20image%2020210326092618.png"/></p>
<p>传输一个变量x的<span class="arithmatex">\(h(x)\)</span>的期望，就是x的entropy
<img alt="" src="../Pasted%20image%2020210326093022.png"/></p>
<p>之后的讨论中，entropy的底数为e</p>
<hr/>
<p>multiplicity：
把N个相同的物体分到n个箱子中的分法：
首先选第一个物体有N种选法，第二个物体有N-1种选法。一共有<span class="arithmatex">\(N!\)</span>种选法
而n个箱子内部本身是无序的，因此最终结果为：
<img alt="" src="../Pasted%20image%2020210326103126.png"/>
which is called the <strong>multiplicity</strong></p>
<p>而entropy则是 <strong>logarithm of the multiplicity scaled by an appropriate constant</strong>
<img alt="" src="../Pasted%20image%2020210326103244.png"/></p>
<hr/>
<p>离散值的熵：
<img alt="" src="../Pasted%20image%2020210326110557.png"/>
对于连续值，differential entropy：
<img alt="" src="../Pasted%20image%2020210326110634.png"/></p>
<p>对于离散变量，用拉格朗日maximize extropy得到uniform distribution</p>
<p>对于连续变量，用拉格朗日maximize extropy得到Gaussian distribution</p>
<hr/>
<p>conditional entropy：
如果对于变量x和y，我们先观察到了y，那么观察x得到的信息量为−ln p(y|x)，x此时的条件熵为：
<img alt="" src="../Pasted%20image%2020210326110806.png"/></p>
<p>条件熵满足
<img alt="" src="../Pasted%20image%2020210326110946.png"/>
where <span class="arithmatex">\(H[x, y]\)</span> is the differential entropy of <span class="arithmatex">\(p(x, y)\)</span> and <span class="arithmatex">\(H[x]\)</span> is the differential entropy of the marginal distribution <span class="arithmatex">\(p(x)\)</span></p>
<h3 id="161-relative-entropy-and-mutual-information">1.6.1 Relative entropy and mutual information<a class="headerlink" href="#161-relative-entropy-and-mutual-information" title="Permanent link">¶</a></h3>
<p>Consider some unknown distribution <span class="arithmatex">\(p(x)\)</span>, and suppose that we have modelled this using an approximating distribution <span class="arithmatex">\(q(x)\)</span>. 
If we use <span class="arithmatex">\(q(x)\)</span> to construct a coding scheme for the purpose of transmitting values of <span class="arithmatex">\(x\)</span> to a receiver, then the average <strong>additional</strong> amount of information (in nats) required to specify the value of <span class="arithmatex">\(x\)</span> (assuming we choose an efficient coding scheme) as a result of using <span class="arithmatex">\(q(x\)</span>) instead of the true distribution <span class="arithmatex">\(p(x)\)</span> is given by
<img alt="" src="../Pasted%20image%2020210326111431.png"/>
This is known as the relative entropy or Kullback-Leibler divergence,or KL divergence
KL(p||q) &gt;=0 with equality if, and only if, p(x)= q(x).</p>
<hr/>
<p>后面要用到jensen不等式，因此先说明凸函数的定义：
<img alt="" src="../Pasted%20image%2020210326115503.png"/>
This is equivalent to the requirement that the second derivative of the function be everywhere positive</p>
<p><img alt="" src="../Pasted%20image%2020210326120018.png"/></p>
<p>we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x).</p>
<p>KL divergence实际上求不出来，因为我们不知道真正的p(x).
但是我们知道由p(x)产生的N个x
因此p(x)可以用<span class="arithmatex">\(p(\mathrm{x})\)</span>近似</p>
<p><img alt="" src="All%20About%20Data%20Science/PRML/chap1/Pasted%20image%2020210329005907.png"/></p>
<p><img alt="" src="All%20About%20Data%20Science/PRML/chap1/Pasted%20image%2020210329005953.png"/>
Thus we see that <strong>minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function</strong></p>
<hr/>
<p>Mutual information</p>
<p>对于joint distribution中的一组变量x和y，如果它们不独立，就不能表示为p(x, y)= p(x)p(y)
但是，我们可以用KL divergence衡量它们之间“有多么不独立”</p>
<p><img alt="" src="All%20About%20Data%20Science/PRML/chap1/Pasted%20image%2020210329010426.png"/>
I(x, y) &gt;= 0 with equality if, and only if, x and y are independent</p>
<p>mutual information is related to the conditional entropy through
<img alt="" src="All%20About%20Data%20Science/PRML/chap1/Pasted%20image%2020210329010614.png"/>
推导：
<img alt="" src="All%20About%20Data%20Science/PRML/chap1/Pasted%20image%2020210329010626.png"/></p>
<p>Thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y (or vice versa). 
我们可以把mutual information看作是观察到y之后，对于x的uncertainty减少了多少（通过entropy和conditional entropy来衡量）
或者我们可以把p(x)看作先验，p(x|y)看作是后验。mutual information表示观测到y之后the reduction in uncertainty about x</p>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../../assets/javascripts/bundle.51198bba.min.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</body>
</html>