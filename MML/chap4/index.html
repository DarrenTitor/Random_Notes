
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../chap3/" rel="prev"/>
<link href="../chap5/" rel="next"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.1.7" name="generator"/>
<title>chap 4 - Matrix Decompositions - Some random ML notes</title>
<link href="../../assets/stylesheets/main.ded33207.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.a0c5b2b5.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="cyan" data-md-color-primary="white" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#chap-4-matrix-decompositions">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Some random ML notes" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Some random ML notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Some random ML notes
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              chap 4 - Matrix Decompositions
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="cyan" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="white" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="light-blue" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="deep-purple" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Some random ML notes" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Some random ML notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    Some random ML notes
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Nothing special here
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          MML
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          MML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../chap1/">
        chap 1: Introduction and Motivation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chap2/">
        chap2: Linear Algebra
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chap3/">
        chap3 Analytic Geometry
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          chap 4 - Matrix Decompositions
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        chap 4 - Matrix Decompositions
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#41-determinant-and-trace">
    4.1 Determinant and Trace
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#42-eigenvalues-and-eigenvectors">
    4.2 Eigenvalues and Eigenvectors
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#43-cholesky-decomposition">
    4.3 Cholesky Decomposition
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#44-eigendecomposition-and-diagonalization">
    4.4 Eigendecomposition and Diagonalization
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#45-singular-value-decomposition">
    4.5 Singular Value Decomposition
  </a>
<nav aria-label="4.5 Singular Value Decomposition" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#451-geometric-intuitions-for-the-svd">
    4.5.1 Geometric Intuitions for the SVD
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#452-construction-of-the-svd">
    4.5.2 Construction of the SVD
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#453-eigenvalue-decomposition-vs-singular-value-decomposition">
    4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#46-matrix-approximation">
    4.6 Matrix Approximation
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chap5/">
        chap5 - Vector Calculus
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chap6%20-%20Probability%20and%20Distributions/">
        Probability and Distributions
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          NLP
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          NLP
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          Lecture
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_1">
<span class="md-nav__icon md-icon"></span>
          Lecture
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_1_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          HMM
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_1_1_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_3_1_1">
<span class="md-nav__icon md-icon"></span>
          HMM
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../NLP/Lecture/HMM/HMM/">
        HMM
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          Speech Synthesis
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_2">
<span class="md-nav__icon md-icon"></span>
          Speech Synthesis
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../NLP/Speech%20Synthesis/Speech%20Papers/">
        Speech Papers
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PGM
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          PGM
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          Course
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_1">
<span class="md-nav__icon md-icon"></span>
          Course
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture01-Introduction/">
        lecture01-Introduction
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture02-MRFrepresentation/">
        lecture02-MRFrepresentation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture03-BNrepresentation/">
        lecture03-BNrepresentation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture04-ExactInference/">
        lecture04-ExactInference
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture05-ParameterEst/">
        lecture05 ParameterEst
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture06-HMMCRF/">
        lecture06 HMMCRF
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture07-VI1/">
        lecture07-VI1
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture08-VI2/">
        lecture08 VI2
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture09-MC/">
        lecture09-MC
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture10-MCMC-opt/">
        lecture10-MCMC-opt
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture11-NN/">
        lecture11-NN
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/course/lecture12-DGM1/">
        lecture12-DGM1
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          Pyro
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_2">
<span class="md-nav__icon md-icon"></span>
          Pyro
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PGM/pyro/Untitled/">
        An Introduction to Models in Pyro
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PRML
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          PRML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          Chap1
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_1">
<span class="md-nav__icon md-icon"></span>
          Chap1
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap1/chap1/">
        Chap1
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          Chap10
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
          Chap10
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap10/Pasted%20image%2020210519194435.png/">
        Pasted image 20210519194435.png
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap10/chap10/">
        10. Approximate Inference
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Chap2
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
          Chap2
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap2/chap2/">
        Chap2
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
          Chap3
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_4">
<span class="md-nav__icon md-icon"></span>
          Chap3
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap3/chap3/">
        Chap3
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
          Chap8
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_5">
<span class="md-nav__icon md-icon"></span>
          Chap8
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap8/chap8/">
        8. GRAPHICAL MODELS
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
          Chap9
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_6">
<span class="md-nav__icon md-icon"></span>
          Chap9
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../PRML/chap9/chap9/">
        9. Mixture Models and EM
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#41-determinant-and-trace">
    4.1 Determinant and Trace
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#42-eigenvalues-and-eigenvectors">
    4.2 Eigenvalues and Eigenvectors
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#43-cholesky-decomposition">
    4.3 Cholesky Decomposition
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#44-eigendecomposition-and-diagonalization">
    4.4 Eigendecomposition and Diagonalization
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#45-singular-value-decomposition">
    4.5 Singular Value Decomposition
  </a>
<nav aria-label="4.5 Singular Value Decomposition" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#451-geometric-intuitions-for-the-svd">
    4.5.1 Geometric Intuitions for the SVD
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#452-construction-of-the-svd">
    4.5.2 Construction of the SVD
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#453-eigenvalue-decomposition-vs-singular-value-decomposition">
    4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#46-matrix-approximation">
    4.6 Matrix Approximation
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="chap-4-matrix-decompositions">chap 4 - Matrix Decompositions<a class="headerlink" href="#chap-4-matrix-decompositions" title="Permanent link">¶</a></h1>
<p><img alt="" src="../Pasted%20image%2020221015101710.png"/></p>
<h2 id="41-determinant-and-trace">4.1 Determinant and Trace<a class="headerlink" href="#41-determinant-and-trace" title="Permanent link">¶</a></h2>
<p>Determinants are only defined for square matrices.
we write the determinant as det(A) or sometimes as |A|.</p>
<p><code>n*n</code>方阵A的determinant是将A映射到实数的一个函数。</p>
<p>矩阵可逆&lt;=&gt;determinant不为0.
<img alt="" src="../Pasted%20image%2020221015102243.png"/>
determinant一旦为0，说明经过线性变换之后，原本线性独立的basis变得线性相关了，以至于determinant所代表的signed volume变为了0。换句话说经过线性变换之后，dimension减小了。
一个向量空间降维之后，自然就没办法还原会原来的空间，因此determinant为0的时候矩阵不可逆。</p>
<p>下图中是n=1,2,3时，determinant的closed form，比较常用。
<img alt="" src="../Pasted%20image%2020221015102400.png"/>
<img alt="" src="../Pasted%20image%2020221015102818.png"/></p>
<p>upper-triangular矩阵：
对角线下边全为0
lower-triangular矩阵：
对角线上边全为0
注意对角线不一定为0.
<img alt="" src="../Pasted%20image%2020221015112157.png"/></p>
<p><img alt="" src="../Pasted%20image%2020221015112558.png"/></p>
<p><img alt="" src="../Pasted%20image%2020221015112645.png"/></p>
<p>determinant的计算：
Laplace expansion
<img alt="" src="../Pasted%20image%2020221015112949.png"/>
这个式子很难看懂，所以直接看下面的例子：
<img alt="" src="../Pasted%20image%2020221015113023.png"/>
<img alt="" src="../Pasted%20image%2020221015113032.png"/></p>
<p>一些性质：
<img alt="" src="../Pasted%20image%2020221015123106.png"/></p>
<p>第二章提到：silimar matrix本质上描述的是同一个线性变换，只不过使用的是不同的basis，因此determinant相同。
<img alt="" src="../Pasted%20image%2020221015123229.png"/></p>
<p><img alt="" src="../Pasted%20image%2020221015123305.png"/></p>
<p>由于上面最后三条性质，我们也可以用高斯消元法求determinant。
先将矩阵变成row-echelon form，当矩阵变成triangular form时，再利用对角线乘积。
<img alt="" src="../Pasted%20image%2020221015123535.png"/></p>
<p>下面这三个说的是一回事，都是在讲线性变换之后向量空间的dimension没有降低。
<span class="arithmatex">\(det(A)\neq 0\iff rk(A)=n \iff A\,is \,invertible\)</span>.
<img alt="" src="../Pasted%20image%2020221015131939.png"/></p>
<p>trace：
trace=方阵的对角线元素之和。
<img alt="" src="../Pasted%20image%2020221015132837.png"/>
<img alt="" src="../Pasted%20image%2020221015132843.png"/>
trace的性质：
<img alt="" src="../Pasted%20image%2020221015132944.png"/></p>
<p>上面的最后一天我可以推广到多个矩阵连乘的trace：
只要将矩阵的顺序进行cyclic permutations，trace都保持不变。
<img alt="" src="../Pasted%20image%2020221015133238.png"/>
由此可以得到以下的结论：
x和y都是n维向量，<span class="arithmatex">\(xy^T\)</span>本身是一个<code>n*n</code>方阵，但这个方阵的trace可以转化为求<span class="arithmatex">\(tr(x^Ty)\)</span>，等于x和y的dot product <span class="arithmatex">\(x^Ty\)</span>。
<img alt="" src="../Pasted%20image%2020221015133354.png"/></p>
<p>接下来讨论basis的选择会不会影响trace的值：
chap2.7.2 提到，对线性变换A进行basis变换，可以得到<span class="arithmatex">\(S^{-1}AS\)</span> ，然后就有
<img alt="" src="../Pasted%20image%2020221015134455.png"/>
说明basis的选择不会影响trace。</p>
<p>借助determinant和trace，接下来我们要将一个矩阵表示为一个多项式，这个多项式之后会经常用到
Taking together our understanding of determinants and traces we can now define an important equation describing a matrix A in terms of a polynomial,
Characteristic Polynomial
<img alt="" src="../Pasted%20image%2020221015140207.png"/>
其中
<img alt="" src="../Pasted%20image%2020221015140225.png"/></p>
<p>这式子完全不知道咋来的。</p>
<h2 id="42-eigenvalues-and-eigenvectors">4.2 Eigenvalues and Eigenvectors<a class="headerlink" href="#42-eigenvalues-and-eigenvectors" title="Permanent link">¶</a></h2>
<p><img alt="" src="../Pasted%20image%2020221015140426.png"/></p>
<p>As we will see, the eigenvalues of a linear mapping will tell us how a special set of vectors, the eigenvectors, is transformed by the linear mapping.</p>
<p>定义：
<img alt="" src="../Pasted%20image%2020221015165427.png"/></p>
<p>在很多地方，都是默认eigenvalue时降序排列的，但本书没有这个假设。
<img alt="" src="../Pasted%20image%2020221015165501.png"/></p>
<p>下面这些结论都是等价的：
<img alt="" src="../Pasted%20image%2020221015165656.png"/></p>
<p>Collinearity and Codirection
两向量方向相同称为codirection，
两向量方向相同或相反称为collinear。
<img alt="" src="../Pasted%20image%2020221015165732.png"/></p>
<p>假如x时A的eigenvector，则与x collinear的向量都是A的eigenvector。
<img alt="" src="../Pasted%20image%2020221015170018.png"/></p>
<p><span class="arithmatex">\(\lambda\)</span>是eigenvalue&lt;=&gt;<span class="arithmatex">\(\lambda\)</span>是characteristic polynomial的一个root。（不知道有啥用）
<img alt="" src="../Pasted%20image%2020221015170055.png"/>
=&gt;
上面这个式子characteristic polynomial 是用来具体计算eigenvalue的。
我们不需要记住那一长串，只需要记住<span class="arithmatex">\(p_A(\lambda)=det(A-\lambda I)\)</span>.
然后因为这个式子的root是eigenvalue，所以我们令<span class="arithmatex">\(det(A-\lambda I)=0\)</span>，然后就能解出所有的<span class="arithmatex">\(\lambda\)</span>.</p>
<p>然后这里定义了一个algebraic multiplicity，也不知道是干嘛的。
<img alt="" src="../Pasted%20image%2020221015170355.png"/></p>
<p>一个eigenvalue对应很多个eigenvector，我们将某一个eigenvalue <span class="arithmatex">\(\lambda\)</span>所对应的所有eigenvector的span称为eigenspace of A with respect to λ 。记作<span class="arithmatex">\(E_\lambda\)</span>.
A的所有eigenvalue（注意不是eigenvector）构成的集合称为eigenspectrum或者spectrum。
<img alt="" src="../Pasted%20image%2020221015170904.png"/></p>
<p>如果<span class="arithmatex">\(\lambda\)</span>是A的一个eigenvalue，则<span class="arithmatex">\(E_\lambda\)</span>是homogeneous system of linear equations <span class="arithmatex">\((A-\lambda I)x=0\)</span>的solution space。
<img alt="" src="../Pasted%20image%2020221015171043.png"/></p>
<p>几何意义上，如果一个eigenvector对应的是一个非零的eigenvalue，
说明eigenvector指向的这个方向是被线性变换“stretch”了的，而“stretch”的程度则由eigenvalue来描述。
<img alt="" src="../Pasted%20image%2020221015171459.png"/>
我的理解：
观察<span class="arithmatex">\(Ax=\lambda x\)</span>，在线性变换之后，x并没有被旋转，而是和原来的x成collinear关系。
也就是说eigenvector代表着线性变换在这个方向只有伸缩，没有旋转。</p>
<p><img alt="" src="../Pasted%20image%2020221015172129.png"/></p>
<p>一些常用性质：
- transpose之后eigenvalue不变，但eigenvector可能改变。
- eigenspace <span class="arithmatex">\(E_\lambda\)</span> is the null space of <span class="arithmatex">\(A-\lambda I\)</span>。
<img alt="" src="../Pasted%20image%2020221015172327.png"/></p>
<ul>
<li>similar matrices的eigenvalue相同。
因为similar matrices本身描述的是同一个线性变换，而eigenvalues记录的是只受到“stretch”的方向被“stretch”的程度，而这个“程度”与basis选择无关。所以eigenvalue相同。
由此我们得到了三个不随basis change而改变的量：
eigenvalues、determinant和trace。用它们来描述线性变换本身的性质。
<img alt="" src="../Pasted%20image%2020221015172659.png"/></li>
<li>Symmetric, positive definite matrices的eigenvalue一定大于0
<img alt="" src="../Pasted%20image%2020221015174559.png"/>
<img alt="" src="../Pasted%20image%2020221015175023.png"/></li>
</ul>
<p>具体计算eigenvalues，eigenvectors的过程：
<img alt="" src="../Pasted%20image%2020221015181131.png"/>
<img alt="" src="../Pasted%20image%2020221015181200.png"/>
<img alt="" src="../Pasted%20image%2020221015181212.png"/></p>
<p>某一个eigenvalue对应的linear independent eigenvector的个数，就是这个eigenvalue的geometric multiplicity。
也就是说geometric multiplicity就是这个eigenvalue对应的eigenspace的dimensionality。
<img alt="" src="../Pasted%20image%2020221015185837.png"/></p>
<p>geometric multiplicity不会超过algebraic multiplicity。
An eigenvalue’s geometric multiplicity cannot exceed its algebraic multiplicity, but it may be lower.
<img alt="" src="../Pasted%20image%2020221015185728.png"/></p>
<p>注意：对于代表旋转操作的线性变换，因为没有哪一个方向是只经过“stretch”而没有经过旋转的，因此这样的矩阵只有complex eigenvalue，而没有实数eigenvalue。
<img alt="" src="../Pasted%20image%2020221015190608.png"/></p>
<p>如果一个矩阵的eigenvalues各不相同，则其对应的eigenvectors线性独立。
也就是说这些eigenvector可以视作basis。
<img alt="" src="../Pasted%20image%2020221015191112.png"/></p>
<p>Defective matrix:
这个概念还挺重要。
简单来说就是它线性独立的eigenvector个数小于n，或者说它的dimension of eigenspaces之和小于n。
<img alt="" src="../Pasted%20image%2020221015192421.png"/>
non-defective matrix不一定有n distinct eigenvalues。
（也就是说
<span class="arithmatex">\(n\,distinct\,eigenvalues \Rightarrow n\, linear\, independent\, eigenvectors\)</span>.
<span class="arithmatex">\(n\, linear\, independent\, eigenvectors \nRightarrow n\,distinct\,eigenvalues\)</span>.
）
但defective matrix一定没有n distinct eigenvalues。</p>
<hr/>
<p>任意矩阵的<span class="arithmatex">\(A^TA\)</span>一定是symmetric, positive semidefinite的。
当rk(A)=n时，<span class="arithmatex">\(A^TA\)</span>一定是symmetric, positive definite的。（positive definite的部分之前证明过了）
<img alt="" src="../Pasted%20image%2020221015194354.png"/>
证明：
<img alt="" src="../Pasted%20image%2020221015194537.png"/></p>
<hr/>
<p>Spectral Theorem
对于symmetric矩阵A，一定存在一组orthonormal 的eigenvectors作为basis，而且所有eigenvalue都为实数。
<img alt="" src="../Pasted%20image%2020221015194913.png"/></p>
<p>换句话说，
<strong>symmetric矩阵的eigendecomposition一定存在。</strong> 我们可以将A分解为<span class="arithmatex">\(A=PDP^T\)</span>, where D is diagonal and the columns of P contain the eigenvectors.
<img alt="" src="../Pasted%20image%2020221015231333.png"/>
我的理解：
<span class="arithmatex">\(A=PDP^T\)</span>这个式子其实就是<a href="All%20About%20Data%20Science/MML/chap2.md#^dbbab1">chap2矩阵相似</a>提到的basis变换<span class="arithmatex">\(\hat{A}=S^{-1}AS\)</span>，这里要求S regular。
P的columns是orthonormal的eigenvectors，因此P是orthogonal matrix，P regular，因为P的column已经orthogonal了就一定linear independent。此外因为P是orthogonal的，有<span class="arithmatex">\(P^T=P^{-1}\)</span>.
这时对比<span class="arithmatex">\(A=PDP^T\)</span>和<span class="arithmatex">\(\hat{A}=S^{-1}AS\)</span>，可以发现就是一回事。
P（eigenvectors）描述的其实相当于是一个basis change，而A和D是similar的，本质上描述的是同一个线性变换。</p>
<hr/>
<p>下面讨论一下eigenvalue, trace, determinant这三者之间的关系。
<img alt="" src="../Pasted%20image%2020221015234848.png"/>
determinant不随basis变换而改变，A和D描述的又是同一个线性变换，那么det(A)=det(D)。
而D是diagonal矩阵，算出来的det(D)就等于上面这个式子。</p>
<p><img alt="" src="../Pasted%20image%2020221015235143.png"/>
trace也不随basis变换而改变，那么tr(A)=tr(D)。而tr(D)对角线上的元素就是eigenvalue，因此得到上面这个式子。</p>
<h2 id="43-cholesky-decomposition">4.3 Cholesky Decomposition<a class="headerlink" href="#43-cholesky-decomposition" title="Permanent link">¶</a></h2>
<p>Cholesky Decomposition可以类比于实数的开根号运算。</p>
<p>注意这里要求是正定矩阵，而不是半正定矩阵。
对称的正定矩阵可以分解为<span class="arithmatex">\(A=LL^T\)</span>，其中L是一个lower triangular矩阵。
而且L唯一确定。
<img alt="" src="../Pasted%20image%2020221022223721.png"/></p>
<p>具体计算：
写出对应关系解方程就好，不怎么重要。
<img alt="" src="../Pasted%20image%2020221022224001.png"/>
<img alt="" src="../Pasted%20image%2020221022224049.png"/></p>
<p>Cholesky decomposition在很多出现了symmetric positive definite matrices的场景都有应用。
- the covariance matrix of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive definite. The Cholesky factorization of this covariance matrix allows us to generate samples from a Gaussian distribution
- It also allows us to perform a linear transformation of random variables, which is heavily exploited when computing gradients in deep stochastic models, such as the variational auto-encoder
- The Cholesky decomposition also allows us to compute determinants very efficiently.
- 
Cholesky decomposition可以用于求determinant。
<img alt="" src="../Pasted%20image%2020221022224403.png"/></p>
<h2 id="44-eigendecomposition-and-diagonalization">4.4 Eigendecomposition and Diagonalization<a class="headerlink" href="#44-eigendecomposition-and-diagonalization" title="Permanent link">¶</a></h2>
<p>diagonal matrix</p>
<p><img alt="" src="../Pasted%20image%2020221022224518.png"/></p>
<p>determinant：product of its diagonal entries
a matrix power <span class="arithmatex">\(D^k\)</span>：each diagonal element raised to the power k
the inverse <span class="arithmatex">\(D^{-1}\)</span>：对角线元素取倒数
<img alt="" src="../Pasted%20image%2020221022224557.png"/></p>
<p>矩阵的Diagonalization是basis change we discussed in Section 2.7.2 and eigenvalues from Section 4.2的综合应用。</p>
<p>之前提到过如果有<span class="arithmatex">\(D=P^{-1}AP\)</span>，则A与D similar。
现在我们只关注D是diagonal matrix、而且对角线上的元素都是A的eigenvalue的情况。</p>
<p><img alt="" src="../Pasted%20image%2020221022230646.png"/></p>
<p>下面来证明diagonalization本质就是basis change，且选择的basis是A的eigenvectors。
<img alt="" src="../Pasted%20image%2020221022225711.png"/></p>
<p>上面对于diagonalization的定义，并没有提到eigenvalue和eigenvector。下面来证明<strong>diagonalization带来的必然是eigenvalue和eigenvector</strong>。
<img alt="" src="../Pasted%20image%2020221022230136.png"/>
将<span class="arithmatex">\(AP=PD\)</span>展开，然后令等式两边对应相等，正好能得到很多组<span class="arithmatex">\(Ap_i=\lambda_i p_i\)</span>，因此解得的向量是eigenvector，解得的<span class="arithmatex">\(\lambda\)</span>是eigenvalue。
<img alt="" src="../Pasted%20image%2020221022230454.png"/>
这里我们要求P is invertible，也就是P is full rank，也就是说这些eigenvectors <span class="arithmatex">\(p_i\)</span>线性独立，是一组basis。</p>
<p>那什么样的A才是diagonalizable的呢？
根据下面的定理，A is diagonalizable当且仅当A的eigenvectors线性独立。
<img alt="" src="../Pasted%20image%2020221022231426.png"/></p>
<p>eigenvectors线性独立也就是说A是non-defective的，也就是说只有non-defective才能被diagonalize。
<img alt="" src="../Pasted%20image%2020221022231610.png"/></p>
<p>而前面提到，根据spectum theorem，symmetric矩阵的eigenvector一定orthonormal。因此<strong>symmetric矩阵一定能被diagonalized.</strong>
<img alt="" src="../Pasted%20image%2020221022231903.png"/></p>
<p>Diagonalization的几何意义：
A和D描述的是同一个线性变换，
<span class="arithmatex">\(e_i\)</span>和<span class="arithmatex">\(p_i\)</span>是两组不同的basis，其中<span class="arithmatex">\(p_i\)</span>和D搭配使用时，可以看到经过D变换之后，沿着<span class="arithmatex">\(p_i\)</span>的方向都是只有被伸缩，而没有旋转。
<span class="arithmatex">\(P\)</span>和<span class="arithmatex">\(P^{-1}\)</span>都只是basis change，只起到旋转的作用。
<img alt="" src="../Pasted%20image%2020221022232352.png"/></p>
<p>Eigendecomposition的具体计算：
<img alt="" src="../Pasted%20image%2020221022233202.png"/>
<img alt="" src="../Pasted%20image%2020221022233224.png"/></p>
<p>一些性质：
- 经过eigendecomposition，可以简化矩阵幂运算的计算量。<img alt="" src="../Pasted%20image%2020221022233508.png"/>
- 简化determinant的计算量
<img alt="" src="../Pasted%20image%2020221022233526.png"/>
<img alt="" src="../Pasted%20image%2020221022233548.png"/>
（P是basis change，代表旋转，因此线性变换先后signed volume不变，determinant为1）.</p>
<h2 id="45-singular-value-decomposition">4.5 Singular Value Decomposition<a class="headerlink" href="#45-singular-value-decomposition" title="Permanent link">¶</a></h2>
<p>SVD很通用，因为任何时候都存在。
SVD has been referred to as the “fundamental theorem of linear algebra” (Strang, 1993) because it can be applied to all matrices, not only to square matrices, and it always exists.
而且SVD可以用来quantifies the change between the underlying geometry of these two vector spaces。
<img alt="" src="../Pasted%20image%2020221022233902.png"/></p>
<p>SVD Theorem
<span class="arithmatex">\(A=U\Sigma V^T\)</span>，
其中U是<code>m*m</code>orthogonal 方阵，V是<code>n*n</code>orthogonal 方阵。
<span class="arithmatex">\(\Sigma\)</span>是<code>m*n</code>diagonal矩阵且对角线元素非负（尽管不一定是方阵）。</p>
<p><img alt="" src="../Pasted%20image%2020221022234127.png"/>
其中<span class="arithmatex">\(\Sigma\)</span>的对角线元素被称为singular values，
<span class="arithmatex">\(u_i\)</span>称为left-singular vectors，
<span class="arithmatex">\(v_j\)</span>称为right-singular vectors。
<img alt="" src="../Pasted%20image%2020221022234538.png"/>
singular matrix <span class="arithmatex">\(\Sigma\)</span>唯一确定。
<img alt="" src="../Pasted%20image%2020221022234758.png"/>
注意<span class="arithmatex">\(\Sigma\)</span>是<code>m*n</code>的，与A的shape相同。
由于不一定是方阵，<span class="arithmatex">\(\Sigma\)</span>可能长这样：
<img alt="" src="../Pasted%20image%2020221022234922.png"/>
或者是这样：
<img alt="" src="../Pasted%20image%2020221022234946.png"/></p>
<p>所有矩阵都存在SVD。
<img alt="" src="../Pasted%20image%2020221022234955.png"/></p>
<h3 id="451-geometric-intuitions-for-the-svd">4.5.1 Geometric Intuitions for the SVD<a class="headerlink" href="#451-geometric-intuitions-for-the-svd" title="Permanent link">¶</a></h3>
<p>we will discuss the SVD as sequential linear transformations performed on the bases.</p>
<p>SVD的思路跟eigendecomposition很类似。
<img alt="" src="../Pasted%20image%2020221023000304.png"/>
- basis change via <span class="arithmatex">\(V^T\)</span>.
- scaling and augmentation (or reduction) in dimensionality via the singular value matrix <span class="arithmatex">\(\Sigma\)</span>.
- a second basis change via <span class="arithmatex">\(U\)</span>.</p>
<p><img alt="" src="../Pasted%20image%2020221023001531.png"/>
<img alt="" src="../Pasted%20image%2020221023001409.png"/></p>
<p>SVD在V和W都有basis change，并且本身V和W就是两个不同的vector space。
相比之下，eigendicomposition是在相同的向量空间V中，并且使用使用的是同一组basis。
我的理解：
参照<a href="All%20About%20Data%20Science/MML/chap2.md#^dbbab1">矩阵等价&amp;矩阵相似</a>这里，可以看到eigendicomposition对应的是矩阵相似，A与D相似，并且在同一个向量空间、线性变换前后使用的是同一组basis。
而SVD更像是equivalent的条件设置。
<img alt="" src="../Pasted%20image%2020221023001659.png"/>
<img alt="" src="../Pasted%20image%2020221023003217.png"/></p>
<h3 id="452-construction-of-the-svd">4.5.2 Construction of the SVD<a class="headerlink" href="#452-construction-of-the-svd" title="Permanent link">¶</a></h3>
<p>We will next discuss why the SVD exists and show how to compute it in detail.</p>
<p>对比eigendecompositionh和SVD的式子，
<img alt="" src="../Pasted%20image%2020221023003359.png"/>
<img alt="" src="../Pasted%20image%2020221023003406.png"/>
如果我们令
<img alt="" src="../Pasted%20image%2020221023003443.png"/>
则SVD就是eigendecomposition。</p>
<p>接下来说明SVD为什么存在。</p>
<p>思路：
<img alt="" src="../Pasted%20image%2020221023004611.png"/></p>
<p>step 1：
已知symmetric矩阵一定可以diagonalize，
又知任意矩阵A的<span class="arithmatex">\(A^TA\)</span>一定symmetric positive semidefinite。
所以<span class="arithmatex">\(A^TA\)</span>一定可以diagonalize。
得到：
<img alt="" src="../Pasted%20image%2020221023005107.png"/></p>
<p>假设A的SVD存在，<span class="arithmatex">\(A=U\Sigma V^T\)</span>，
则
<img alt="" src="../Pasted%20image%2020221023005334.png"/>
对比上面两个式子，
<img alt="" src="../Pasted%20image%2020221023005350.png"/></p>
<p><img alt="" src="../Pasted%20image%2020221023005450.png"/></p>
<p>可以得到：
对<span class="arithmatex">\(A^TA\)</span>进行eigendecomposition，
得到的eigenvector就是A的right-singular vectors V，
得到的eigenvalue就是A的singular value<span class="arithmatex">\(\Sigma\)</span>的平方。</p>
<p>step2：
同理，对<span class="arithmatex">\(AA^T\)</span>进行相同的操作，可以得到
（注意这里<span class="arithmatex">\(A^TA\)</span>是<code>n*n</code>的，<span class="arithmatex">\(AA^T\)</span>是<code>m*m</code>的，）
<img alt="" src="../Pasted%20image%2020221023005749.png"/>
对<span class="arithmatex">\(AA^T\)</span>进行eigendecomposition，
得到的eigenvector就是A的left-singular vectors V，
得到的eigenvalue就是A的singular value<span class="arithmatex">\(\Sigma\)</span>的平方。</p>
<p>step3：
<img alt="" src="../Pasted%20image%2020221023010138.png"/>
上面两步中用到的<span class="arithmatex">\(\Sigma\)</span>一定是相同的。
（这一步没看懂有啥用）</p>
<p>step4：
（这步中间有些细节不懂，就这样吧。）
现在我们借助对<span class="arithmatex">\(A^TA\)</span>和<span class="arithmatex">\(AA^T\)</span>的eigendecomposition，得到了A的<span class="arithmatex">\(U\)</span>、<span class="arithmatex">\(V^T\)</span>和<span class="arithmatex">\(\Sigma\)</span>。如果只是只是为了求A的SVD，那我们现在已经算完了。
但我们现在另外再来探索一下U和V的关系。</p>
<p>首先我们观察到<span class="arithmatex">\(Av_j\)</span>本身就是orthogonal的。
<img alt="" src="../Pasted%20image%2020221023011909.png"/></p>
<p>但我们想让U是orthonormal的，因此就对<span class="arithmatex">\(Av_j\)</span>除以其norm得到单位向量：
<img alt="" src="../Pasted%20image%2020221023012132.png"/>
至此得到u和Av的数值关系：
<img alt="" src="../Pasted%20image%2020221023012243.png"/></p>
<p>也就是
<img alt="" src="../Pasted%20image%2020221023012342.png"/>
这个式子很像eigenvalue equation，但是注意等式两边的的向量是不同的。</p>
<p>注意上面式子中<span class="arithmatex">\(v_i\)</span>是n维。<span class="arithmatex">\(u_i\)</span>是m维。如果只看上面这个式子的话，感觉<span class="arithmatex">\(v_i\)</span>和<span class="arithmatex">\(u_i\)</span>中维度更高的那一方，只有一部分值受到了约束。
比如n=6， m=4，向量<span class="arithmatex">\(v_i\)</span>比<span class="arithmatex">\(u_i\)</span>长，上面的式子只展示了向量<span class="arithmatex">\(v_i\)</span>的前4维等于啥，并没说后2维需要满足什么条件。但经过之前的SVD推导过程，我们知道<span class="arithmatex">\(v_i\)</span>本身还需要是orthonormal的。</p>
<p><img alt="" src="../Pasted%20image%2020221023013317.png"/></p>
<h3 id="453-eigenvalue-decomposition-vs-singular-value-decomposition">4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition<a class="headerlink" href="#453-eigenvalue-decomposition-vs-singular-value-decomposition" title="Permanent link">¶</a></h3>
<p>Eigenvalue Decomposition和SVD的对比：</p>
<ul>
<li>SVD始终存在。eigendecomposition要求矩阵是方阵，且eigenvectors是n维空间的basis。</li>
<li>eigendecomposition的eigenvector不一定orthogonal。SVD的U和V中的向量一定是orthonormal的，因此描述的一定是旋转操作。</li>
<li>SVD的U和V之间一般没有inverse关系（而且可能连shape都不一样）。eigendecomposition的<span class="arithmatex">\(p\)</span>和<span class="arithmatex">\(P^{-1}\)</span>成inverse关系。</li>
<li>SVD的<span class="arithmatex">\(\Sigma\)</span>一定是实数且非负。eigendecomposition中的eigenvalue不一定。</li>
<li>对于symmetric矩阵，eigendecomposition和SVD相同。
<img alt="" src="../Pasted%20image%2020221023013912.png"/>
<img alt="" src="../Pasted%20image%2020221023013920.png"/></li>
</ul>
<p>SVD相关的一些terminology and conventions：</p>
<p><img alt="" src="../Pasted%20image%2020221023015014.png"/>
SVD的另一种定义方法：
<span class="arithmatex">\(\Sigma\)</span>是方阵，U和V是矩形。
<img alt="" src="../Pasted%20image%2020221023015107.png"/>
<img alt="" src="../Pasted%20image%2020221023015224.png"/></p>
<p><img alt="" src="../Pasted%20image%2020221023015332.png"/></p>
<h2 id="46-matrix-approximation">4.6 Matrix Approximation<a class="headerlink" href="#46-matrix-approximation" title="Permanent link">¶</a></h2>
<p>这部分等哪天有心情了再看，先看chap5</p>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../assets/javascripts/bundle.51198bba.min.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</body>
</html>