{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nothing special here","text":""},{"location":"MML/chap1/","title":"chap 1: Introduction and Motivation","text":"<p>The book is split into two parts, where Part I lays the mathematical foundations and Part II applies the concepts from Part I to a set of fundamental machine learning problems, which form four pillars of machine learning as illustrated in Figure 1.1:  regression, dimensionality reduction, density estimation, and classification.</p>"},{"location":"MML/chap2/","title":"chap2: Linear Algebra","text":"<p>\u9664\u4e86\u901a\u5e38\u610f\u4e49\u4e0a\u7684\u51e0\u4f55\u610f\u4e49\u4e0a\u7684vector\uff0c\u591a\u9879\u5f0f\u4e5f\u53ef\u4ee5\u770b\u4f5c\u662fvector\uff1a\u4e24\u4e2a\u591a\u9879\u5f0f\u76f8\u52a0\u53ef\u4ee5\u5f97\u5230\u591a\u9879\u5f0f\u3001\u591a\u9879\u5f0f\u4e0e\u6807\u91cf\u76f8\u4e58\u53ef\u4ee5\u5f97\u5230\u591a\u9879\u5f0f\u3002\u56e0\u4e3a\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u6761\u4ef6\uff0c\u56e0\u6b64\u53ef\u4ee5\u89c6\u4f5cvector\u3002  \u6211\u4eec\u66f4\u5173\u5fc3\u7684\u5176\u5b9e\u662fn\u7ef4\u5b9e\u6570\u7a7a\u95f4\u4e0a\u7684vector\uff0c\u6bd4\u51e0\u4f55\u610f\u4e49\u4e0a\u7684vector\u66f4\u62bd\u8c61\u4e00\u70b9\uff1a </p> <p>In this book, we will focus on finitedimensional vector spaces, in which case there is a 1:1 correspondence between any kind of vector and \\(\\mathbb{R}^n\\).</p> <p>\u5728\u6570\u5b66\u4e2d\u6211\u4eec\u5173\u5fc3closure\u8fd9\u4e2a\u6982\u5ff5\uff0cWhat is the set of all things that can result from my proposed operations?  In the case of vectors: What is the set of vectors that can result by starting with a small set of vectors, and adding them to each other and scaling them?  \u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5f97\u5230vector space\u3002</p> <p></p>"},{"location":"MML/chap2/#21-systems-of-linear-equations","title":"2.1 Systems of Linear Equations","text":"<p>\u901a\u5e38\uff0c\u6211\u4eec\u5c06  \u5199\u4f5c  \u8fd9\u6837\u7684\uff0c\u77e9\u9635\u76f8\u4e58\u7684\u5f62\u5f0f\u3002\u7136\u540e\u6c42\u89e3\u5411\u91cfX\u3002</p> <p>In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions.</p>"},{"location":"MML/chap2/#22-matrices","title":"2.2 Matrices","text":"<p>\u77e9\u9635\u7684element-wise product\u79f0\u4e3ahadamard product\u3002 </p>"},{"location":"MML/chap2/#222-inverse-and-transpose","title":"2.2.2 Inverse and Transpose","text":"<p>Unfortunately, not every matrix A possesses an inverse A\u22121. If this inverse does exist, A is called regular/invertible/nonsingular, otherwise singular/noninvertible. </p> <p>\u5f53determinant=0\u65f6\uff0c\u77e9\u9635\u4e0d\u53ef\u9006\u3002</p> <p>\u4e00\u4e9b\u91cd\u8981\u6027\u8d28\uff1a  </p> <p>\u5bf9\u79f0\u77e9\u9635\u7684sum\u4e00\u5b9a\u662f\u5bf9\u79f0\u7684\uff0c\u4f46product\u4e0d\u4e00\u5b9a\u5bf9\u79f0\u3002 </p>"},{"location":"MML/chap2/#223-multiplication-by-a-scalar","title":"2.2.3 Multiplication by a Scalar","text":""},{"location":"MML/chap2/#224-compact-representations-of-systems-of-linear-equations","title":"2.2.4 Compact Representations of Systems of Linear Equations","text":"<p>\u7ebf\u6027\u65b9\u7a0b\u7ec4  \u53ef\u4ee5\u5199\u4f5c\uff1a</p> <p> \u6ce8\u610f\u6bcf\u4e2ax\u6240\u5bf9\u5e94\u7684\u662fcolumn\uff0c\u4e0d\u662frow\u3002 \u7edf\u79f0\u4e3a\\(Ax=b\\)\u3002 \\(Ax\\) is a (linear) combination of the columns of \\(A\\).</p>"},{"location":"MML/chap2/#23-solving-systems-of-linear-equations","title":"2.3 Solving Systems of Linear Equations","text":""},{"location":"MML/chap2/#231-particular-and-general-solution","title":"2.3.1 Particular and General Solution","text":""},{"location":"MML/chap2/#232-elementary-transformations","title":"2.3.2 Elementary Transformations","text":"<p>elementary transformations\u5728\u4e0d\u6539\u53d8solution\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u5bf9\u65b9\u7a0b\u7ec4\u8fdb\u884c\u7b80\u5316\u3002</p> <p></p> <p> row-echelon form\u7684\u8981\u6c42\uff1a - \u51680\u7684row\u653e\u5230\u6700\u4e0b\u9762 - \u9636\u68af\u72b6\u77e9\u9635\uff0c\u6bcf\u4e00\u884c\u4ece\u5de6\u8d77\u7684\u7b2c\u4e00\u4e2a\u975e0\u5143\u7d20\u5fc5\u987b\u4e25\u683c\u5728\u4e0a\u4e00\u884c\u7b2c\u4e00\u4e2a\u975e0\u5143\u7d20\u7684\u53f3\u8fb9\u3002 -   \u9996\u5148\u628a\u65b9\u7a0b\u7ec4\u5199\u6210augmented matrix\u3002 \u7136\u540e\u8f6c\u6362\u6210row-echelon form  </p> <p> </p> <p>\u501f\u52a9row-echelon form\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u627e\u5230\u7279\u89e3\u3002 \u56e0\u4e3a\u4e0a\u9762\u4f8b\u5b50\u4e2d\u7684pivot\u4e3ax1, x3, x4\uff0c \u56e0\u6b64\u4ee4augmented matrix\u4e2d\u7684\u8fd9\u4e09\u5217\u7684\u7ebf\u6027\u7ec4\u5408\u7b49\u4e8e\u5411\u91cfb\u3002</p> <p> \u7136\u540e\u628afree variable\u8bbe\u4e3a0\uff0c\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u7279\u89e3\u3002 </p> <p>Reduced Row Echelon Form\uff1a  Reduced Row Echelon Form\u7684\u8981\u6c42\uff1a - \u51680\u7684row\u653e\u5230\u6700\u4e0b\u9762 - \u4e25\u683c\u5448\u9636\u68af\u72b6 - pivot\u5168\u4e3a1 - \u6bcf\u4e00\u5217\u9664\u4e86pivot\uff0c\u5176\u4f59\u5168\u4e3a0</p> <p>reduced row-echelon form\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u627e\u901a\u89e3\u3002 </p> <p>Gaussian elimination\u5c31\u662f\u4e00\u4e2a\u5c06augmented matrix\u8f6c\u5316\u4e3areduced row-echelon form\u7684\u7b97\u6cd5\u3002 </p> <p> </p>"},{"location":"MML/chap2/#233-the-minus-1-trick","title":"2.3.3 The Minus-1 Trick","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u53ef\u4ee5\u5feb\u901f\u6c42\u51faAx=0\u7684\u65b9\u6cd5\u3002\uff08\u6c42\u51fa\u7684\u7ed3\u679c\u8bb0\u5f97\u8981\u518d\u52a0\u4e0a\u7279\u89e3\uff0c\u624d\u662f\u6700\u7ec8\u7684\u901a\u89e3\u3002\uff09</p> <p>\u9002\u7528\u4e8ereduced row-echelon form\u6ca1\u6709\u51680row\u7684\u60c5\u51b5\uff0c A\u4e3a\u8f6c\u6362\u540e\u5f97\u5230\u7684reduced row-echelon form\uff0c\u901a\u5e38col\u6570\u5927\u4e8erow\u6570\uff0c \u8fd9\u65f6\u6211\u4eec\u5728A\u7684\u5404\u5904\u589e\u52a0\u82e5\u5e72\u884c\uff0c\u5c06A\u8865\u5168\u6210\u4e00\u4e2a\u65b9\u9635\u3002\uff08\u4e0d\u4e00\u5b9a\u52a0\u5728A\u7684\u5e95\u90e8\uff0c\u53ef\u4ee5\u63d2\u5165\u5230A\u7684\u4e2d\u95f4\uff09  \u4f7f\u5f97\u65b9\u9635\u7684\u5bf9\u89d2\u7ebfcontains eather 1 or -1. </p> <p>  \u6b64\u65f6\uff0cpivot -1\u6240\u5728\u7684columns\u5c31\u662fAx=0\u7684solution\u3002 \u5b9e\u9645\u4e0a\u8fd9\u4e9bcolumn\u7ec4\u6210\u4e86Ax=0\u7684\u89e3\u7a7a\u95f4\u7684basis\uff0c\u79f0\u4e3akernel\u6216\u8005null space\u3002 \u4f8b\u5982\uff1a </p> <p>\u77e9\u9635\u6c42\u9006\u7684\u7b97\u6cd5\uff1a \u4e4b\u524d\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\u6211\u4eec\u662f\u6c42\u89e3\\(AX=b\\)\u4e2d\u7684X\uff0c \u73b0\u5728\u5982\u679c\u60f3\u7b97A\u7684\u9006\uff0c\u5176\u5b9e\u5c31\u662f\u6c42\u89e3\\(AX=I_n\\). \u8fd9\u6837\u4e00\u6765\u6211\u4eec\u53ea\u9700\u8981\u5c06\\([A|I_n]\\)\u89c6\u4f5caugmented matrix\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3areduced row-echelon form\u3002  \u8fd9\u6837\u4e00\u6765\uff0caugmented matrix\u53f3\u8fb9\u7684\u90e8\u5206\u5c31\u662f\u6211\u4eec\u60f3\u8981\u7684X\uff0c\u4e5f\u5c31\u662f\\(A^{-1}\\)\u3002 </p>"},{"location":"MML/chap2/#234-algorithms-for-solving-a-system-of-linear-equations","title":"2.3.4 Algorithms for Solving a System of Linear Equations","text":"<p>\u5728\u4e4b\u524d\u7684\u8ba8\u8bba\u4e2d\uff0c\u6211\u4eec\u90fd\u5047\u8bbe\\(Ax=b\\)\u6709\u89e3\u3002\u5f53\u65b9\u7a0b\u7ec4\u65e0\u89e3\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ea\u80fd\u6c42\u5176\u8fd1\u4f3c\u89e3\uff0c\u5176\u4e2d\u4e00\u79cd\u65b9\u5f0f\u5c31\u662flinear regression\u3002</p> <p>\u5728\u67d0\u4e9b\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u6211\u4eec\u80fd\u627e\u5230\\(A^{-1}\\)\uff0c\u5c31\u80fd\u76f4\u63a5\u627e\u5230\\(Ax=b\\)\u7684\u89e3\uff0c\\(x=A^{-1}b\\)\u3002 However, this is only possible if A is a square matrix and invertible, which is often not the case.</p> <p>\u5728\u66f4\u591a\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u4e00\u822c\u5f15\u5165\u5047\u8bbe\uff1a A has linearly independent columns\uff0c\u6216\u8005rank of A is identical to its column rank\uff0c\u6216\u8005A\u7684\u884c\u5217\u5f0f\u4e0d\u7b49\u4e8e0\uff0c</p> <p>\u5219\u6b64\u65f6\\(A^T\\cdot A\\)\u53ef\u9006</p> <p> \u8fd9\u6837\u5f97\u5230\u7684\\(x=(A^TA)^{-1}A^T\\)\u79f0\u4e3aMoore-Penrose pseudo-inverse\uff0cwhich also corresponds to the minimum norm least-squares solution. \uff08\u8fd9\u4e5f\u662f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u89e3\uff09\u3002</p>"},{"location":"MML/chap2/#rkaniff-ata-is-invertible","title":"\u8bc1\u660e\\(rk(A)=n\\iff A^TA \\, is \\, invertible\\).","text":"<p>^401219</p> <p>\u8bc1\u660e\\(rk(A)=n\\iff A^TA \\, is \\, invertible\\). \uff08\u4e0b\u9762\u7684\u8bc1\u660e\u9700\u8981\u7528\u5230chap2.7.3\u4e2d\u7684\u5b9a\u7406\uff09  \u4e0a\u9762\u8bc1\u660e\u4e86\\(rk(A)=rk(A^TA)\\)\uff0c \u63a5\u4e0b\u6765\u53ea\u9700\u8bc1\u660e\\(rk(A^TA)=n\\iff A^TA \\, is \\, invertible\\)\u3002 \u4e5f\u5c31\u662f\u53ea\u9700\u8bc1\u660e\u4e3a\u4ec0\u4e48 \u77e9\u9635full rank &lt;=&gt;\u77e9\u9635\u53ef\u9006\u3002 \u8fd9\u662fchap2.6.2\u4e2d\u7684\u4e00\u4e2a\u63a8\u8bba\u3002 \u6240\u4ee5\u8bc1\u660e\u7ed3\u675f\u3002</p> <p>\u53e6\u5916\u7528\u4e0a\u56fe\u4e2d\u7684\u601d\u8def\u5176\u5b9e\u53ef\u4ee5\u8bc1\u660e\uff1a \\(rk(A)=rk(A^T)=rk(A^TA)=rk(AA^T)\\). \u8fd9\u662f\u4e00\u4e2a\u633a\u5e38\u7528\u7684\u7ed3\u8bba\u3002 \u5c31\u4e0d\u8bc1\u660e\u4e86\uff0c\u4e86\u89e3\u601d\u8def\u5c31\u884c\u3002</p> <p>disadvantage: - \u8ba1\u7b97\u91cf\u5927 - for reasons of numerical precision it is generally not recommended to compute the inverse or pseudo-inverse</p> <p>\u5176\u4ed6\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u65b9\u6cd5\uff1a - \u9ad8\u65af\u6d88\u5143\u6cd5\u5c3d\u7ba1\u5f88\u91cd\u8981\uff0c\u4f46\u5bf9\u4e8e\u53d8\u91cf\u4e2a\u6570\u5de8\u5927\u7684\u573a\u666f\u4e0d\u9002\u7528\u3002 - In practice, systems of many linear equations are solved indirectly</p>"},{"location":"MML/chap2/#24-vector-spaces","title":"2.4 Vector Spaces","text":"<p>\u8fd9\u4e00\u90e8\u5206\uff0c\u5c06\u501f\u52a9group\u6765\u6b63\u5f0f\u5b9a\u4e49vector\u3002</p>"},{"location":"MML/chap2/#241-groups","title":"2.4.1 Groups","text":"<p> group\u9700\u8981\u6ee1\u8db3\u7684\u6761\u4ef6\uff1a - \u662foperation\u7684closure - operation\u6ee1\u8db3\u7ed3\u5408\u5f8b - \u6709\u5355\u4f4d\u5143 - \u6709\u9006\u5143</p> <p>\u5982\u679cgroup\u53e6\u5916\u8fd8\u6ee1\u8db3\u4ea4\u6362\u5f8b\uff0c\u5219\u4e3aAbelian group\u3002 </p> <p></p>"},{"location":"MML/chap2/#242-vector-spaces","title":"2.4.2 Vector Spaces","text":"<p>\u7528group\u6765\u5b9a\u4e49vector space\uff1a </p> <p> \u6ce8\u610f\uff1a \u5411\u91cf\u3001\u77e9\u9635\u548c\u865a\u6570\u7684\u96c6\u5408\u90fd\u662fvector space\uff0c\u56e0\u4e3a\u6ee1\u8db3\u4e0a\u9762\u7684\u6027\u8d28\u3002</p>"},{"location":"MML/chap2/#243-vector-subspaces","title":"2.4.3 Vector Subspaces","text":"<p>Intuitively, they are sets contained in the original vector space with the property that when we perform vector space operations on elements within this subspace, we will never leave it. In this sense, they are \u201cclosed\u201d.</p> <p> \u901a\u5e38\u6211\u4eec\u9700\u8981\u5224\u65ad\uff1a - \u96f6\u5143\u5728\u5b50\u7a7a\u95f4\u5185 - \u6240\u6709\u8fd0\u7b97\u90fd\u5728\u5b50\u7a7a\u95f4\u5185\u6ee1\u8db3\u95ed\u5305</p> <p>\u4e00\u4e9b\u7ed3\u8bba\uff1a  </p>"},{"location":"MML/chap2/#25-linear-independence","title":"2.5 Linear Independence","text":"<p>\u73b0\u5728\u6709finite\u4e2avector\uff0c\u5219\u5b83\u4eec\u7684\u7ebf\u6027\u7ec4\u5408\u5b9a\u4e49\u4e3a\uff1a \u5411\u91cf0\u6c38\u8fdc\u53ef\u4ee5\u5199\u6210\u4e00\u7ec4vector\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u4f46\u8fd9\u662ftrivial\u7ebf\u6027\u7ec4\u5408\u3002 \u901a\u5e38\u6211\u4eec\u66f4\u5173\u5fc3\u7ebf\u6027\u7ec4\u5408\u7cfb\u6570\u4e0d\u5168\u4e3a0\u7684\u60c5\u51b5\u3002 </p> <p>\u7ebf\u6027\u72ec\u7acb\u3001\u7ebf\u6027\u76f8\u5173\u662f\u501f\u52a9\\(0=\\sum^k_{i=1}\\lambda_ix_i\\)\u6765\u5b9a\u4e49\u7684\u3002 \u5f53\u8fd9\u4e2a\u5f0f\u5b50\u53ea\u6709trivial\u89e3\uff0c\u6b64\u65f6\u8fd9\u4e9b\u5411\u91cf\u7ebf\u6027\u72ec\u7acb\u3002 \u5982\u679c\u5b58\u5728non-trivial\u89e3\uff0c\u5219\u8fd9\u4e9b\u5411\u91cf\u7ebf\u6027\u76f8\u5173\u3002 </p> <p>\u5f53set\u4e2d\u67090\u5411\u91cf\u3001\u6709\u4e24\u4e2a\u76f8\u7b49\u5411\u91cf\u3001\u67d0\u4e00\u5411\u91cf\u53ef\u4ee5\u7528\u5176\u4ed6\u5411\u91cf\u8868\u793a\u65f6\uff0c\u6574\u4e2aset\u7ebf\u6027\u76f8\u5173\u3002 </p> <p>\u5b9e\u9645\u5224\u65ad\u7ebf\u6027\u76f8\u5173\u901a\u5e38\u4f7f\u7528Gaussian Elimination\uff1a \u9996\u5148\u6211\u4eec\u5f97\u5230row echelon form (the reduced row-echelon form is unnecessary here): \u6240\u6709pivot column\u90fd\u4e0e\u5176\u5de6\u8fb9\u7684column\u7ebf\u6027\u72ec\u7acb\u3002 \u6240\u6709non-pivot column\u90fd\u53ef\u4ee5\u8868\u793a\u4e3a\u5176\u5de6\u8fb9\u7684pivot column\u7684\u7ebf\u6027\u7ec4\u5408\u3002 </p> <p>All column vectors are linearly independent if and only if all columns are pivot columns.  \u6ce8\u610f\u8fd9\u91cc\u8bf4\u7684\u662f\u6240\u6709column\u90fd\u662fpivot column\uff0c\u6211\u4eec\u4e0d\u5173\u5fc3row\u3002  \u6bd4\u5982\u8bf4\u6211\u4eec\u5f97\u5230\u4e86\u8fd9\u4e2arow echelon form\uff0c\u5c31\u80fd\u5f97\u51fa\u7ed3\u8bba\u8fd93\u4e2a\u5411\u91cf\u7ebf\u6027\u72ec\u7acb\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u73b0\u5728\u6709k\u4e2a\u7ebf\u6027\u72ec\u7acb\u7684\u5411\u91cf\\(b_i\\)\uff0c\u7528\u8fd9\u4e9b\u5411\u91cf\u7ec4\u5408\u51fa\u4e86m\u4e2a\u65b0\u7684\u5411\u91cf\uff08\u7ebf\u6027\u7ec4\u5408\uff09\\(x_i\\)\uff0c\u5219\u6211\u53ef\u4ee5\u628a\u6574\u4e2a\u8fc7\u7a0b\u7b80\u5199\u4e3a\u77e9\u9635\u4e58\u6cd5\u7684\u5f62\u5f0f\uff1a </p> <p>\u7136\u540e\u5c31\u5f15\u51fa\u4e00\u4e2a\u95ee\u9898\uff0c\u5f97\u5230\u7684\u8fd9\u4e00\u7ec4\u65b0\u5411\u91cf\\(x_i\\)\u662f\u4e0d\u662f\u4e5f\u662f\u7ebf\u6027\u72ec\u7acb\u7684\uff1f \u8fd8\u662f\u6cbf\u7528\u81f3\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u4ee4x\u7684\u7ebf\u6027\u7ec4\u5408\u7b49\u4e8e\u96f6\u5411\u91cf\uff0c  \u7136\u540e\u770b\u6709\u6ca1\u6709non-trivial\u89e3\u3002  \u7136\u540e\u53d1\u73b0\uff0c\u5f53\u4e14\u4ec5\u5f53\u5bf9b\u8fdb\u884c\u7ebf\u6027\u7ec4\u5408\u7684\u7cfb\u6570\u5411\u91cf\\(\\{\\lambda_1,\\lambda_2,...,\\lambda_m\\}\\)\u7ebf\u6027\u72ec\u7acb\u65f6\uff0c\u5f97\u5230\u7684\u8fd9\u4e00\u7ec4x\u624d\u7ebf\u6027\u72ec\u7acb\u3002</p> <p>\u5982\u679c\u7528k\u4e2a\u7ebf\u6027\u72ec\u7acb\u5411\u91cf\u8868\u793am&gt;k\u4e2a\u65b0\u5411\u91cf\uff0c\u5219\u65b0\u5411\u91cf\u6709\u5197\u4f59\uff0c\u5fc5\u5b9a\u7ebf\u6027\u76f8\u5173\u3002 </p> <p>\u4f8b\u5b50\uff1a \u6ce8\u610f\u4e0b\u9762\u7684\\(b_i\\)\u90fd\u662f\u5411\u91cf\uff0c\u800c\u975e\u6807\u91cf\u3002  </p>"},{"location":"MML/chap2/#26-basis-and-rank","title":"2.6 Basis and Rank","text":"<p>\u5982\u679c\u4e00\u4e2avector space\u4e2d\u7684\u6240\u6709\u5411\u91cf\u90fd\u53ef\u4ee5\u7528\u96c6\u5408A\u4e2d\u7684\u5411\u91cf\u7ebf\u6027\u7ec4\u5408\u5f97\u5230\uff0c\u79f0A\u4e3aV\u7684generating set\uff0cV\u4e3aA\u7684span\u3002 </p> <p>\u7ebf\u6027\u72ec\u7acb\u7684generating set\u6700\u5c0f\uff0c\u79f0\u4e3a\u8fd9\u4e2a\u5411\u91cf\u7a7a\u95f4V\u7684\u4e00\u4e2abasis\u3002 </p> <p>\u6ce8\u610f\uff1a\u4e00\u7ec4\u7ebf\u6027\u72ec\u7acb\u7684\u5411\u91cf\u4e0d\u4e00\u5b9a\u662fbasis\u3002 </p> <p>\u5bf9\u4e8efinite-dimensional vector spaces V\uff0cV\u7684\u7ef4\u5ea6\u5c31\u7b49\u4e8eV\u7684basis vector\u7684\u4e2a\u6570\u3002  \uff08\u6ce8\u610f\uff1avector space\u7684\u7ef4\u5ea6\u662f\u901a\u8fc7basis\u5b9a\u4e49\u7684\u3002\uff09 \u533a\u5206vector\u7684\u7ef4\u5ea6\u548cvector space\u7684\u7ef4\u5ea6\u3002 \u5047\u5982\u6709\u4e00\u7ec4\u5411\u91cf\u662f3\u7ef4\u7684\uff0c\u4f46\u5b83\u4eec\u53ea\u80fd\u5f20\u6210\u4e00\u4e2a2\u7ef4\u5e73\u9762\uff0c\u5219\u8fd9\u4e2a\u5f20\u6210\u7a7a\u95f4\u662f2\u7ef4\u800c\u4e0d\u662f3\u7ef4\u3002</p> <p></p> <p>\u5982\u4f55\u8ba1\u7b97\u4e00\u4e2asubspace\u7684basis\uff1a  \u5c06spanning vector\u5199\u6210\u77e9\u9635A\uff0cA\u7684echelon form pivot\u5bf9\u5e94\u7684column\u5c31\u662f\u8fd9\u4e2asubspace\u7684basis\u3002</p>"},{"location":"MML/chap2/#262-rank","title":"2.6.2 Rank","text":"<p>^e436c7</p> <p>rank\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u77e9\u9635\u7684linearly independent columns\u7684\u4e2a\u6570\u3002 </p> <p></p> <p>\u5bf9\u4e8e<code>n*n</code> \u7684\u65b9\u9635\uff0c\u77e9\u9635\u53ef\u9006\u5f53\u4e14\u4ec5\u5f53rank=n\u3002 </p> <p>Ax=b\u6709\u89e3\u5f53\u4e14\u4ec5\u5f53\\(rk(A)=rk(A|b)\\). </p> <p></p> <p></p>"},{"location":"MML/chap2/#27-linear-mappings","title":"2.7 Linear Mappings","text":"<p>\u73b0\u5728\u6709\u4e24\u4e2avector space V\u548cW\u3002\u6211\u4eec\u60f3\u8ba9V-&gt;W\u7684\u6620\u5c04\u53ef\u4ee5\u4fdd\u6301\u5411\u91cf\u6240\u5177\u6709\u7684\u201c\u53ef\u4ee5\u76f8\u52a0\uff0c\u53ef\u4ee5\u6807\u91cf\u4e58\u6cd5\u201d\u7684\u6027\u8d28\uff0c\u6ee1\u8db3\u8fd9\u6837\u6761\u4ef6\u7684\u6620\u5c04\u79f0\u4e3a linear mapping \u6216\u8005 linear transformation\u3002 </p> <p>\u77e9\u9635\u672c\u8eab\u5c31\u662f\u4e00\u79cd\u7ebf\u6027\u53d8\u6362\u3002\u6b64\u5916\uff0c\u4e4b\u524d\u63d0\u5230\u77e9\u9635\u8fd8\u53ef\u4ee5\u8868\u793a\u5411\u91cf\u7684\u96c6\u5408\u3002 \u53ef\u89c1\u77e9\u9635\u6709\u4e24\u79cd\u7406\u89e3\u65b9\u5f0f\u3002 When working with matrices, we have to keep in mind what the matrix represents: a linear mapping or a collection of vectors. </p> <p> Injective: \u4e0d\u5b58\u5728V\u4e2d\u591a\u4e2a\u5143\u7d20\u6620\u5c04\u5230W\u4e2d\u540c\u4e00\u5143\u7d20\u7684\u60c5\u51b5\u3002 Surjective: W\u4e2d\u6240\u6709\u5143\u7d20\u90fd\u80fd\u901a\u8fc7V\u4e2d\u7684\u67d0\u4e00\u5143\u7d20\u6620\u5c04\u5f97\u5230\u3002</p> <p>\u4e00\u65e6\u4e00\u4e2a\u6620\u5c04\u6ee1\u8db3Bijective\uff0c\u5219\u8fd9\u4e2a\u6620\u5c04\u5b58\u5728\u5bf9\u5e94\u7684\u9006\u6620\u5c04\u3002 </p> <p>\u5982\u679c\u53ea\u8003\u8651\u7ebf\u6027\u6620\u5c04\uff0c\u5219\u53ef\u4ee5\u5f97\u5230\u51e0\u4e2a\u65b0\u6982\u5ff5\uff1a </p> <p> \u5bf9\u4e8e\u4e24\u4e2afinite-dimensional vector space V\u548cW\uff0c V and W are isomorphic if and only if dim(V) = dim(W).</p> <p>V\u548cW\u540c\u6784\uff08\u7ebf\u6027\u6620\u5c04+\u53cc\u5c04\uff09&lt;=&gt; dim(V) = dim(W).</p> <p>Intuitively, this means that vector spaces of the same dimension are kind of the same thing, as they can be transformed into each other without incurring any loss. </p> <p>\u540c\u65f6\u7531\u4e0a\u9762\u8fd9\u4e2a\u5b9a\u7406\uff0c\u6211\u4eec\u53d1\u73b0mxn\u7684\u77e9\u9635\u548cmn\u7ef4\u7684\u5411\u91cf\u5176\u5b9e\u662f\u540c\u6784\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e4b\u95f4\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u53cc\u5c04\u7684\u6620\u5c04\u4e92\u76f8\u8f6c\u6362\u3002</p> <p>\u4e00\u4e9b\u6027\u8d28\uff1a  </p>"},{"location":"MML/chap2/#271-matrix-representation-of-linear-mappings","title":"2.7.1 Matrix Representation of Linear Mappings","text":"<p>\u63a5\u4e0b\u6765\u7684\u8ba8\u8bba\u4e2d\uff0cbasis\u4e4b\u95f4\u7684\u987a\u5e8f\u5f88\u91cd\u8981\uff0c B\u4e3a\u5411\u91cf\u7a7a\u95f4V\u7684n\u4e2abasis\u7ec4\u6210\u7684n-tuple\uff0c \u79f0\u4e3aV\u7684ordered basis\u3002 </p> <p>\u6ce8\u610f\u533a\u5206\u4e0b\u9762\u8fd93\u79cdnotation\u3002 </p> <p>\u56e0\u4e3aV\u4e2d\u7684\u4efb\u610f\u5143\u7d20x\u90fd\u53ef\u4ee5\u88abbasis\u552f\u4e00\u8868\u793a\uff0c\u800cB\u53c8\u662fbasis\u7684\u6709\u5e8ftuple\uff0c\u5219\u5bf9\u4e8e\u6bcf\u4e2ax\u6211\u4eec\u90fd\u53ef\u4ee5\u7528\u4e00\u7cfb\u5217\u7cfb\u6570\u7ec4\u6210\u7684vector\u8868\u793a\u3002 \u5b9a\u4e49\u8fd9\u4e2avector\u5c31\u662fthe coordinate vector/coordinate representation of x with respect to the ordered basis B.</p> <p>Transformation Matrix \u6211\u4eec\u5c06V\u7684\u6bcf\u4e00\u4e2abasis b \u7684\u6620\u5c04\u540e\u7684\u5411\u91cf\\(\\Phi(b_j)\\)\u90fd\u8868\u793a\u4e3aW\u7684basis c\u7684\u7ebf\u6027\u7ec4\u5408\u3002 \uff08\u6ce8\u610f\u4e0d\u662f\u7528c\u8868\u793ab\uff0c\u800c\u662f\u7528c\u8868\u793a\u6620\u5c04\u4e4b\u540e\u7684\\(\\Phi(b_j)\\)\u3002\uff09 \u6362\u53e5\u8bdd\u8bf4\uff0c\\(b_j\\)\u662f\u8d77\u70b9\uff0c\u662f\u81ea\u53d8\u91cf\u3002\\(\\Phi(b_j)\\)\u662f\u7ec8\u70b9\uff0c\u662f\u56e0\u53d8\u91cf\u3002\u800c\u6211\u4eec\u60f3\u8981\u7528\u67d0\u79cd\u5f62\u5f0f\u628a\u8fd9\u4e2a\u6620\u5c04\u8fc7\u7a0b\u8bb0\u5f55\u4e0b\u6765\uff0c\u56e0\u6b64\u5c31\u8bb0\u5f55\u4e0b\\(\\Phi(b_j)\\)\u76f8\u5bf9\u4e8ebasis c\u7684\u5750\u6807\u3002\u8fd9\u6837\u4e00\u6765\uff0c\u6211\u4eec\u5c31\u5c06\u6574\u4e2a\u6620\u5c04\u7684\u8fc7\u7a0b\u201c\u7ffb\u8bd1\u201d\u6210\u4e86\u76f8\u5bf9\u4e8ebasis c\u7684\u5750\u6807\uff0c\u8bb0\u5f55\u5230\u77e9\u9635A\u4e2d\u3002 \u7136\u540e\u628a\u5bf9\u5e94\u7684\u7cfb\u6570\u5b58\u5230\u4e00\u4e2a\u77e9\u9635A\u4e2d\uff0c\u8fd9\u4e2aA\u5c31\u8bb0\u5f55\u7740V-&gt;W\u8fd9\u4e2a\u6620\u5c04\u7684\u4fe1\u606f\u3002 \u56e0\u4e3a </p> <p>\u56e0\u4e3a\u6211\u4eec\u662f\u7528c\u6765\u8868\u793ab\uff0c\u8868\u793a\u6240\u7528\u7684\u7cfb\u6570a\uff0c\u5c31\u662f\u7ebf\u6027\u53d8\u6362\u4e4b\u540eb\u7684\u5750\u6807\u3002 \u4e5f\u5c31\u662f\u8bf4\\(\\Phi(b_j)\\)\u7684\u5750\u6807\u5c31\u662fA\u7684\u7b2cj\u5217\u7684\u5411\u91cf\u3002 </p> <p>\u8fd9\u4e2a\u77e9\u9635A\u53ef\u4ee5\u7528\u6765\u628a\u4e00\u4e2a\u5411\u91cfx\u76f8\u5bf9\u4e8eB\u7684\u5750\u6807\u201c\u7ffb\u8bd1\u201d\u6210\u76f8\u5bf9\u4e8eC\u7684\u5750\u6807\u3002 </p> <p></p>"},{"location":"MML/chap2/#272-basis-change","title":"2.7.2 Basis Change","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6765\u5206\u6790basis\u7684\u9009\u62e9\u5bf9\u4e8e\u53d8\u6362\u77e9\u9635A\u7684\u5f71\u54cd\u3002 </p> <p>  \u8bc1\u660e\u5c31\u7565\u8fc7\u4e86\uff0c\u811a\u6807\u592a\u590d\u6742\uff0c\u770b\u4e0d\u61c2\u3002 \u76f4\u89c2\u7406\u89e3\uff1a </p> <p> </p>"},{"location":"MML/chap2/#_1","title":"\u77e9\u9635\u7b49\u4ef7\u3001\u77e9\u9635\u76f8\u4f3c","text":"<p>^dbbab1</p> <p>Equivalent\uff1a \u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\u7684\u6574\u4e2a\u8fc7\u7a0b\uff0c\u5176\u5b9e\u5c31\u662f\u5728\u8bb2\\(A\\)\u548c\\(\\tilde{A}\\)\u5176\u5b9e\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\u3002\u56e0\u6b64\u5c06\u5176\u79f0\u4e4b\u4e3aequivalent\u3002 </p> <p>Similar\uff1a \u6ce8\u610f\u4e0b\u9762\u5b9a\u4e49\u4e2d\\(A\\)\u548c\\(\\tilde{A}\\)\u90fd\u662f<code>n*n</code>\u65b9\u9635\uff0c\u4e3a\u5565\u5462\uff1f  \u56e0\u4e3asimilar\u662fequivalent\u7684\u7279\u6b8a\u60c5\u51b5\u3002 equivalent\u7684\u573a\u666f\u662fV-&gt;W\uff0c \u800csimilar\u662fV-&gt;V\uff0c\u6240\u4ee5\\(A\\)\u548c\\(\\tilde{A}\\)\u90fd\u662fV\u5411V\u8fd9\u4e2a\u5411\u91cf\u7a7a\u95f4\u81ea\u5df1\u7684\u6620\u5c04\u3002 \u800c\u4e3a\u4ec0\u4e48S=T\uff1f</p> <p></p> <p>\u6839\u636e So we are only choosing one basis for both sides. This restricts our freedom of action, but also preserves more properties of the matrix A.\uff0c \u4e0a\u56fe\u5176\u5b9e\u753b\u7684\u4e0d\u5bf9\uff0c\u5e94\u8be5\u662f\u4e0b\u56fe\u8fd9\u6837\u3002 V-&gt;V\u5e76\u4e0d\u80fd\u5f97\u51faS=T\uff0c \u4e4b\u6240\u4ee5S=T\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u5728\u77e9\u9635\u76f8\u4f3c\u7684\u573a\u666f\u4e2d\u5f15\u5165\u4e86\u66f4\u5f3a\u7684\u5047\u8bbe\uff1a linear transformation\u524d\u540e\u4f7f\u7528\u76f8\u540c\u7684basis\uff0c\u800c\u4e0d\u662f\u4e24\u7ec4\u4e0d\u540c\u7684basis\uff08B\u548cC\uff09\u3002 \u8fd9\u6837\u4e00\u6765\u81ea\u7136S\u662f\u540c\u4e00\u4e2aS\u3002  \u6839\u636e\u4e0a\u9762\u7684\u5e16\u5b50\uff0c\u5f15\u5165\u65b0\u7684\u5047\u8bbe\u2014\u2014 \"restricts our freedom of action, but also preserves more properties of the matrix A. Where \u77e9\u9635\u76f8\u4f3c\u53ea\u80fd\u63a8\u51fa \\(rk(A)=rk(\\tilde{A})\\), now we get \\(det(A)=det(\\tilde{A}\\)), \\(trace(A)=trace(\\tilde{A}\\)) and the Eigenvalues of \\(A\\) and \\(\\tilde{A}\\) coincide. \" \u867d\u7136\u8bbe\u5b9a\u66f4\u4e25\u683c\uff0c\u4f46\u540c\u65f6\u5e26\u6765\u4e86\u66f4\u591a\u7684\u6027\u8d28\u3002</p> <p> </p>"},{"location":"MML/chap2/#273-image-and-kernel","title":"2.7.3 Image and Kernel","text":"<p>\u5047\u8bbe\u6709\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04\\(\\Phi:V\\to W\\)\uff0cV\u548cW\u4e3a\u5411\u91cf\u7a7a\u95f4\u3002 \\(ker(\\Phi)\\)\u662fV\u4e2d\u88ab\u6620\u5c04\u5230W\u7684\u96f6\u5143\u7684\u5411\u91cf\u7684\u96c6\u5408\u3002 \\(Im(\\Phi)\\)\u662fW\u4e2d\uff0c\u6240\u6709\u53ef\u4ee5\u7ecf\u8fc7\u6620\u5c04\u5f97\u5230\u7684\u5143\u7d20\u7684\u96c6\u5408 \u3002   \u6027\u8d28\uff1a </p> <p>\u5047\u8bbeW\u662fV\u7ecf\u8fc7\u7ebf\u6027\u53d8\u6362A\u5f97\u5230\u7684\uff0c\u90a3\u4e48\u8fd9\u4e2a\u6620\u5c04\u7684image\u5c31\u662fA\u7684columns\u7684span\uff0c\u79f0\u4e3acolumn space\u3002 </p> <p></p> <p>kernel/null space\u5bf9\u5e94Ax=0\u7684\u901a\u89e3\u3002 </p> <p>kernel\u548cimage\u7684dimention\u4e4b\u548c\u7b49\u4e8eV\u7684dimention\u3002  \u4e0a\u9762\u8fd9\u4e2a\u5b9a\u7406\u7684\u63a8\u8bba\uff0c\u61d2\u5f97\u770b\u4e86\uff1a </p>"},{"location":"MML/chap2/#28-affine-spaces","title":"2.8 Affine Spaces","text":"<p>\u63a5\u4e0b\u6765\u6765\u8ba8\u8bba\u4e00\u4e9b\u4e0d\u7ecf\u8fc7origin\u7684\u7a7a\u95f4\u3002\u56e0\u4e3a\u4e0d\u5305\u542b\u96f6\u5143\uff0c\u8fd9\u4e9b\u7a7a\u95f4\u4e0d\u518d\u662f\u5411\u91cf\u7a7a\u95f4\u3002 </p> <p>ML\u8bed\u5883\u4e0b\u5f88\u591a\u65f6\u5019linear\u548caffine\u662f\u6df7\u7528\u7684\u3002 </p>"},{"location":"MML/chap2/#281-affine-subspaces","title":"2.8.1 Affine Subspaces","text":"<p>Affine Subspace\uff1a V\u662f\u5411\u91cf\u7a7a\u95f4\uff0cU\u662fV\u7684\u5b50\u7a7a\u95f4\u3002\\(x_0\\)\u662fV\u7684\u4e00\u4e2a\u5411\u91cf\u3002 \u5219\\(x_0\\)\u52a0\u4e0aU\u4e2d\u4efb\u4e00\u5411\u91cf\u5f97\u5230\u7684\u7ed3\u679c\u7684\u96c6\u5408\u5c31\u79f0\u4e3aaffine subspace\u6216\u8005linear manifold\u3002 U\u79f0\u4e3adirection\u6216direction space\uff0c \\(x_0\\)\u79f0\u4e3asupport point\u3002 \u5728\u540e\u9762\u7684\u7ae0\u8282\u79f0\u8fd9\u6837\u7684subspace\u4e3ahyerplane\u3002  \u5176\u5b9e\u76f8\u5f53\u4e8e\u628a\u5b50\u7a7a\u95f4U\u4e2d\u7684\u6240\u6709\u5143\u7d20\u90fd\u5e73\u79fb\u4e86\\(x_0\\)\u3002 U\u672c\u8eab\u4e00\u5b9a\u5305\u542b\u96f6\u5143\uff0c\u4f46\u5e73\u79fb\u4e4b\u540e\u53ef\u80fd\u4e0d\u5305\u542b\u3002\u56e0\u6b64affine subspace\u4e0d\u4e00\u5b9a\u662fvector space\u3002 Examples of affine subspaces are points, lines, and planes in R3, which do not (necessarily) go through the origin.</p> <p>\u4e0b\u9762\u6211\u4eec\u6765\u5b9a\u4e49parameters\uff1a \uff08parameter\u662f\u501f\u52a9affine space\u5b9a\u4e49\u7684\u3002\uff09 \u5047\u8bbe\u6709\u4e00\u4e2ak-dim affine space \\(L=x_0+U\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u7528U\u7684ordered basis \\((b_1,b_2,...,b_k)\\)\u6765uniquely\u8868\u793a\u8fd9\u4e2aaffine space  \u8fd9\u79cd\u8868\u793a\u79f0\u4e3aparametric equation of L\uff0c\u6240\u7528\u5230\u7684\u7cfb\u6570\u79f0\u4e3aparameters.</p> <p>line\uff0cplane\uff0chyperspace\u90fd\u662f\u501f\u52a9affine subspaces\u5b9a\u4e49\u7684\uff1a   plane\uff1a U\u662f\u4e24\u4e2a\u7ebf\u6027\u72ec\u7acb\u7684basis\u6240\u5f20\u6210\u7684span\uff0c\u7136\u540e\\(x_0\\)\u4e0eU\u76f8\u52a0\u5f97\u5230\u7684affine subspace\u5c31\u662f\u4e00\u4e2a\u5e73\u9762\u3002</p> <p></p> <p>Inhomogeneous systems of linear equations Ax=b\u5bf9\u5e94\u7684\u662f\u4e00\u4e2a\u7ef4\u5ea6\u4e3an-rk(A)\u7684affine subspace\uff0c homogeneous equation systems Ax = 0\u5bf9\u5e94\u7684\u662fvector subspace\u3002 </p>"},{"location":"MML/chap2/#282-affine-mappings","title":"2.8.2 Affine Mappings","text":"<p>affine mapping\u53ef\u4ee5\u770b\u4f5c\u662f\u7ebf\u6027\u6620\u5c04\\(V\\to W\\)\u548ctranslation \\(W\\to W\\)\u7684\u7ec4\u5408\u3002 </p> <p>affine mapping \u7ec4\u5408\u4e4b\u540e\u8fd8\u662faffine mapping.  </p>"},{"location":"MML/chap3/","title":"chap3 Analytic Geometry","text":""},{"location":"MML/chap3/#31-norms","title":"3.1 Norms","text":"<p>norm \u662f\u4e00\u4e2a\u5c06\u5411\u91cf\u6620\u5c04\u4e3a\u5b9e\u6570\u7684\u51fd\u6570\u3002 norm\u9700\u8981\u6ee1\u8db3\u53ef\u4e58\u3001\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u975e\u8d1f\u8fd9\u4e09\u4e2a\u6027\u8d28\u3002 </p> <p> </p>"},{"location":"MML/chap3/#32-inner-products","title":"3.2 Inner Products","text":""},{"location":"MML/chap3/#321-dot-product","title":"3.2.1 Dot Product","text":"<p>dot product\u53ea\u662finner product\u7684\u4e00\u79cd\u5f62\u5f0f\u3002 </p>"},{"location":"MML/chap3/#322-general-inner-products","title":"3.2.2 General Inner Products","text":"<p>\u4e4b\u524d\u63d0\u5230\u7684mapping\u90fd\u53ea\u6709\u4e00\u4e2aargument\uff0c \u63a5\u4e0b\u6765\u7528\u5230\u7684\u6620\u5c04\\(\\Omega\\) \u6709\u4e24\u4e2aargument\uff0c\u5e76\u4e14\u5bf9\u6bcf\u4e2aargument\u90fd\u662flinear\u7684\uff0c\u79f0\u4e3abilinear mapping\u3002 </p> <p>\u5047\u8bbe\\(\\Omega\\) \u662f\u4e00\u4e2a\\(V\\times V \\to R\\) \u7684bilinear mapping\uff0c\u5c06\u540c\u4e00\u4e2a\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u4e3a\u4e00\u4e2a\u5b9e\u6570\uff0c \u5f53\u4e24\u4e2aargument\u7684\u987a\u5e8f\u4e0d\u91cd\u8981\u7684\u65f6\u5019\uff0c\u79f0\\(\\Omega\\) \u662fsymmetric\u7684\u3002 \u5982\u679c\\(\\Omega\\)\u6ee1\u8db3\uff1a\u53ea\u8981x\u4e0d\u662f0\u5411\u91cf\uff0c\\(\\Omega(x,x)\\)\u5c31\u4e00\u5b9a\u5927\u4e8e0\uff1b\\(\\Omega(0,0)=0\\). \u5c31\u79f0\\(\\Omega\\)\u662fpositive definite\u7684\u3002</p> <p></p> <p>Inner product \u7684\u5b9a\u4e49\uff1a A positive definite, symmetric bilinear mapping \\(\\Omega\\) \u88ab\u79f0\u4e3aV\u4e0a\u7684inner product\u3002\uff08\u6b63\u5b9a+\u5bf9\u79f0\uff09. </p> <p>V\u4e0einner product\u7684\u7ec4\u5408\u79f0\u4e3ainner product space\u3002 \u5f53\u4f7f\u7528dot product\u4f5c\u4e3ainner product\u65f6\uff0c\u79f0V\u4e3aEuclidean vector space\u3002 </p>"},{"location":"MML/chap3/#323-symmetric-positive-definite-matrices","title":"3.2.3 Symmetric, Positive Definite Matrices","text":"<p>\u673a\u5668\u5b66\u4e60\u4e2d\uff0cSymmetric, positive definite matrices\u5f88\u91cd\u8981\uff0c\u800c\u5b83\u4eec\u662f\u901a\u8fc7inner product\u6765\u5b9a\u4e49\u7684\u3002</p> <p>\u5047\u8bbe\u5411\u91cf\u7a7a\u95f4V\u5e26\u6709\u4e00\u4e2ainner product\u8fd0\u7b97\\(\\langle \\cdot,\\cdot\\rangle\\) \uff0c\u5e76\u4e14\u6709\u4e00\u7ec4ordered basis B\u3002 \u73b0\u6709V\u4e2d\u7684\u4e24\u4e2a\u5411\u91cfx\u548cy\uff0c\u5219\u53ef\u4ee5\u7528B\u7684\u7ebf\u6027\u7ec4\u5408\u8868\u793a\u8fd9\u4e24\u4e2a\u5411\u91cf\u3002   \u901a\u8fc7\u4e0a\u9762\u53ef\u4ee5\u770b\u51fa\uff0c  \\(\\hat{x}\\)\u548c\\(\\hat{y}\\)\u662f\u5411\u91cfx\u548cy\u76f8\u5bf9\u4e8eB\u7684\u5750\u6807\uff0c\u4e5f\u5c31\u662f\u8bf4inner product\u672c\u8d28\u4e0a\u5176\u5b9e\u662fA\u51b3\u5b9a\u7684\u3002</p> <p>The symmetry of the inner product also means that A is symmetric.  innner product\u7684\u5bf9\u79f0\u51b3\u5b9a\u4e86A\u4e5f\u662f\u5bf9\u79f0\u7684\u3002</p> <p>\u7136\u540e\uff0c\u56e0\u4e3ainner product\u7684\u5b9a\u4e49  \u53ef\u4ee5\u76f4\u63a5\u5f97\u51fa\u4e0b\u9762\u7684\u5f0f\u5b50\uff1a \\(\\forall x \\in V\\setminus \\{0\\}: x^TAx&gt;0\\). </p> <p>Symmetric, Positive Definite Matrix \u5c31\u662f\u901a\u8fc7\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u5b9a\u4e49\u7684\uff1a \u5bf9\u4e8eV\u4e2d\u4efb\u610f\u975e0\u5411\u91cfx\u90fd\u6709\\(x^TAx&gt;0\\). \u4e14\u5982\u679cA\u5bf9\u79f0\uff0c \u5219A\u88ab\u79f0\u4e3aSymmetric, Positive Definite.  \u4e5f\u5c31\u662f\u8bf4\uff0c\u6b63\u5b9a\u77e9\u9635\u5176\u5b9e\u53cd\u6620\u7684\u662f\u8fd9\u4e2a\u5411\u91cf\u7a7a\u95f4\u4e2d\u6240\u5b9a\u4e49\u7684inner product\u8fd0\u7b97\u7684\u6027\u8d28\u3002</p> <p></p> <p>\u5982\u679c\u5df2\u77e5A\u662f\u4e00\u4e2asymmetric, positive definite\u77e9\u9635\u4e86\uff08\u5bf9\u79f0\uff0c\u5bf9\u975e\u96f6\u5411\u91cf\\(x^TAx&gt;0\\)\uff09\uff0c\u5219\\(\\hat{x}^TA\\hat{y}\\)\u5c31\u662fV\u4e0a\u7684\u4e00\u4e2ainner product\u8fd0\u7b97\u3002 </p> <p>\u67d0\u4e00\u4e2a\u6620\u5c04\u662finner product\uff0c\u5f53\u4e14\u4ec5\u5f53  \u8fd9\u4e2a\u6620\u5c04\u64cd\u4f5c\u53ef\u4ee5\u88ab\u8868\u793a\u4e3a\\(\\hat{x}^TA\\hat{y}\\)\u4e14A\u4e3asymmetric, positive definite\u77e9\u9635\u3002</p> <p>\u4e00\u4e9b\u63a8\u8bba\uff1a </p> <p>A\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u90fd&gt;0. \u56e0\u4e3a\u4ee4x\u4e3astandard basis\u65f6\uff0c\\(x^TAx&gt;0\\)\u5176\u5b9e\u5c31\u7ea6\u675f\u7740A\u7684\u5bf9\u89d2\u7ebf\u4e0a\u7684\u5143\u7d20&gt;0. </p>"},{"location":"MML/chap3/#33-lengths-and-distances","title":"3.3 Lengths and Distances","text":"<p>\u6240\u6709inner product\u8fd0\u7b97\u5f00\u6839\u53f7\u4e4b\u540e\u90fd\u6ee1\u8db3norm\u8fd0\u7b97\u7684\u8981\u6c42\uff0c\u4e5f\u5c31\u662f\u8bf4\u6240\u6709inner product\u5f00\u6839\u53f7\u4e4b\u540e\u90fd\u662f\u4e00\u4e2anorm\u8fd0\u7b97\u3002  \u4f46\u662f\u5e76\u4e0d\u662f\u6240\u6709norm\u90fd\u6709\u5bf9\u5e94\u7684inner product\uff0c\u6bd4\u5982L1-norm\u3002</p> <p>\u5b9a\u4e49distance\uff1a  </p> <p>\u5c3d\u7ba1 inner product\u4e0emetric\u90fd\u662f\u5c06\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u4e3a\u4e00\u4e2a\u5b9e\u6570\uff0c\u800c\u4e14\u90fd\u662fsymmetric, positive definite\u7684\uff0c\u4e5f\u90fd\u6ee1\u8db3\u4e09\u89d2\u4e0d\u7b49\u5f0f\u3002 \u4f46\u662f\uff0c\u5b83\u4eec\u7684behavior\u6b63\u597d\u76f8\u53cd </p>"},{"location":"MML/chap3/#34-angles-and-orthogonality","title":"3.4 Angles and Orthogonality","text":"<p>inner products also capture the geometry of a vector space by defining the angle \u03c9 between two vectors.</p> <p>\u7531\u4e8e\u67ef\u897f\u4e0d\u7b49\u5f0f\uff1a  \u56e0\u6b64\u6709  \u7531\u6b64\u5c31\u53ef\u4ee5\u5b9a\u4e49\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u7684\u5939\u89d2angle\u3002 </p> <p>\u6b63\u4ea4\uff1a \u4e24\u4e2a\u5411\u91cfinner product\u4e3a0\u79f0\u4e3aorthogonal\uff0c \u6b63\u4ea4+\u4e24\u4e2a\u5411\u91cf\u90fd\u662f\u5355\u4f4d\u5411\u91cf\uff0c\u79f0\u4e3aorthonormal\u3002 </p> <p>\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4e24\u4e2a\u5411\u91cf\u5728\u4f7f\u7528dot product\u65f6\u6b63\u4ea4\uff0c \u4f46\u4f7f\u7528  \u53e6\u4e00\u4e2ainner product\u65f6\u4e0d\u6b63\u4ea4\u3002  </p> <p>Orthogonal Matrix\uff1a  \u5f53\u77e9\u9635\u7684\u6bcf\u4e00\u5217\u90fdorthonormal\u7684\uff08\u6ce8\u610f\u4e0d\u662forthogonal\uff09\uff0c\u5411\u91cf\u4e0e\u81ea\u5df1\u4f5cinner product\u5f97\u52301\uff0c\u5176\u4f59\u5f97\u52300\u3002\u56e0\u6b64  \u7136\u540e\u5c31\u53ef\u4ee5\u76f4\u63a5\u63a8\u51fa </p> <p>\u6ce8\u610f\uff1a \u6839\u636e\u4e0a\u9762\u7684\u5b9a\u4e49\uff0c\u867d\u7136\u6211\u4eec\u53ebA\u201corthogonal matrix\u201d\uff0c\u4f46\u5176\u5b9eA\u662f\u201corthonormal\u201d\u7684\uff0c\u8fd9\u53ea\u662f\u547d\u540d\u7684\u95ee\u9898\uff0c\u6709\u70b9\u602a\u3002 \u6ce8\u610f\u4ee5\u540e\u63d0\u5230\u201corthogonal matrix\u201d\uff0c\u5c31\u9690\u542b\u4e86\u6bcf\u4e2acolumn\u90fd\u662forthonormal\u7684\uff0c\u662f\u5355\u4f4d\u77e9\u9635\u3002</p> <p>\u7531\u4e0b\u9762\u8fd9\u4e2a\u5f0f\u5b50\u53ef\u4ee5\u770b\u51fa\uff0c\u67d0\u4e00\u5411\u91cfx\u5728\u7ecf\u8fc7\u4e00\u4e2aorthogonal matrix\u7684\u53d8\u6362\u4e4b\u540e\uff0c\u5176norm\u4e0d\u4f1a\u6539\u53d8\uff0c\u672c\u8d28\u4e0a\u662f\u56e0\u4e3a\\(A^TA=I\\)\u3002 </p> <p>\u6b64\u5916\uff0c\u7531\u4e0b\u9762\u8fd9\u4e2a\u5f0f\u5b50\u53ef\u4ee5\u770b\u51fa\uff0c\u5bf9x\u3001y\u540c\u65f6\u65bd\u52a0orthogonal matrix\u7684\u53d8\u6362\uff0c\u53d8\u6362\u524d\u540e\u4e24\u4e2a\u5411\u91cf\u7684\u5939\u89d2\u4e5f\u4e0d\u4f1a\u6539\u53d8\u3002 </p> <p>\u5176\u5b9eorthogonal matrix\u5bf9\u5e94\u7684\u662f\u65cb\u8f6c\u53d8\u6362\uff08with the possibility of flips\uff09.</p>"},{"location":"MML/chap3/#35-orthonormal-basis","title":"3.5 Orthonormal Basis","text":"<p>\u4e4b\u524d\u6211\u4eec\u63d0\u5230n\u7ef4\u5411\u91cf\u7a7a\u95f4\u9700\u8981n\u4e2a\u7ebf\u6027\u72ec\u7acb\u7684\u5411\u91cf\uff08basis\uff09\uff0c \u73b0\u5728\u6211\u4eec\u589e\u52a0\u4e24\u4e2a\u7ea6\u675f\uff0c basis\u6b63\u4ea4\uff0cbasis\u4e3a\u5355\u4f4d\u5411\u91cf\uff08\u4e5f\u5c31\u662f\u8bf4\u662forthonormal basis\uff09\uff0c\u770b\u770b\u80fd\u5f97\u51fa\u4ec0\u4e48\u6027\u8d28\u3002</p> <p></p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u9ad8\u65af\u6d88\u5143\u6cd5\u5f97\u5230\u4e00\u4e2aspan\u7684orthonormal basis\uff0c \u4f7f\u7528\u7684augmented matrix\u4e3a\\(\\hat{B}\\hat{B}^T|\\hat{B}\\). \u76f4\u89c2\u4e0a\u4e5f\u4e0d\u96be\u7406\u89e3\uff0c\u6211\u4eec\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u6c42\u5f97\u4e00\u4e2a\\(\\hat{B}^T\\)\uff0c\u800c\u7531\u4e8e\\(\\hat{B}^T\\)\u6240\u5904\u7684\u4f4d\u7f6e\uff0c\u9700\u8981\\(\\hat{B}\\hat{B}^T=\\hat{B}\\)\uff0c \u4e5f\u5c31\u662f\u8bf4\\(\\hat{B}^T=\\hat{B}^{-1}\\)\uff0c\u56e0\u6b64\u9ad8\u65af\u6d88\u5143\u5f97\u5230\u7684\u77e9\u9635\u662f\u6b63\u4ea4\u77e9\u9635\uff0c\u5f97\u5230\u7684\u5411\u91cf\u662forthonormal basis\u3002 </p>"},{"location":"MML/chap3/#36-orthogonal-complement","title":"3.6 Orthogonal Complement","text":"<p>Having defined orthogonality, we will now look at vector spaces that are orthogonal to each other. </p> <p>\u5047\u8bbeV\u662fD\u7ef4\u5411\u91cf\u7a7a\u95f4\uff0cU\u662fV\u7684M\u7ef4\u5b50\u7a7a\u95f4\uff0c \u5219V\u4e2d\u90a3\u4e9b\u4e0eU\u4e2d\u6240\u6709\u5411\u91cf\u90fd\u6b63\u4ea4\u7684\u5411\u91cf\u7ec4\u6210\u7684\u96c6\u5408\uff08\u51c6\u786e\u7684\u8bf4\u662f\u5b50\u7a7a\u95f4\uff09\uff0c\u5c31\u662fU\u7684orthogonal complement \\(U^\\perp\\). \u5e76\u4e14\u5b83\u7684\u7ef4\u5ea6\u662fD-M\u3002 </p> <p>\u5e76\u4e14V\u4e2d\u7684\u4efb\u610f\u5411\u91cf\u90fd\u53ef\u4ee5\u7528\\(U\\)\u7684basis\u548c\\(U^\\perp\\)\u7684basis\uff08\u7684\u5e76\u96c6\uff09\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8868\u793a\u3002 </p> <p>\u5047\u5982\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\uff0cU\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5e73\u9762\uff0c\u5219\u4e0eU\u6b63\u4ea4\u7684\u5355\u4f4d\u5411\u91cf\u03c9\u5c31\u662fU\u7684orthonormal complement\u8fd9\u4e2a\u5b50\u7a7a\u95f4\u7684basis\u3002\u6211\u4eec\u79f0\u03c9\u662fU\u7684\u6cd5\u5411\u91cf\uff08normal vector\uff09\u3002 </p>"},{"location":"MML/chap3/#37-inner-product-of-functions","title":"3.7 Inner Product of Functions","text":"<p>In the following, we will look at an example of inner products of a different type of vectors: inner products of functions. \u6211\u4eec\u5c06\u5411\u91cf\u63a8\u5e7f\u5230\u51fd\u6570\uff0c\u6765\u8ba8\u8bbafunction\u4e4b\u95f4\u7684inner product\u548cfunction\u4e4b\u95f4\u7684orthogonal\u3002</p> <p>We can think of a vector \\(x\\in \\mathbb{R}^n\\)  as a function with n function values. \u7136\u540e\uff0c\u5c06\u6709\u9650\u7ef4\u5411\u91cf\u63a8\u5e7f\u5230\u65e0\u9650\u7ef4\uff0c\u5c06\u79bb\u6563\u7684\u4e00\u4e2a\u4e2aargument\u63a8\u5e7f\u5230\u4e00\u4e2a\u8fde\u7eed\u7684\u5b9a\u4e49\u57df\uff0c \u8fd9\u6837\u4e00\u6765\u539f\u672c\u7684inner product\u9700\u8981\u7684\u6c42\u548c\u5c31\u53ef\u4ee5\u63a8\u5e7f\u5230\u6c42\u5b9a\u79ef\u5206\u3002</p> <p> </p> <p>\u5f53\u5b9a\u79ef\u5206\u7ed3\u679c\u4e3a0\uff0c\u5219\u79f0\u8fd9\u4e24\u4e2a\u51fd\u6570\u4e3aorthogonal. \u4e0e\u6709\u9650\u7ef4\u5411\u91cf\u6c42inner product\u4e0d\u540c\uff0c\u5bf9\u4e24\u4e2a\u51fd\u6570\u6c42inner product\u6c42\u79ef\u5206\uff0c\u7ed3\u679c\u6709\u53ef\u80fddiverge\uff08\u65e0\u7a77\u5927\uff09\uff0c\u56e0\u6b64\u8fd9\u91cc\u9700\u8981\u6570\u5b66\u4e0a\u6709\u5176\u4ed6\u8981\u6c42\u3002\u4f46\u672c\u4e66\u4e0d\u6d89\u53ca\u8fd9\u4e9b\u3002 </p> <p></p>"},{"location":"MML/chap3/#38-orthogonal-projections","title":"3.8 Orthogonal Projections","text":"<p>we can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns</p> <p>projection\u7684\u5b9a\u4e49\uff1a \u6709\u70b9\u96be\u7406\u89e3\uff0c\u5982\u679c\u5bf9V\u8fdb\u884c\u4e24\u6b21\u53d8\u6362\u7684\u7ed3\u679c\uff0c\u548c\u8fdb\u884c\u4e00\u4e2a\u53d8\u6362\u7684\u7ed3\u679c\u76f8\u540c\uff0c\u5219\u8fd9\u4e2a\u53d8\u6362\u79f0\u4e3a\u6295\u5f71\u3002 \u5c31\u6bd4\u5982\u5c06\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e9b\u5411\u91cf\u6295\u5f71\u5230xy\u5e73\u9762\u4e0a\uff0c\u6295\u5f71\u4e00\u6b21\u7684\u7ed3\u679c\uff0c\u548c\u201c\u5148\u6295\u5f71\u4e00\u6b21\uff0c\u518d\u6295\u5f71\u4e00\u6b21\u201d\u7684\u7ed3\u679c\u662f\u76f8\u540c\u7684\u3002 </p> <p>\u7ebf\u6027\u53d8\u6362\u90fd\u53ef\u4ee5\u7528\u77e9\u9635\u63cf\u8ff0\uff0c \u63cf\u8ff0\u6295\u5f71\u64cd\u4f5c\u7684\u77e9\u9635\u79f0\u4e3aprojection matrices\u3002 \u9700\u8981\u6ee1\u8db3\\(P_\\pi^2=P_\\pi\\). </p> <p>\u4e0b\u9762\u6211\u4eec\u8ba8\u8bba\u5c06inner product spaces\u6295\u5f71\u5230subspace\u4e2d\u3002 \u5176\u4e2dinner product\u6211\u4eec\u9ed8\u8ba4\u5168\u90e8\u4f7f\u7528dot product\u3002</p>"},{"location":"MML/chap3/#381-projection-onto-one-dimensional-subspaces-lines","title":"3.8.1 Projection onto One-Dimensional Subspaces (Lines)","text":"<p>\u9996\u5148\u6211\u4eec\u6709\u4e00\u6761\u8fc7\u539f\u70b9\u7684\u76f4\u7ebf\uff0cbasis\u4e3ab\u3002\u8fd9\u6761\u76f4\u7ebf\u53ef\u4ee5\u770b\u4f5c\u662fbasis b\u5f20\u6210\u76841-dim subspace U\u3002 \u5f53\u6211\u4eec\u5c06\u5411\u91cfx\u6295\u5f71\u5230U\u65f6\uff0c\u5176\u5b9e\u5c31\u662f\u5bfb\u627e\u6295\u5f71\u5230U\u4e4b\u540e\u7684\u5411\u91cf\\(\\pi_U(x)\\in U\\)\u4e2d\uff0c\u54ea\u4e00\u4e2a\u8ddd\u79bbx\u6700\u8fd1\u3002 </p> <p> \u7136\u540e\uff1a </p> <p> </p> <p>\u4ece\u4e0a\u9762\u7684\u5f0f\u5b50\u53ef\u4ee5\u76f4\u63a5\u89c2\u5bdf\u51fa\\(P_\\pi\\)\u662fsymmetric\u7684\u3002</p> <p>\\(P_\\pi\\)\u53ef\u4ee5\u5c06\u4efb\u610f\u5411\u91cf\u6295\u5f71\u5230b\u6240\u5f20\u6210\u7684\u4e00\u7ef4\u5b50\u7a7a\u95f4\uff08\u76f4\u7ebf\uff09\u4e0a\u3002</p> <p>\u6ce8\u610f\uff0c\u6295\u5f71\u7684\u7ed3\u679c\u4ecd\u7136\u662fn\u7ef4\u5411\u91cf\uff0c\u800c\u4e0d\u662f\u6807\u91cf\u3002 \u4f46\u662f\uff0c\u6211\u4eec\u5176\u5b9e\u53ef\u4ee5\u76f8\u5bf9\u4e8ebasis b\u76f4\u63a5\u7528\\(\\lambda\\)\u6765\u8868\u8fbe\u6295\u5f71\u540e\u7684\u7ed3\u679c\u3002  </p> <p>chap4\u5c06\u4f1a\u8bb2\u5230\uff0c\\(\\pi_U(x)\\)\u5176\u5b9e\u662f\u77e9\u9635\\(P_\\pi\\)\u7684\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\uff0c\u5176\u5bf9\u5e94\u7684\u7279\u5f81\u503c\u662f1. </p>"},{"location":"MML/chap3/#382-projection-onto-general-subspaces","title":"3.8.2 Projection onto General Subspaces","text":"<p>\u4e0b\u9762\u6211\u4eec\u4ece\u6295\u5f71\u52301\u7ef4\u76f4\u7ebfU\u63a8\u5e7f\u5230\u6295\u5f71\u5230m\u7ef4\u7a7a\u95f4U\u3002 \u5047\u8bbeU\u6709\u4e00\u7ec4ordered basis \\(B=(b_1,b_2,...,b_m)\\)\uff0c\u5219\u5c06x\u6295\u5f71\u5230U\u4e2d\u4e4b\u540e\u53ef\u4ee5\u7528B\u6765uniquely\u5f97\u5230\\(\\pi_U(x)\\)\u7684\u5750\u6807\uff1a  </p> <p>\u7136\u540e\u6211\u4eec\u60f3\u8ba9\\(\\pi_U(x)\\)\u4e0e\\(x\\)\u8d8a\u8fd1\u8d8a\u597d\uff0c \u8fd9\u6837\u4e00\u6765\u5c31\u53ef\u4ee5\u63a8\u51fa\\(\\pi_U(x)-x\\)\u9700\u8981\u4e0eB\u6b63\u4ea4\u3002 </p> <p>\u5199\u6210\u77e9\u9635\u7684\u5f62\u5f0f\u5c31\u5f97\u5230\u4e86\u4e0b\u9762\u7684\u5f0f\u5b50\uff1a   \u6240\u4ee5normal equation\u5176\u5b9e\u8868\u793a\u7684\u5c31\u662f\u6295\u5f71\u64cd\u4f5c\u3002</p> <p>\u5e76\u4e14\u56e0\u4e3a\\(B=(b_1,b_2,...,b_m)\\)\u672c\u8eab\u662fbasis\uff0ccolumn\u4e00\u5b9a\u7ebf\u6027\u72ec\u7acb\uff0c\u56e0\u6b64\\(B^TB\\)\u4e00\u5b9a\u662f\u53ef\u9006\u7684\u3002\u8bc1\u660e \u56e0\u6b64\u53ef\u4ee5\u89e3\u51fa\u6295\u5f71\u4e4b\u540e\u5411\u91cf\u7684\u5750\u6807\\(\\lambda\\)\u3002 </p> <p> </p> <p>\u6709\u4e86\\(\\lambda\\)\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u8868\u793a\u51fa\\(\\pi_U(x)\\)\uff0c\u6700\u7ec8\u8868\u793a\u51faprojection matrix \\(P_\\pi\\)\u3002 </p> <p></p> <p>\u5f53Ax=b\u65e0\u89e3\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u6295\u5f71\u5f97\u5230\u4e00\u4e2a\u8fd1\u4f3c\u89e3\u3002 Ax=b\u65e0\u89e3\uff0c\u8bf4\u660eb\u4e0d\u4f4d\u4e8eA\u7684column\u7684span\u4e2d\u3002 \u8fd9\u65f6\u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u6295\u5f71\u627e\u5230A\u7684span\u4e2d\u79bbb\u6700\u8fd1\u7684\u4e00\u4e2a\uff0c\u4e5f\u5c31\u662f\u5c06b\u6295\u5f71\u5230A\u7684span\u4e2d\u3002 \u5982\u679c\u8fd9\u91cc\u7684inner product\u4f7f\u7528\u7684\u65f6dot product\uff0c\u5219\u8fd9\u4e2a\u6c42\u89e3\u8fc7\u7a0b\u5176\u5b9e\u5c31\u662f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002 </p> <p>\u5f53B\u662fOrthonormal Basis\u65f6\uff0c\u4e0a\u9762\u5f97\u5230\u7684\\(\\lambda\\)\u7684\u5f0f\u5b50\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7b80\u5316\u3002 </p>"},{"location":"MML/chap3/#383-gram-schmidt-orthogonalization","title":"3.8.3 Gram-Schmidt Orthogonalization","text":""},{"location":"MML/chap3/#384-projection-onto-affine-subspaces","title":"3.8.4 Projection onto Affine Subspaces","text":"<p>\u5047\u8bbe\u6709affine space \\(L = x_0+U\\)\uff0c\u800c\u6211\u4eec\u60f3\u8981\u628ax\u6295\u5f71\u5230L\u4e0a\u3002 \u4e8e\u662f\u6211\u4eec\u5c06\u5176\u8f6c\u5316\u4e3a\u628a\\(x-x_0\\)\u6295\u5f71\u5230\\(L-x_0=U\\)\u7684\u4efb\u52a1\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u6210\u4e86\u6295\u5f71\u5230\u5411\u91cf\u7a7a\u95f4\u7684\u4efb\u52a1\u3002\u6295\u5f71\u7ed3\u675f\u4e4b\u540e\uff0c\u518d\u52a0\u4e0a\\(x_0\\)\u5c06\u7ed3\u679c\u7ffb\u8bd1\u56deL\u3002 </p> <p>\u800c\u8fd9\u4e2a\u8f6c\u6362\u7684\u8fc7\u7a0b\u5e76\u4e0d\u4f1a\u5f71\u54cdx\u5230L\u7684\u8ddd\u79bb\u3002 </p>"},{"location":"MML/chap3/#39-rotations","title":"3.9 Rotations","text":"<p>A rotation is a linear mapping (more specifically, an automorphism of rotation a Euclidean vector space) that rotates a plane by an angle \u03b8 about the origin\u3002</p>"},{"location":"MML/chap3/#393-rotations-in-n-dimensions","title":"3.9.3 Rotations in n Dimensions","text":"<p>\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u65cb\u8f6c\uff1a\u56fa\u5b9a\u4f4f\u4e00\u4e2a\u8f74\u4e0d\u52a8\uff0c\u5c06\u5782\u76f4\u4e8e\u8fd9\u4e2a\u56fa\u5b9a\u8f74\u7684\u5e73\u9762\u8fdb\u884c\u65cb\u8f6c\u3002 \u63a8\u5e7f\u5230n\u7ef4\u7a7a\u95f4\u5c31\u662f\u56fa\u5b9a\u4f4fn-2\u7ef4\u4e0d\u52a8\u3002  </p>"},{"location":"MML/chap4/","title":"chap 4 - Matrix Decompositions","text":""},{"location":"MML/chap4/#41-determinant-and-trace","title":"4.1 Determinant and Trace","text":"<p>Determinants are only defined for square matrices. we write the determinant as det(A) or sometimes as |A|.</p> <p><code>n*n</code>\u65b9\u9635A\u7684determinant\u662f\u5c06A\u6620\u5c04\u5230\u5b9e\u6570\u7684\u4e00\u4e2a\u51fd\u6570\u3002</p> <p>\u77e9\u9635\u53ef\u9006&lt;=&gt;determinant\u4e0d\u4e3a0.  determinant\u4e00\u65e6\u4e3a0\uff0c\u8bf4\u660e\u7ecf\u8fc7\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\uff0c\u539f\u672c\u7ebf\u6027\u72ec\u7acb\u7684basis\u53d8\u5f97\u7ebf\u6027\u76f8\u5173\u4e86\uff0c\u4ee5\u81f3\u4e8edeterminant\u6240\u4ee3\u8868\u7684signed volume\u53d8\u4e3a\u4e860\u3002\u6362\u53e5\u8bdd\u8bf4\u7ecf\u8fc7\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\uff0cdimension\u51cf\u5c0f\u4e86\u3002 \u4e00\u4e2a\u5411\u91cf\u7a7a\u95f4\u964d\u7ef4\u4e4b\u540e\uff0c\u81ea\u7136\u5c31\u6ca1\u529e\u6cd5\u8fd8\u539f\u4f1a\u539f\u6765\u7684\u7a7a\u95f4\uff0c\u56e0\u6b64determinant\u4e3a0\u7684\u65f6\u5019\u77e9\u9635\u4e0d\u53ef\u9006\u3002</p> <p>\u4e0b\u56fe\u4e2d\u662fn=1,2,3\u65f6\uff0cdeterminant\u7684closed form\uff0c\u6bd4\u8f83\u5e38\u7528\u3002  </p> <p>upper-triangular\u77e9\u9635\uff1a \u5bf9\u89d2\u7ebf\u4e0b\u8fb9\u5168\u4e3a0 lower-triangular\u77e9\u9635\uff1a \u5bf9\u89d2\u7ebf\u4e0a\u8fb9\u5168\u4e3a0 \u6ce8\u610f\u5bf9\u89d2\u7ebf\u4e0d\u4e00\u5b9a\u4e3a0. </p> <p></p> <p></p> <p>determinant\u7684\u8ba1\u7b97\uff1a Laplace expansion  \u8fd9\u4e2a\u5f0f\u5b50\u5f88\u96be\u770b\u61c2\uff0c\u6240\u4ee5\u76f4\u63a5\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\uff1a  </p> <p>\u4e00\u4e9b\u6027\u8d28\uff1a </p> <p>\u7b2c\u4e8c\u7ae0\u63d0\u5230\uff1asilimar matrix\u672c\u8d28\u4e0a\u63cf\u8ff0\u7684\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u53ea\u4e0d\u8fc7\u4f7f\u7528\u7684\u662f\u4e0d\u540c\u7684basis\uff0c\u56e0\u6b64determinant\u76f8\u540c\u3002 </p> <p></p> <p>\u7531\u4e8e\u4e0a\u9762\u6700\u540e\u4e09\u6761\u6027\u8d28\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u6c42determinant\u3002 \u5148\u5c06\u77e9\u9635\u53d8\u6210row-echelon form\uff0c\u5f53\u77e9\u9635\u53d8\u6210triangular form\u65f6\uff0c\u518d\u5229\u7528\u5bf9\u89d2\u7ebf\u4e58\u79ef\u3002 </p> <p>\u4e0b\u9762\u8fd9\u4e09\u4e2a\u8bf4\u7684\u662f\u4e00\u56de\u4e8b\uff0c\u90fd\u662f\u5728\u8bb2\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\u5411\u91cf\u7a7a\u95f4\u7684dimension\u6ca1\u6709\u964d\u4f4e\u3002 \\(det(A)\\neq 0\\iff rk(A)=n \\iff A\\,is \\,invertible\\). </p> <p>trace\uff1a trace=\u65b9\u9635\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e4b\u548c\u3002   trace\u7684\u6027\u8d28\uff1a </p> <p>\u4e0a\u9762\u7684\u6700\u540e\u4e00\u5929\u6211\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u4e2a\u77e9\u9635\u8fde\u4e58\u7684trace\uff1a \u53ea\u8981\u5c06\u77e9\u9635\u7684\u987a\u5e8f\u8fdb\u884ccyclic permutations\uff0ctrace\u90fd\u4fdd\u6301\u4e0d\u53d8\u3002  \u7531\u6b64\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u7ed3\u8bba\uff1a x\u548cy\u90fd\u662fn\u7ef4\u5411\u91cf\uff0c\\(xy^T\\)\u672c\u8eab\u662f\u4e00\u4e2a<code>n*n</code>\u65b9\u9635\uff0c\u4f46\u8fd9\u4e2a\u65b9\u9635\u7684trace\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6c42\\(tr(x^Ty)\\)\uff0c\u7b49\u4e8ex\u548cy\u7684dot product \\(x^Ty\\)\u3002 </p> <p>\u63a5\u4e0b\u6765\u8ba8\u8bbabasis\u7684\u9009\u62e9\u4f1a\u4e0d\u4f1a\u5f71\u54cdtrace\u7684\u503c\uff1a chap2.7.2 \u63d0\u5230\uff0c\u5bf9\u7ebf\u6027\u53d8\u6362A\u8fdb\u884cbasis\u53d8\u6362\uff0c\u53ef\u4ee5\u5f97\u5230\\(S^{-1}AS\\) \uff0c\u7136\u540e\u5c31\u6709  \u8bf4\u660ebasis\u7684\u9009\u62e9\u4e0d\u4f1a\u5f71\u54cdtrace\u3002</p> <p>\u501f\u52a9determinant\u548ctrace\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u5c06\u4e00\u4e2a\u77e9\u9635\u8868\u793a\u4e3a\u4e00\u4e2a\u591a\u9879\u5f0f\uff0c\u8fd9\u4e2a\u591a\u9879\u5f0f\u4e4b\u540e\u4f1a\u7ecf\u5e38\u7528\u5230 Taking together our understanding of determinants and traces we can now define an important equation describing a matrix A in terms of a polynomial, Characteristic Polynomial  \u5176\u4e2d </p> <p>\u8fd9\u5f0f\u5b50\u5b8c\u5168\u4e0d\u77e5\u9053\u548b\u6765\u7684\u3002</p>"},{"location":"MML/chap4/#42-eigenvalues-and-eigenvectors","title":"4.2 Eigenvalues and Eigenvectors","text":"<p>As we will see, the eigenvalues of a linear mapping will tell us how a special set of vectors, the eigenvectors, is transformed by the linear mapping.</p> <p>\u5b9a\u4e49\uff1a </p> <p>\u5728\u5f88\u591a\u5730\u65b9\uff0c\u90fd\u662f\u9ed8\u8ba4eigenvalue\u65f6\u964d\u5e8f\u6392\u5217\u7684\uff0c\u4f46\u672c\u4e66\u6ca1\u6709\u8fd9\u4e2a\u5047\u8bbe\u3002 </p> <p>\u4e0b\u9762\u8fd9\u4e9b\u7ed3\u8bba\u90fd\u662f\u7b49\u4ef7\u7684\uff1a </p> <p>Collinearity and Codirection \u4e24\u5411\u91cf\u65b9\u5411\u76f8\u540c\u79f0\u4e3acodirection\uff0c \u4e24\u5411\u91cf\u65b9\u5411\u76f8\u540c\u6216\u76f8\u53cd\u79f0\u4e3acollinear\u3002 </p> <p>\u5047\u5982x\u65f6A\u7684eigenvector\uff0c\u5219\u4e0ex collinear\u7684\u5411\u91cf\u90fd\u662fA\u7684eigenvector\u3002 </p> <p>\\(\\lambda\\)\u662feigenvalue&lt;=&gt;\\(\\lambda\\)\u662fcharacteristic polynomial\u7684\u4e00\u4e2aroot\u3002\uff08\u4e0d\u77e5\u9053\u6709\u5565\u7528\uff09  =&gt; \u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50characteristic polynomial \u662f\u7528\u6765\u5177\u4f53\u8ba1\u7b97eigenvalue\u7684\u3002 \u6211\u4eec\u4e0d\u9700\u8981\u8bb0\u4f4f\u90a3\u4e00\u957f\u4e32\uff0c\u53ea\u9700\u8981\u8bb0\u4f4f\\(p_A(\\lambda)=det(A-\\lambda I)\\). \u7136\u540e\u56e0\u4e3a\u8fd9\u4e2a\u5f0f\u5b50\u7684root\u662feigenvalue\uff0c\u6240\u4ee5\u6211\u4eec\u4ee4\\(det(A-\\lambda I)=0\\)\uff0c\u7136\u540e\u5c31\u80fd\u89e3\u51fa\u6240\u6709\u7684\\(\\lambda\\).</p> <p>\u7136\u540e\u8fd9\u91cc\u5b9a\u4e49\u4e86\u4e00\u4e2aalgebraic multiplicity\uff0c\u4e5f\u4e0d\u77e5\u9053\u662f\u5e72\u561b\u7684\u3002 </p> <p>\u4e00\u4e2aeigenvalue\u5bf9\u5e94\u5f88\u591a\u4e2aeigenvector\uff0c\u6211\u4eec\u5c06\u67d0\u4e00\u4e2aeigenvalue \\(\\lambda\\)\u6240\u5bf9\u5e94\u7684\u6240\u6709eigenvector\u7684span\u79f0\u4e3aeigenspace of A with respect to \u03bb \u3002\u8bb0\u4f5c\\(E_\\lambda\\). A\u7684\u6240\u6709eigenvalue\uff08\u6ce8\u610f\u4e0d\u662feigenvector\uff09\u6784\u6210\u7684\u96c6\u5408\u79f0\u4e3aeigenspectrum\u6216\u8005spectrum\u3002 </p> <p>\u5982\u679c\\(\\lambda\\)\u662fA\u7684\u4e00\u4e2aeigenvalue\uff0c\u5219\\(E_\\lambda\\)\u662fhomogeneous system of linear equations \\((A-\\lambda I)x=0\\)\u7684solution space\u3002 </p> <p>\u51e0\u4f55\u610f\u4e49\u4e0a\uff0c\u5982\u679c\u4e00\u4e2aeigenvector\u5bf9\u5e94\u7684\u662f\u4e00\u4e2a\u975e\u96f6\u7684eigenvalue\uff0c \u8bf4\u660eeigenvector\u6307\u5411\u7684\u8fd9\u4e2a\u65b9\u5411\u662f\u88ab\u7ebf\u6027\u53d8\u6362\u201cstretch\u201d\u4e86\u7684\uff0c\u800c\u201cstretch\u201d\u7684\u7a0b\u5ea6\u5219\u7531eigenvalue\u6765\u63cf\u8ff0\u3002  \u6211\u7684\u7406\u89e3\uff1a \u89c2\u5bdf\\(Ax=\\lambda x\\)\uff0c\u5728\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\uff0cx\u5e76\u6ca1\u6709\u88ab\u65cb\u8f6c\uff0c\u800c\u662f\u548c\u539f\u6765\u7684x\u6210collinear\u5173\u7cfb\u3002 \u4e5f\u5c31\u662f\u8bf4eigenvector\u4ee3\u8868\u7740\u7ebf\u6027\u53d8\u6362\u5728\u8fd9\u4e2a\u65b9\u5411\u53ea\u6709\u4f38\u7f29\uff0c\u6ca1\u6709\u65cb\u8f6c\u3002</p> <p></p> <p>\u4e00\u4e9b\u5e38\u7528\u6027\u8d28\uff1a - transpose\u4e4b\u540eeigenvalue\u4e0d\u53d8\uff0c\u4f46eigenvector\u53ef\u80fd\u6539\u53d8\u3002 - eigenspace \\(E_\\lambda\\) is the null space of \\(A-\\lambda I\\)\u3002 </p> <ul> <li>similar matrices\u7684eigenvalue\u76f8\u540c\u3002 \u56e0\u4e3asimilar matrices\u672c\u8eab\u63cf\u8ff0\u7684\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u800ceigenvalues\u8bb0\u5f55\u7684\u662f\u53ea\u53d7\u5230\u201cstretch\u201d\u7684\u65b9\u5411\u88ab\u201cstretch\u201d\u7684\u7a0b\u5ea6\uff0c\u800c\u8fd9\u4e2a\u201c\u7a0b\u5ea6\u201d\u4e0ebasis\u9009\u62e9\u65e0\u5173\u3002\u6240\u4ee5eigenvalue\u76f8\u540c\u3002 \u7531\u6b64\u6211\u4eec\u5f97\u5230\u4e86\u4e09\u4e2a\u4e0d\u968fbasis change\u800c\u6539\u53d8\u7684\u91cf\uff1a eigenvalues\u3001determinant\u548ctrace\u3002\u7528\u5b83\u4eec\u6765\u63cf\u8ff0\u7ebf\u6027\u53d8\u6362\u672c\u8eab\u7684\u6027\u8d28\u3002 </li> <li>Symmetric, positive definite matrices\u7684eigenvalue\u4e00\u5b9a\u5927\u4e8e0  </li> </ul> <p>\u5177\u4f53\u8ba1\u7b97eigenvalues\uff0ceigenvectors\u7684\u8fc7\u7a0b\uff1a  </p> <p>\u67d0\u4e00\u4e2aeigenvalue\u5bf9\u5e94\u7684linear independent eigenvector\u7684\u4e2a\u6570\uff0c\u5c31\u662f\u8fd9\u4e2aeigenvalue\u7684geometric multiplicity\u3002 \u4e5f\u5c31\u662f\u8bf4geometric multiplicity\u5c31\u662f\u8fd9\u4e2aeigenvalue\u5bf9\u5e94\u7684eigenspace\u7684dimensionality\u3002 </p> <p>geometric multiplicity\u4e0d\u4f1a\u8d85\u8fc7algebraic multiplicity\u3002 An eigenvalue\u2019s geometric multiplicity cannot exceed its algebraic multiplicity, but it may be lower. </p> <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4ee3\u8868\u65cb\u8f6c\u64cd\u4f5c\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u56e0\u4e3a\u6ca1\u6709\u54ea\u4e00\u4e2a\u65b9\u5411\u662f\u53ea\u7ecf\u8fc7\u201cstretch\u201d\u800c\u6ca1\u6709\u7ecf\u8fc7\u65cb\u8f6c\u7684\uff0c\u56e0\u6b64\u8fd9\u6837\u7684\u77e9\u9635\u53ea\u6709complex eigenvalue\uff0c\u800c\u6ca1\u6709\u5b9e\u6570eigenvalue\u3002 </p> <p>\u5982\u679c\u4e00\u4e2a\u77e9\u9635\u7684eigenvalues\u5404\u4e0d\u76f8\u540c\uff0c\u5219\u5176\u5bf9\u5e94\u7684eigenvectors\u7ebf\u6027\u72ec\u7acb\u3002 \u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e9beigenvector\u53ef\u4ee5\u89c6\u4f5cbasis\u3002 </p> <p>Defective matrix: \u8fd9\u4e2a\u6982\u5ff5\u8fd8\u633a\u91cd\u8981\u3002 \u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5b83\u7ebf\u6027\u72ec\u7acb\u7684eigenvector\u4e2a\u6570\u5c0f\u4e8en\uff0c\u6216\u8005\u8bf4\u5b83\u7684dimension of eigenspaces\u4e4b\u548c\u5c0f\u4e8en\u3002  non-defective matrix\u4e0d\u4e00\u5b9a\u6709n distinct eigenvalues\u3002 \uff08\u4e5f\u5c31\u662f\u8bf4 \\(n\\,distinct\\,eigenvalues \\Rightarrow n\\, linear\\, independent\\, eigenvectors\\). \\(n\\, linear\\, independent\\, eigenvectors \\nRightarrow n\\,distinct\\,eigenvalues\\). \uff09 \u4f46defective matrix\u4e00\u5b9a\u6ca1\u6709n distinct eigenvalues\u3002</p> <p>\u4efb\u610f\u77e9\u9635\u7684\\(A^TA\\)\u4e00\u5b9a\u662fsymmetric, positive semidefinite\u7684\u3002 \u5f53rk(A)=n\u65f6\uff0c\\(A^TA\\)\u4e00\u5b9a\u662fsymmetric, positive definite\u7684\u3002\uff08positive definite\u7684\u90e8\u5206\u4e4b\u524d\u8bc1\u660e\u8fc7\u4e86\uff09  \u8bc1\u660e\uff1a </p> <p>Spectral Theorem \u5bf9\u4e8esymmetric\u77e9\u9635A\uff0c\u4e00\u5b9a\u5b58\u5728\u4e00\u7ec4orthonormal \u7684eigenvectors\u4f5c\u4e3abasis\uff0c\u800c\u4e14\u6240\u6709eigenvalue\u90fd\u4e3a\u5b9e\u6570\u3002 </p> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c symmetric\u77e9\u9635\u7684eigendecomposition\u4e00\u5b9a\u5b58\u5728\u3002 \u6211\u4eec\u53ef\u4ee5\u5c06A\u5206\u89e3\u4e3a\\(A=PDP^T\\), where D is diagonal and the columns of P contain the eigenvectors.  \u6211\u7684\u7406\u89e3\uff1a \\(A=PDP^T\\)\u8fd9\u4e2a\u5f0f\u5b50\u5176\u5b9e\u5c31\u662fchap2\u77e9\u9635\u76f8\u4f3c\u63d0\u5230\u7684basis\u53d8\u6362\\(\\hat{A}=S^{-1}AS\\)\uff0c\u8fd9\u91cc\u8981\u6c42S regular\u3002 P\u7684columns\u662forthonormal\u7684eigenvectors\uff0c\u56e0\u6b64P\u662forthogonal matrix\uff0cP regular\uff0c\u56e0\u4e3aP\u7684column\u5df2\u7ecforthogonal\u4e86\u5c31\u4e00\u5b9alinear independent\u3002\u6b64\u5916\u56e0\u4e3aP\u662forthogonal\u7684\uff0c\u6709\\(P^T=P^{-1}\\). \u8fd9\u65f6\u5bf9\u6bd4\\(A=PDP^T\\)\u548c\\(\\hat{A}=S^{-1}AS\\)\uff0c\u53ef\u4ee5\u53d1\u73b0\u5c31\u662f\u4e00\u56de\u4e8b\u3002 P\uff08eigenvectors\uff09\u63cf\u8ff0\u7684\u5176\u5b9e\u76f8\u5f53\u4e8e\u662f\u4e00\u4e2abasis change\uff0c\u800cA\u548cD\u662fsimilar\u7684\uff0c\u672c\u8d28\u4e0a\u63cf\u8ff0\u7684\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\u3002</p> <p>\u4e0b\u9762\u8ba8\u8bba\u4e00\u4e0beigenvalue, trace, determinant\u8fd9\u4e09\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002  determinant\u4e0d\u968fbasis\u53d8\u6362\u800c\u6539\u53d8\uff0cA\u548cD\u63cf\u8ff0\u7684\u53c8\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u90a3\u4e48det(A)=det(D)\u3002 \u800cD\u662fdiagonal\u77e9\u9635\uff0c\u7b97\u51fa\u6765\u7684det(D)\u5c31\u7b49\u4e8e\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u3002</p> <p> trace\u4e5f\u4e0d\u968fbasis\u53d8\u6362\u800c\u6539\u53d8\uff0c\u90a3\u4e48tr(A)=tr(D)\u3002\u800ctr(D)\u5bf9\u89d2\u7ebf\u4e0a\u7684\u5143\u7d20\u5c31\u662feigenvalue\uff0c\u56e0\u6b64\u5f97\u5230\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u3002</p>"},{"location":"MML/chap4/#43-cholesky-decomposition","title":"4.3 Cholesky Decomposition","text":"<p>Cholesky Decomposition\u53ef\u4ee5\u7c7b\u6bd4\u4e8e\u5b9e\u6570\u7684\u5f00\u6839\u53f7\u8fd0\u7b97\u3002</p> <p>\u6ce8\u610f\u8fd9\u91cc\u8981\u6c42\u662f\u6b63\u5b9a\u77e9\u9635\uff0c\u800c\u4e0d\u662f\u534a\u6b63\u5b9a\u77e9\u9635\u3002 \u5bf9\u79f0\u7684\u6b63\u5b9a\u77e9\u9635\u53ef\u4ee5\u5206\u89e3\u4e3a\\(A=LL^T\\)\uff0c\u5176\u4e2dL\u662f\u4e00\u4e2alower triangular\u77e9\u9635\u3002 \u800c\u4e14L\u552f\u4e00\u786e\u5b9a\u3002 </p> <p>\u5177\u4f53\u8ba1\u7b97\uff1a \u5199\u51fa\u5bf9\u5e94\u5173\u7cfb\u89e3\u65b9\u7a0b\u5c31\u597d\uff0c\u4e0d\u600e\u4e48\u91cd\u8981\u3002  </p> <p>Cholesky decomposition\u5728\u5f88\u591a\u51fa\u73b0\u4e86symmetric positive definite matrices\u7684\u573a\u666f\u90fd\u6709\u5e94\u7528\u3002 - the covariance matrix of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive definite. The Cholesky factorization of this covariance matrix allows us to generate samples from a Gaussian distribution - It also allows us to perform a linear transformation of random variables, which is heavily exploited when computing gradients in deep stochastic models, such as the variational auto-encoder - The Cholesky decomposition also allows us to compute determinants very efficiently. -  Cholesky decomposition\u53ef\u4ee5\u7528\u4e8e\u6c42determinant\u3002 </p>"},{"location":"MML/chap4/#44-eigendecomposition-and-diagonalization","title":"4.4 Eigendecomposition and Diagonalization","text":"<p>diagonal matrix</p> <p></p> <p>determinant\uff1aproduct of its diagonal entries a matrix power \\(D^k\\)\uff1aeach diagonal element raised to the power k the inverse \\(D^{-1}\\)\uff1a\u5bf9\u89d2\u7ebf\u5143\u7d20\u53d6\u5012\u6570 </p> <p>\u77e9\u9635\u7684Diagonalization\u662fbasis change we discussed in Section 2.7.2 and eigenvalues from Section 4.2\u7684\u7efc\u5408\u5e94\u7528\u3002</p> <p>\u4e4b\u524d\u63d0\u5230\u8fc7\u5982\u679c\u6709\\(D=P^{-1}AP\\)\uff0c\u5219A\u4e0eD similar\u3002 \u73b0\u5728\u6211\u4eec\u53ea\u5173\u6ce8D\u662fdiagonal matrix\u3001\u800c\u4e14\u5bf9\u89d2\u7ebf\u4e0a\u7684\u5143\u7d20\u90fd\u662fA\u7684eigenvalue\u7684\u60c5\u51b5\u3002</p> <p></p> <p>\u4e0b\u9762\u6765\u8bc1\u660ediagonalization\u672c\u8d28\u5c31\u662fbasis change\uff0c\u4e14\u9009\u62e9\u7684basis\u662fA\u7684eigenvectors\u3002 </p> <p>\u4e0a\u9762\u5bf9\u4e8ediagonalization\u7684\u5b9a\u4e49\uff0c\u5e76\u6ca1\u6709\u63d0\u5230eigenvalue\u548ceigenvector\u3002\u4e0b\u9762\u6765\u8bc1\u660ediagonalization\u5e26\u6765\u7684\u5fc5\u7136\u662feigenvalue\u548ceigenvector\u3002  \u5c06\\(AP=PD\\)\u5c55\u5f00\uff0c\u7136\u540e\u4ee4\u7b49\u5f0f\u4e24\u8fb9\u5bf9\u5e94\u76f8\u7b49\uff0c\u6b63\u597d\u80fd\u5f97\u5230\u5f88\u591a\u7ec4\\(Ap_i=\\lambda_i p_i\\)\uff0c\u56e0\u6b64\u89e3\u5f97\u7684\u5411\u91cf\u662feigenvector\uff0c\u89e3\u5f97\u7684\\(\\lambda\\)\u662feigenvalue\u3002  \u8fd9\u91cc\u6211\u4eec\u8981\u6c42P is invertible\uff0c\u4e5f\u5c31\u662fP is full rank\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e9beigenvectors \\(p_i\\)\u7ebf\u6027\u72ec\u7acb\uff0c\u662f\u4e00\u7ec4basis\u3002</p> <p>\u90a3\u4ec0\u4e48\u6837\u7684A\u624d\u662fdiagonalizable\u7684\u5462\uff1f \u6839\u636e\u4e0b\u9762\u7684\u5b9a\u7406\uff0cA is diagonalizable\u5f53\u4e14\u4ec5\u5f53A\u7684eigenvectors\u7ebf\u6027\u72ec\u7acb\u3002 </p> <p>eigenvectors\u7ebf\u6027\u72ec\u7acb\u4e5f\u5c31\u662f\u8bf4A\u662fnon-defective\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\u53ea\u6709non-defective\u624d\u80fd\u88abdiagonalize\u3002 </p> <p>\u800c\u524d\u9762\u63d0\u5230\uff0c\u6839\u636espectum theorem\uff0csymmetric\u77e9\u9635\u7684eigenvector\u4e00\u5b9aorthonormal\u3002\u56e0\u6b64symmetric\u77e9\u9635\u4e00\u5b9a\u80fd\u88abdiagonalized. </p> <p>Diagonalization\u7684\u51e0\u4f55\u610f\u4e49\uff1a A\u548cD\u63cf\u8ff0\u7684\u662f\u540c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c \\(e_i\\)\u548c\\(p_i\\)\u662f\u4e24\u7ec4\u4e0d\u540c\u7684basis\uff0c\u5176\u4e2d\\(p_i\\)\u548cD\u642d\u914d\u4f7f\u7528\u65f6\uff0c\u53ef\u4ee5\u770b\u5230\u7ecf\u8fc7D\u53d8\u6362\u4e4b\u540e\uff0c\u6cbf\u7740\\(p_i\\)\u7684\u65b9\u5411\u90fd\u662f\u53ea\u6709\u88ab\u4f38\u7f29\uff0c\u800c\u6ca1\u6709\u65cb\u8f6c\u3002 \\(P\\)\u548c\\(P^{-1}\\)\u90fd\u53ea\u662fbasis change\uff0c\u53ea\u8d77\u5230\u65cb\u8f6c\u7684\u4f5c\u7528\u3002 </p> <p>Eigendecomposition\u7684\u5177\u4f53\u8ba1\u7b97\uff1a  </p> <p>\u4e00\u4e9b\u6027\u8d28\uff1a - \u7ecf\u8fc7eigendecomposition\uff0c\u53ef\u4ee5\u7b80\u5316\u77e9\u9635\u5e42\u8fd0\u7b97\u7684\u8ba1\u7b97\u91cf\u3002 - \u7b80\u5316determinant\u7684\u8ba1\u7b97\u91cf   \uff08P\u662fbasis change\uff0c\u4ee3\u8868\u65cb\u8f6c\uff0c\u56e0\u6b64\u7ebf\u6027\u53d8\u6362\u5148\u540esigned volume\u4e0d\u53d8\uff0cdeterminant\u4e3a1\uff09.</p>"},{"location":"MML/chap4/#45-singular-value-decomposition","title":"4.5 Singular Value Decomposition","text":"<p>SVD\u5f88\u901a\u7528\uff0c\u56e0\u4e3a\u4efb\u4f55\u65f6\u5019\u90fd\u5b58\u5728\u3002 SVD has been referred to as the \u201cfundamental theorem of linear algebra\u201d (Strang, 1993) because it can be applied to all matrices, not only to square matrices, and it always exists. \u800c\u4e14SVD\u53ef\u4ee5\u7528\u6765quantifies the change between the underlying geometry of these two vector spaces\u3002 </p> <p>SVD Theorem \\(A=U\\Sigma V^T\\)\uff0c \u5176\u4e2dU\u662f<code>m*m</code>orthogonal \u65b9\u9635\uff0cV\u662f<code>n*n</code>orthogonal \u65b9\u9635\u3002 \\(\\Sigma\\)\u662f<code>m*n</code>diagonal\u77e9\u9635\u4e14\u5bf9\u89d2\u7ebf\u5143\u7d20\u975e\u8d1f\uff08\u5c3d\u7ba1\u4e0d\u4e00\u5b9a\u662f\u65b9\u9635\uff09\u3002</p> <p> \u5176\u4e2d\\(\\Sigma\\)\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u88ab\u79f0\u4e3asingular values\uff0c \\(u_i\\)\u79f0\u4e3aleft-singular vectors\uff0c \\(v_j\\)\u79f0\u4e3aright-singular vectors\u3002  singular matrix \\(\\Sigma\\)\u552f\u4e00\u786e\u5b9a\u3002  \u6ce8\u610f\\(\\Sigma\\)\u662f<code>m*n</code>\u7684\uff0c\u4e0eA\u7684shape\u76f8\u540c\u3002 \u7531\u4e8e\u4e0d\u4e00\u5b9a\u662f\u65b9\u9635\uff0c\\(\\Sigma\\)\u53ef\u80fd\u957f\u8fd9\u6837\uff1a  \u6216\u8005\u662f\u8fd9\u6837\uff1a </p> <p>\u6240\u6709\u77e9\u9635\u90fd\u5b58\u5728SVD\u3002 </p>"},{"location":"MML/chap4/#451-geometric-intuitions-for-the-svd","title":"4.5.1 Geometric Intuitions for the SVD","text":"<p>we will discuss the SVD as sequential linear transformations performed on the bases.</p> <p>SVD\u7684\u601d\u8def\u8ddfeigendecomposition\u5f88\u7c7b\u4f3c\u3002  - basis change via \\(V^T\\). - scaling and augmentation (or reduction) in dimensionality via the singular value matrix \\(\\Sigma\\). - a second basis change via \\(U\\).</p> <p> </p> <p>SVD\u5728V\u548cW\u90fd\u6709basis change\uff0c\u5e76\u4e14\u672c\u8eabV\u548cW\u5c31\u662f\u4e24\u4e2a\u4e0d\u540c\u7684vector space\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0ceigendicomposition\u662f\u5728\u76f8\u540c\u7684\u5411\u91cf\u7a7a\u95f4V\u4e2d\uff0c\u5e76\u4e14\u4f7f\u7528\u4f7f\u7528\u7684\u662f\u540c\u4e00\u7ec4basis\u3002 \u6211\u7684\u7406\u89e3\uff1a \u53c2\u7167\u77e9\u9635\u7b49\u4ef7&amp;\u77e9\u9635\u76f8\u4f3c\u8fd9\u91cc\uff0c\u53ef\u4ee5\u770b\u5230eigendicomposition\u5bf9\u5e94\u7684\u662f\u77e9\u9635\u76f8\u4f3c\uff0cA\u4e0eD\u76f8\u4f3c\uff0c\u5e76\u4e14\u5728\u540c\u4e00\u4e2a\u5411\u91cf\u7a7a\u95f4\u3001\u7ebf\u6027\u53d8\u6362\u524d\u540e\u4f7f\u7528\u7684\u662f\u540c\u4e00\u7ec4basis\u3002 \u800cSVD\u66f4\u50cf\u662fequivalent\u7684\u6761\u4ef6\u8bbe\u7f6e\u3002  </p>"},{"location":"MML/chap4/#452-construction-of-the-svd","title":"4.5.2 Construction of the SVD","text":"<p>We will next discuss why the SVD exists and show how to compute it in detail.</p> <p>\u5bf9\u6bd4eigendecompositionh\u548cSVD\u7684\u5f0f\u5b50\uff0c   \u5982\u679c\u6211\u4eec\u4ee4  \u5219SVD\u5c31\u662feigendecomposition\u3002</p> <p>\u63a5\u4e0b\u6765\u8bf4\u660eSVD\u4e3a\u4ec0\u4e48\u5b58\u5728\u3002</p> <p>\u601d\u8def\uff1a </p> <p>step 1\uff1a \u5df2\u77e5symmetric\u77e9\u9635\u4e00\u5b9a\u53ef\u4ee5diagonalize\uff0c \u53c8\u77e5\u4efb\u610f\u77e9\u9635A\u7684\\(A^TA\\)\u4e00\u5b9asymmetric positive semidefinite\u3002 \u6240\u4ee5\\(A^TA\\)\u4e00\u5b9a\u53ef\u4ee5diagonalize\u3002 \u5f97\u5230\uff1a </p> <p>\u5047\u8bbeA\u7684SVD\u5b58\u5728\uff0c\\(A=U\\Sigma V^T\\)\uff0c \u5219  \u5bf9\u6bd4\u4e0a\u9762\u4e24\u4e2a\u5f0f\u5b50\uff0c </p> <p></p> <p>\u53ef\u4ee5\u5f97\u5230\uff1a \u5bf9\\(A^TA\\)\u8fdb\u884ceigendecomposition\uff0c \u5f97\u5230\u7684eigenvector\u5c31\u662fA\u7684right-singular vectors V\uff0c \u5f97\u5230\u7684eigenvalue\u5c31\u662fA\u7684singular value\\(\\Sigma\\)\u7684\u5e73\u65b9\u3002</p> <p>step2\uff1a \u540c\u7406\uff0c\u5bf9\\(AA^T\\)\u8fdb\u884c\u76f8\u540c\u7684\u64cd\u4f5c\uff0c\u53ef\u4ee5\u5f97\u5230 \uff08\u6ce8\u610f\u8fd9\u91cc\\(A^TA\\)\u662f<code>n*n</code>\u7684\uff0c\\(AA^T\\)\u662f<code>m*m</code>\u7684\uff0c\uff09  \u5bf9\\(AA^T\\)\u8fdb\u884ceigendecomposition\uff0c \u5f97\u5230\u7684eigenvector\u5c31\u662fA\u7684left-singular vectors V\uff0c \u5f97\u5230\u7684eigenvalue\u5c31\u662fA\u7684singular value\\(\\Sigma\\)\u7684\u5e73\u65b9\u3002</p> <p>step3\uff1a  \u4e0a\u9762\u4e24\u6b65\u4e2d\u7528\u5230\u7684\\(\\Sigma\\)\u4e00\u5b9a\u662f\u76f8\u540c\u7684\u3002 \uff08\u8fd9\u4e00\u6b65\u6ca1\u770b\u61c2\u6709\u5565\u7528\uff09</p> <p>step4\uff1a \uff08\u8fd9\u6b65\u4e2d\u95f4\u6709\u4e9b\u7ec6\u8282\u4e0d\u61c2\uff0c\u5c31\u8fd9\u6837\u5427\u3002\uff09 \u73b0\u5728\u6211\u4eec\u501f\u52a9\u5bf9\\(A^TA\\)\u548c\\(AA^T\\)\u7684eigendecomposition\uff0c\u5f97\u5230\u4e86A\u7684\\(U\\)\u3001\\(V^T\\)\u548c\\(\\Sigma\\)\u3002\u5982\u679c\u53ea\u662f\u53ea\u662f\u4e3a\u4e86\u6c42A\u7684SVD\uff0c\u90a3\u6211\u4eec\u73b0\u5728\u5df2\u7ecf\u7b97\u5b8c\u4e86\u3002 \u4f46\u6211\u4eec\u73b0\u5728\u53e6\u5916\u518d\u6765\u63a2\u7d22\u4e00\u4e0bU\u548cV\u7684\u5173\u7cfb\u3002</p> <p>\u9996\u5148\u6211\u4eec\u89c2\u5bdf\u5230\\(Av_j\\)\u672c\u8eab\u5c31\u662forthogonal\u7684\u3002 </p> <p>\u4f46\u6211\u4eec\u60f3\u8ba9U\u662forthonormal\u7684\uff0c\u56e0\u6b64\u5c31\u5bf9\\(Av_j\\)\u9664\u4ee5\u5176norm\u5f97\u5230\u5355\u4f4d\u5411\u91cf\uff1a  \u81f3\u6b64\u5f97\u5230u\u548cAv\u7684\u6570\u503c\u5173\u7cfb\uff1a </p> <p>\u4e5f\u5c31\u662f  \u8fd9\u4e2a\u5f0f\u5b50\u5f88\u50cfeigenvalue equation\uff0c\u4f46\u662f\u6ce8\u610f\u7b49\u5f0f\u4e24\u8fb9\u7684\u7684\u5411\u91cf\u662f\u4e0d\u540c\u7684\u3002</p> <p>\u6ce8\u610f\u4e0a\u9762\u5f0f\u5b50\u4e2d\\(v_i\\)\u662fn\u7ef4\u3002\\(u_i\\)\u662fm\u7ef4\u3002\u5982\u679c\u53ea\u770b\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u7684\u8bdd\uff0c\u611f\u89c9\\(v_i\\)\u548c\\(u_i\\)\u4e2d\u7ef4\u5ea6\u66f4\u9ad8\u7684\u90a3\u4e00\u65b9\uff0c\u53ea\u6709\u4e00\u90e8\u5206\u503c\u53d7\u5230\u4e86\u7ea6\u675f\u3002 \u6bd4\u5982n=6\uff0c m=4\uff0c\u5411\u91cf\\(v_i\\)\u6bd4\\(u_i\\)\u957f\uff0c\u4e0a\u9762\u7684\u5f0f\u5b50\u53ea\u5c55\u793a\u4e86\u5411\u91cf\\(v_i\\)\u7684\u524d4\u7ef4\u7b49\u4e8e\u5565\uff0c\u5e76\u6ca1\u8bf4\u540e2\u7ef4\u9700\u8981\u6ee1\u8db3\u4ec0\u4e48\u6761\u4ef6\u3002\u4f46\u7ecf\u8fc7\u4e4b\u524d\u7684SVD\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u6211\u4eec\u77e5\u9053\\(v_i\\)\u672c\u8eab\u8fd8\u9700\u8981\u662forthonormal\u7684\u3002</p> <p></p>"},{"location":"MML/chap4/#453-eigenvalue-decomposition-vs-singular-value-decomposition","title":"4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition","text":"<p>Eigenvalue Decomposition\u548cSVD\u7684\u5bf9\u6bd4\uff1a</p> <ul> <li>SVD\u59cb\u7ec8\u5b58\u5728\u3002eigendecomposition\u8981\u6c42\u77e9\u9635\u662f\u65b9\u9635\uff0c\u4e14eigenvectors\u662fn\u7ef4\u7a7a\u95f4\u7684basis\u3002</li> <li>eigendecomposition\u7684eigenvector\u4e0d\u4e00\u5b9aorthogonal\u3002SVD\u7684U\u548cV\u4e2d\u7684\u5411\u91cf\u4e00\u5b9a\u662forthonormal\u7684\uff0c\u56e0\u6b64\u63cf\u8ff0\u7684\u4e00\u5b9a\u662f\u65cb\u8f6c\u64cd\u4f5c\u3002</li> <li>SVD\u7684U\u548cV\u4e4b\u95f4\u4e00\u822c\u6ca1\u6709inverse\u5173\u7cfb\uff08\u800c\u4e14\u53ef\u80fd\u8fdeshape\u90fd\u4e0d\u4e00\u6837\uff09\u3002eigendecomposition\u7684\\(p\\)\u548c\\(P^{-1}\\)\u6210inverse\u5173\u7cfb\u3002</li> <li>SVD\u7684\\(\\Sigma\\)\u4e00\u5b9a\u662f\u5b9e\u6570\u4e14\u975e\u8d1f\u3002eigendecomposition\u4e2d\u7684eigenvalue\u4e0d\u4e00\u5b9a\u3002</li> <li>\u5bf9\u4e8esymmetric\u77e9\u9635\uff0ceigendecomposition\u548cSVD\u76f8\u540c\u3002  </li> </ul> <p>SVD\u76f8\u5173\u7684\u4e00\u4e9bterminology and conventions\uff1a</p> <p> SVD\u7684\u53e6\u4e00\u79cd\u5b9a\u4e49\u65b9\u6cd5\uff1a \\(\\Sigma\\)\u662f\u65b9\u9635\uff0cU\u548cV\u662f\u77e9\u5f62\u3002  </p> <p></p>"},{"location":"MML/chap4/#46-matrix-approximation","title":"4.6 Matrix Approximation","text":"<p>\u8fd9\u90e8\u5206\u7b49\u54ea\u5929\u6709\u5fc3\u60c5\u4e86\u518d\u770b\uff0c\u5148\u770bchap5</p>"},{"location":"MML/chap5/","title":"chap5 - Vector Calculus","text":"<p>\u672c\u4e66\u8ba8\u8bba\u7684function\u901a\u5e38input\u4e3a\\(x\\in \\mathbb{R}^D\\)\uff0coutput\u4e3af(x)\u3002</p> <p>\\(\\mathbb{R}^D\\)\u79f0\u4e3adomain of f\uff0c\u51fd\u6570\u503c\u7684\u96c6\u5408\u79f0\u4e3aimage/codomain of f\u3002 </p> <p></p> <p></p> <p>Throughout this book, we assume that functions are differentiable. </p>"},{"location":"MML/chap5/#51-differentiation-of-univariate-functions","title":"5.1 Differentiation of Univariate Functions","text":"<p> The derivative of f points in the direction of steepest ascent of f.</p>"},{"location":"MML/chap5/#511-taylor-series","title":"5.1.1 Taylor Series","text":"<p>  \u5176\u4e2d\u8fd9\u4e2a\\(f\\in C^{\\infty}\\)\u662f\u65e0\u9650\u6b21\u53ef\u5fae\u7684\u610f\u601d\u3002 </p> <p>\u901a\u5e38\uff0cTaylor polynomial of degree n\u662f\u5bf9\u4efb\u610f\u51fd\u6570f\u7684approximation\u3002\u8fd9\u91cc\u5f3a\u8c03f\u662fpolynomial\u662f\u56e0\u4e3a\u591a\u9879\u5f0f\u5728\u6709\u9650\u6b21\u53ef\u5fae\u4e4b\u540e\u5bfc\u6570\u5c31\u4f1a\u53d8\u62100\uff0c\u8fd9\u65f6\u5019\u5f53n\u8db3\u591f\u5927\uff08\u4e14\u6ca1\u6709\u5fc5\u8981\u53d6\u5230\u6b63\u65e0\u7a77\uff09\u65f6\u5c31\u53ef\u4ee5\u5f97\u5230\u5bf9f\u7684\u7cbe\u786erepresentation\u3002 \u53e6\u5916\u6ce8\u610fTaylor polynomial\u6700\u7ec8\u5f97\u5230\u7684\u662ff\u8fd9\u4e2a\u51fd\u6570\u672c\u8eab\uff0c\u800c\u4e0d\u662f\\(f(x_0)\\)\u8fd9\u4e2a\u51fd\u6570\u503c\u3002</p> <p></p>"},{"location":"MML/chap5/#512-differentiation-rules","title":"5.1.2 Differentiation Rules","text":""},{"location":"MML/chap5/#52-partial-differentiation-and-gradients","title":"5.2 Partial Differentiation and Gradients","text":"<p>The generalization of the derivative to functions of several variables is the gradient.</p> <p></p> <p>Partial Derivative\uff1a \u6ce8\u610f\u8fd9\u91ccgradient\u662f\u4e00\u4e2arow vector\u3002 gradient/jacobian\u7684\u884c\u6570\u662fdomain\u7684\u7ef4\u6570\uff0c\u5217\u6570\u662fcodomain\u7684\u7ef4\u6570\u3002 </p> <p>\u5f88\u591a\u4eba\u5c06gradient vector\u5199\u4f5ccolumn vector\uff0c \u672c\u4e66\u5199\u4f5crow vector\u4e3b\u8981\u662f\u4e3a\u4e86multi-variate chain rule\u65f6\u8ba1\u7b97\u65b9\u4fbf\uff0c\u4e0d\u9700\u8981\u989d\u5916\u7559\u610f\u7ef4\u5ea6\u3002 </p>"},{"location":"MML/chap5/#521-basic-rules-of-partial-differentiation","title":"5.2.1 Basic Rules of Partial Differentiation","text":"<p>  \u4e0a\u9762\u7684\u6027\u8d28\u4f9d\u7136\u6210\u7acb\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u77e9\u9635\u4e58\u6cd5\u6ca1\u6709\u4ea4\u6362\u5f8b\u3002</p>"},{"location":"MML/chap5/#522-chain-rule","title":"5.2.2 Chain Rule","text":"<p> \u6ce8\u610f\u4e0a\u9762\u5f0f\u5b50\u4e2d\uff0cgradient\u7684\u5f62\u72b6\uff0c\u884c\u6570\u5bf9\u5e94\u7684\u65f6codomain\uff0c\u5217\u6570\u5bf9\u5e94\u7684\u662fdomain\u3002 \u53ea\u8981\u4e00\u76f4\u9075\u5b88\u4e0a\u9762\u7684\u539f\u5219\uff0cchain rule\u5c31\u53ef\u4ee5\u7b80\u5355\u5730\u5199\u6210\u77e9\u9635\u8fde\u4e58\uff0c\u800c\u4e0d\u7528\u4e0d\u505c\u5730transpose\u3002 </p>"},{"location":"MML/chap5/#53-gradients-of-vector-valued-functions","title":"5.3 Gradients of Vector-Valued Functions","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u63a8\u5e7f\u5230f\u7684\u8f93\u51fa\u4e0d\u662f\u5b9e\u6570\uff0c\u800c\u662fvector\u7684\u60c5\u51b5\uff1a</p> <p>\u4e00\u4e2a\u8f93\u51fa\u4e3am\u7ef4\u5411\u91cf\u7684f\uff0c\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662fm\u4e2a\u8f93\u51fa\u4e3a\u5b9e\u6570\u7684\u51fd\u6570f\u7ec4\u6210\u7684column vector\uff1a </p> <p>\u7531\u6b64\uff0c\u5411\u91cff\u5bf9\u67d0\u4e00\u4e2ax\u7684\u504f\u5bfc\u4e5f\u53ef\u4ee5\u5199\u4e3acolumn vector\uff1a  \u5411\u91cff\u5bf9\u6240\u6709x\u7684\u504f\u5bfc\u7ec4\u6210\u7684\u5c31\u662f\u4e00\u4e2a\u77e9\u9635\uff1a </p> <p></p> <p>jacobian\u7684\u4e00\u4e2a\u91cd\u8981\u5e94\u7528\u5c31\u662f\u6982\u7387\u8bba\u4e2d\u7684\u53d8\u91cf\u66ff\u6362\uff08Section 6.7\uff09\uff0c\u79f0\u4e3achange-of-variable method\u3002 \u800c\u53d8\u91cf\u66ff\u6362\u5e26\u6765\u7684scaling\u8981\u901a\u8fc7jacobian\u7684determinant\u6765\u63ed\u793a\u3002 The amount of scaling due to the transformation of a variable is provided by the determinant.</p> <p>\u7269\u7406\u610f\u4e49\u4e0a\u7684scaling factor\uff08\u4e0d\u8003\u8651flipping\uff09\uff0c\u5c31\u662fdeterminant\u7684\u7edd\u5bf9\u503c\u3002 \u5728\u6c42\u8fd9\u4e2ascaling factor\u7684\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u4e00\u4e2aunit\u5355\u4f4d\u4f53\u79ef\uff0c\u770b\u7ecf\u8fc7\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\u4f53\u79ef\u53d8\u6210\u4e86\u591a\u5c11\u3002 </p> <p>\u4e0b\u9762\u4ecb\u7ecd\u4e24\u79cd\u65b9\u6cd5\u6765\u6c42\u8fd9\u4e2ascaling factor\u3002\u5047\u8bbe\u6211\u4eec\u60f3perform a variable transformation from \\((b_1, b_2)\\) to \\((c_1, c_2)\\)\uff0c\u8fd9\u4e24\u7ec4\u90fd\u662f2\u7ef4\u7a7a\u95f4\u7684basis.  \u7136\u540e\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u8fd9\u4e24\u7ec4basis\u7684\u53d6\u503c\u5982\u4e0b\u56fe\uff1a </p> <p>Approach 1 \u56e0\u4e3abasis \\((b_1, b_2)\\)\u662fstandard basis\uff0c\u6240\u4ee5basis change matrix\u5c31\u76f4\u63a5\u662f\u53d8\u6362\u540ebasis\u7684\u5750\u6807\uff0c\u4e5f\u5c31\u662f\\((c_1, c_2)\\)\u7ec4\u6210\u7684\u77e9\u9635:   \u56e0\u6b64scaling factor\u5c31\u662fdeterminant\u7684\u7edd\u5bf9\u503c\u3002</p> <p>Approach 2 \u4e0a\u9762\u7684\u65b9\u6cd5\u867d\u7136\u7b80\u5355\uff0c\u4f46\u662fbasis change matrix\u7684\u8fd9\u4e2a\u9009\u6cd5\u53ea\u9002\u7528\u4e8e\u7ebf\u6027\u53d8\u6362\u3002\u56e0\u6b64\u4e0b\u9762\u6765\u4ecb\u7ecd\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u504f\u5bfc\u6570\u6765\u8ba1\u7b97\u3002</p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u4ecevariable transformation\u7684\u89d2\u5ea6\u6765\u770bf\uff0c f\u5c06\u4e00\u7ec4\u76f8\u5bf9\u4e8e\\((b_1, b_2)\\)\u7684\u5750\u6807\u6620\u5c04\u5230\u4e00\u7ec4\u76f8\u5bf9\u4e8e\\((c_1, c_2)\\)\u7684\u5750\u6807\u3002  \u6211\u4eec\u60f3\u89c2\u5bdf\u7ecf\u8fc7f\u53d8\u6362\u524d\u540e\uff0c\u5355\u4f4d\u4f53\u79ef\u7684\u53d8\u5316\u3002\u8bf4\u767d\u4e86\u53ef\u4ee5\u4e0d\u4e25\u8c28\u5730\u8868\u793a\u6210f(volume)/volume\uff0c\u8fd9\u5c31\u8ba9\u4eba\u8054\u60f3\u5230\u659c\u7387\uff0c\u8fdb\u800c\u60f3\u5230\u5bfc\u6570/gradient\u3002 </p> <p>\u7136\u540e\u5c31\u80fd\u5f97\u5230\u4e0b\u9762\u7684\u8ba1\u7b97\uff1a  Jacobian\u5c31\u53cd\u6620\u4e86\u5750\u6807\u7684\u53d8\u6362\u3002 \u5e76\u4e14\u5f53\u53d8\u6362\u65f6\u7ebf\u6027\u53d8\u6362\u7684\u65f6\u5019\uff0cjacobian\u662fexact\u7684\u3002 \u5bf9\u4e8e\u975e\u7ebf\u6027\u53d8\u6362\uff0cthe Jacobian approximates this nonlinear transformation locally with a linear one.</p> <p>\u4e0a\u9762\u7684Jacobian determinant\u548cvariable transformations\u901a\u5e38\u7528\u4e8e\u5bf9random variables and probability distributions\u8fdb\u884c\u53d8\u6362\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u975e\u5e38\u91cd\u8981\u3002 \u5728NN\u7684\u8bed\u5883\u4e0b\uff0c\u901a\u5e38\u4f1a\u7528\u4e8ereparametrization trick\uff0c\u4e5f\u53ebinfinite perturbation analysis\u3002 </p> <p>\u4e0b\u9762\u603b\u7ed3\u4e00\u4e0b\u5f53\u51fd\u6570f\u7684\u8f93\u5165\u8f93\u51fa\u5206\u522b\u662fscalar\u6216vector\u7684\u65f6\u5019\uff0cgradient\u7684\u7ef4\u5ea6\uff1a  </p> <p>\\(f(x)=Ax\\)\uff0c\u5219f\u5bf9x\u7684gradient\u5c31\u662fA\uff1a </p> <p>\u4e0b\u9762\u662f\u4e00\u4e2achain rule\u7684\u4f8b\u5b50\uff1a </p> <p></p>"},{"location":"MML/chap5/#54-gradients-of-matrices","title":"5.4 Gradients of Matrices","text":"<p>We will encounter situations where we need to take gradients of matrices with respect to vectors (or other matrices), which results in a multidimensional tensor.</p> <p>\u77e9\u9635\u5bf9\u77e9\u9635\u6c42\u68af\u5ea6\uff0cJacobian\u7684\u5f62\u72b6\u4ecd\u7136\u9075\u5b88\u4e4b\u524d\u7684\u89c4\u5219\u3002 For example, if we compute the gradient of an m\u00d7 n matrix A with respect to a p \u00d7 q matrix B, the resulting Jacobian would be \\((m\\times n)\\times (p\\times q)\\), i.e., a four-dimensional tensor J, whose entries are given as \\(J_{ijkl} = \\partial A_{ij}/\\partial B_{kl}\\). </p> <p>\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u9700\u8981\u6ce8\u610f \u6709\u65f6\u6211\u4eec\u76f4\u63a5\u8fdb\u884c\u77e9\u9635\u5bf9\u77e9\u9635\u7684\u6c42\u5bfc\uff0c\u5f97\u5230\\((m\\times n)\\times (p\\times q)\\)\u7684jacobian tensor\uff1b \u6709\u65f6\u6211\u4eec\u5c06\u77e9\u9635\u5148reshape\u6210mn\u7ef4\u548cpq\u7ef4\u7684vector\uff0c\u7136\u540e\u6c42\u5bfc\u5f97\u5230\\(mn\\times pq\\)\u7684jacobian matrix\uff08\u7136\u540e\u5982\u679c\u6709\u9700\u8981\u5c31\u518d\u5c06\u5176reshape\u56de\\((m\\times n)\\times (p\\times q)\\)\uff09\u3002\u56e0\u4e3achain rule\u5bf9\u4e8e\u77e9\u9635\u6765\u8bf4\u5c31\u53ea\u662f\u76f4\u63a5\u8fde\u4e58\uff0c\u5bf9\u4e8etensor\u8fd8\u8981\u6ce8\u610f\u7ef4\u5ea6\u5bf9\u4e0d\u5bf9\uff0c\u5bb9\u6613\u51fa\u9519\u3002  </p> <p>\u4e0b\u9762\u662f\u4e24\u4e2a\u4f8b\u5b50\uff0c\u4ed4\u7ec6\u770b\u4e5f\u80fd\u770b\u61c2\uff1a  </p>"},{"location":"MML/chap5/#55-useful-identities-for-computing-gradients","title":"5.5 Useful Identities for Computing Gradients","text":"<p>tensor\u7684trace\u548ctranspose\uff1a the trace of a D\u00d7D\u00d7E\u00d7F tensor would be an E\u00d7F-dimensional matrix. Similarly, when we \u201ctranspose\u201d a tensor, we mean swapping the first two dimensions.</p>"},{"location":"MML/chap5/#56-backpropagation-and-automatic-differentiation","title":"5.6 Backpropagation and Automatic Differentiation","text":""},{"location":"MML/chap5/#561-gradients-in-a-deep-network","title":"5.6.1 Gradients in a Deep Network","text":""},{"location":"MML/chap5/#562-automatic-differentiation","title":"5.6.2 Automatic Differentiation","text":"<p>It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation.</p> <p> </p> <p></p> <p></p>"},{"location":"MML/chap5/#57-higher-order-derivatives","title":"5.7 Higher-Order Derivatives","text":""},{"location":"MML/chap5/#58-linearization-and-multivariate-taylor-series","title":"5.8 Linearization and Multivariate Taylor Series","text":""},{"location":"MML/chap6%20-%20Probability%20and%20Distributions/","title":"Probability and Distributions","text":""},{"location":"NLP/Lecture/HMM/HMM/","title":"HMM","text":"<p>\u9996\u5148\u662ftask\u8bbe\u5b9a\uff0c\u6211\u4eec\u5df2\u6709\u53d8\u957fseq \u201cdata/reference\u201d\uff0c\u7136\u540e\u6709\u4e00\u4e2anew-coming \u53d8\u957fseq \u201cobservation\u201d\u3002 \u73b0\u5728\u6211\u4eec\u60f3\u8ba1\u7b97\u8fd9\u4e24\u4e2a\u53d8\u957fseq\u7684\u8ddd\u79bb\uff08\u6216\u8005\u6cdb\u5316\u5730\u8bb2\uff0clikelihood\\(P(O|\\lambda)\\)\uff0c\\(\\lambda\\)\u8868\u793amodel\uff09\u3002</p> <p>\u4e00\u79cd\u65b9\u6cd5\u662f\uff0c\u7528model\u6765\u5bf9data\u8fdb\u884c\u6982\u62ec\uff0c\u8bf4\u767d\u4e86\u5c31\u662f\u4f7f\u5176\u53d8\u6210\u5b9a\u957fseq\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u53ef\u4ee5\u548cobservation\u8fdb\u884c\u6bd4\u8f83\u3002 - distance based: mean (1 centroid), k-means - prob based: gaussian, GMM \u5176\u4e2dmean\u548cgaussian\u5bf9\u5e94\uff0ck-mean\u548cGMM\u5bf9\u5e94\uff0c\u5206\u522b\u4ee3\u8868\u4f7f\u75281\u4e2avector\u548ck\u4e2avector\u7684\u60c5\u51b5\u3002  \u8fd9\u6837\u505a\u7684\u7f3a\u70b9\u662f\uff0c\u6211\u4eec\u5c06model\u9010\u4e2a\u548cobservation \\(o_i\\)\u6bd4\u8f83\uff0c\u4e5f\u5c31\u662f\u5047\u8bbe\\(o_i\\)\u4e4b\u95f4\u662findependent\u7684\uff0c\u8003\u8651\u5230speech\u3001text\u8fd9\u4e9bseq\u7684\u7279\u6027\uff0c\u8fd9\u4e48\u505a\u4e0d\u597d\u3002\uff08\u4eba\u7684reaction time\u662f100ms\uff0cspeech\u7684frame\u662f~10ms\uff0c\u56e0\u6b64frame\u4e4b\u95f4\u9ad8\u5ea6dependent\u3002\uff09</p> <p>HMM\u7684\u76ee\u7684\uff1a - break the \\(o_i\\) independence\u5047\u8bbe - break the idea of \u201c\u5c06\u6240\u6709known\u53ea\u7528\u4e00\u4e2amodel\u8868\u793a\u201d</p> <p>\u56e0\u4e3a\u73b0\u5728\u6211\u4eec\u60f3\u8ba9data\u4e5f\u662f\u53d8\u957fseq\uff0c\u56e0\u6b64\u5c31\u5b58\u5728\u4e00\u4e2aalignment\u7684\u95ee\u9898\u3002 </p> <p>\u4e0d\u540c\u7684alignment\u53ef\u4ee5\u7528\u4e0d\u540c\u7684path\u8868\u793a\u3002 \u5728\u6bcf\u4e2astep\u8ba1\u7b97data\u548cobservation\u7684distance\uff0c\u5bf9\u4e0d\u540c\u7684path\u8ba1\u7b97sum of distance\uff0c\u7136\u540e\u6311\u4e00\u4e2adistance sum\u6700\u5c0f\u7684\u5f53\u4f5c\u6700\u4f18\u7684alignment\u3002 \u53ef\u80fd\u7684path\u6570\u91cf\u5f88\u5927\uff0c\u8fd9\u91cc\u6839\u636etask\u7684\u7279\u6027\u4e00\u822c\u6709\u4e24\u4e2a\u7ea6\u675f\uff1amonotonic\u3001\u4e0d\u80fdskip\u3002 \u7136\u540e\u8fd9\u4e2aalignment\u7684\u8fc7\u7a0b\u79f0\u4e3a\u201cdynamic time warping\u201d\u3002 \u4e0a\u9762\u8bb2\u7684\u662f\u5b8f\u89c2\u4e0a\u7684reference pattern\uff0c\u4f46\u6211\u4eec\u8fd8\u5f97\u60f3\u529e\u6cd5\u5bf9data\u8fdb\u884c\u8868\u793a\u3002\u56e0\u4e3a\u8981\u7684\u662f\u53d8\u957fseq\uff0c\u4e4b\u524d\u7684GMM\u4e4b\u7c7b\u7684\u5c31\u4e0d\u80fd\u7528\u4e86\uff0c\u9700\u8981\u627e\u4e00\u4e2a\u65b0\u7684\u53ef\u4ee5\u7528\u7684model\u3002\u800c\u9009\u7684\u8fd9\u4e2amodel\u5c31\u662f\u72b6\u6001\u673a\u3002</p> <p> \u6570\u5b66\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06state machine\u770b\u4f5c\u4e00\u4e2anumber seq generator\u3002 \u6211\u4eec\u53ef\u4ee5\u8bb0\u5f55\u4e0b\u6bcf\u4e2a\u89e6\u53d1\u72b6\u6001\u8f6c\u79fb\u7684event\uff0c\u4e5f\u53ef\u4ee5\u8bb0\u5f55\u4e00\u4e0b\u90fd\u7ecf\u5386\u4e86\u54ea\u4e9bstate\uff0c\u90fd\u884c\u3002</p> <p>\u7b80\u5355\u6765\u8bf4\u6211\u4eec\u8981\u7528\u4e00\u4e2aprobabilistic state machine\u3002\u6211\u4eec\u5c06state machine\u7684event\u6539\u6210prob\u7684\u5f62\u5f0f\u3002  \u8fd9\u4e2aprobablistic state machine\u5c31\u53ebmarkov chain\u3002</p> <p>\u4e0b\u9762\u628a\u8fd9\u4e9b\u5199\u6210\u5f0f\u5b50\uff1a  \u5728\u72b6\u6001\u673a\u4e2d\u7684\u6bcf\u4e2astate\uff0c\u6211\u4eec\u53ea\u5173\u5fc3outward prob\uff08\u2461\u80fd\u53bb\u54ea\uff09\uff0c\u800c\u4e0d\u5173\u5fc3\u662f\u4ece\u4ec0\u4e48state\u6765\u5230\u2461\u7684\u3002\u6362\u53e5\u8bdd\u8bf4\u6bcf\u4e2astate\u7684prob\u53eacondition on\u524d\u4e00\u4e2astate\u3002\u6240\u4ee5\u5c31\u6709\u4e86\\(P(x)=\\prod_{t} P\\left(x_{t} \\mid x_{t-1}\\right)\\).</p> <p>\u53e6\u5916\uff0c\u5bf9\u6bd4\u4e4b\u524d\u7684GMM\u4e4b\u7c7b\u7684model\uff0c\u56e0\u4e3a\u4e4b\u524d\u90a3\u4e9b\u5047\u8bbe\\(o_i\\)\u4e4b\u95f4independent\uff0c\u4e5f\u5c31\u662f\u53ea\u5173\u5fc3\\(P(x)=\\prod_{t} P\\left(x_{t}\\right)\\)\uff0c\u5e76\u6ca1\u6709condition on\u8fc7\u53bb\u7684state\u3002\u73b0\u5728\u7528\u4e86state machine\uff08markov chain\uff09\uff0c\u591a\u8003\u8651\u4e86\u4e00\u4e2astate\u3002</p> <p>Gaussian\u548cGMM\u662fgenerative model\uff0c\u53ef\u4ee5individually\u751f\u6210\u6837\u672c\uff0c\u4f46\u4e0d\u80fd\u751f\u6210seq\u3002 \u73b0\u5728\u6211\u4eec\u8981\u5c06state machine\u7684\u601d\u60f3\u548cgaussian\u7ed3\u5408\u8d77\u6765\uff0c\u751f\u6210seq\u3002</p> <p>\u4ee4state machine\u4e2d\u7684\u6bcf\u4e00\u4e2astate\u5bf9\u5e94\u4e00\u4e2agaussian\uff0c\u6709\u5404\u81ea\u7684\u53c2\u6570\uff08\u03bc\u548c\u03c3\uff09\uff0c\u7136\u540e\u7528state machine\u51b3\u5b9a\u4e0d\u540cgaussianzhijiande\u8f6c\u79fb\u3002  \u5f53gaussian\u662fmulti-variate gaussian\u65f6\uff0c\u6bcf\u4e2agaussian\u53ef\u4ee5\u751f\u6210vector\u3002\u7531\u6b64\uff0c\u6211\u4eec\u5c06markov model\u53d8\u6210HMM\uff0c\u5c06number seq generator\u53d8\u6210\u4e00\u4e2avector seq generator\u3002 \u4e4b\u6240\u4ee5hidden\uff0c\u662f\u56e0\u4e3a\u53ea\u770b\u751f\u6210\u5f97\u5230\u7684observation seq\u7684\u8bdd\uff0c\u53ea\u80fd\u77e5\u9053\u5b83\u662f\u7531\u4e00\u4e2agaussian\u751f\u6210\u7684\uff0c\u4f46\u6211\u4eec\u4e0d\u80fd\u770b\u51fa\u5176\u5177\u4f53\u7684state\uff0c\u56e0\u6b64hidden\u3002\uff08\u5bf9\u6bd4\uff0c\u5bf9\u4e8enumber seq generator\uff0c\u770b\u5230seq\u201c11233\u201d\uff0c\u6211\u4eec\u76f4\u63a5\u80fd\u770b\u51fa\u6bcf\u4e2a\u6570\u5b57\u5bf9\u5e94\u7684state\u3002\uff09</p> <p>\u6211\u4eec\u7528HMM\u8fd9\u4e2amodel\u8868\u793a\u4e86data\uff0c\u73b0\u5728\u9700\u8981\u8868\u793ahow well the model fits the observation\uff0c\u4e5f\u5c31\u662f\\(P(O|\\lambda_{THE})\\)\uff0c\u53ef\u4ee5\u7c7b\u6bd4\u4e4b\u524d\u7684\u201c\u8ba1\u7b97model\u4e0eobservation\u7684\u8ddd\u79bb\u201d\u3002  \uff08\u5728ASR\u4e2d\uff0c\u8981\u6bd4\u8f83\\(P(O|\\lambda_{THE})\\)\u548c\\(P(O|\\lambda_{A})\\)\uff09</p> <p>\u6211\u4eec\u9700\u8981\u627e\u51fa state seq X\uff0c \\(P(O|\\lambda_{THE})=\\Sigma_{X}P(O,X|\\lambda_{THE})\\) \uff08\u5bf9all possible state seqs\u6c42\u548c\uff09 \uff08\u548c\u4e4b\u524d\u7a77\u4e3epath sum\u5176\u5b9e\u662f\u4e00\u56de\u4e8b\uff09  </p> <p>HMM training: EM</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/","title":"Speech Papers","text":""},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#_1","title":"\u5e38\u7528\u6280\u672f","text":""},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#one-tts-alignment-to-rule-them-all","title":"One TTS Alignment to Rule Them All","text":"<p>The alignment learning framework combines the forward-sum algorithm, Viterbi algorithm, and an efficient static prior. </p> <p>parallel (non-autoregressive) TTS models factor out durations from the decoding process, thereby requiring durations as input for each token. </p> <p>These models generally rely on external aligners [4] like the Montreal Forced Aligner (MFA) [10], or on durations extracted from a pre-trained autoregressive model(forced aligner)</p> <p>non auto regressive\u7684\u95ee\u9898\uff1a - dependency on external alignments - poor training efficiency, require carefully engineered training schedules to prevent unstable learning - difficult to extend to new languages either because pre-existing aligners are unavailable or their output does not exactly fit the desired format</p> <p>\u6a21\u578b\uff1a  \u8f93\u5165\uff1aencoded text input\u548cmel spectrogram \u5c06\u6587\u672c\u957f\u5ea6N\u548cmel frame\u4e2a\u6570T\u5bf9\u9f50\u3002</p> <p>objective\u7684\u7b2c\u4e00\u90e8\u5206\u662flikelihood\uff0c\u57fa\u4e8ehidden markov model\u7684forward sum\u3002 </p> <p>\u7136\u540e\u8981\u5c06text\u548cmel spectrogram\u5206\u522b\u9001\u5165\u4e00\u4e2aencoder\uff0c\u5f97\u5230\u4e24\u4e2alatent vector\uff08\u4e0d\u8981\u6c42\u957f\u5ea6\u76f8\u540c\uff09\u3002  \u7136\u540e\u5bf9\u8fd9\u4e24\u4e2alatent vector\u8ba1\u7b97pairwise\u7684L2\uff0c\u5f97\u5230\u4e00\u4e2a\u8ddd\u79bb\u7684\u77e9\u9635\u3002 \u7136\u540e\u5bf9\u8fd9\u4e2a\u77e9\u9635D\u5173\u4e8etext\u7ef4\u5ea6\u6c42softmax\u3002 </p> <p>\u7136\u800c\u5f97\u5230\u7684\u8fd9\u4e2aA_soft\u8fd8\u4e0d\u80fd\u7528\u4e8ealignment\u3002\u6211\u4eec\u60f3\u8981\u660e\u786e\u7684text\u548cmel frame\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u800c\u4e0d\u662f\u6982\u7387\uff0c\u4e5f\u5c31\u662f\u8bf4A\u5e94\u8be5\u662fhard label\uff0c\u662fbinary\u76840\u62161\uff0c\u800c\u4e0d\u662f01\u4e4b\u95f4\u7684\u5c0f\u6570\u3002  \u6211\u4eec\u5bf9A_soft\u65bd\u52a0viterbi\u7b97\u6cd5\uff0c\u5f97\u5230\u4e00\u4e2a\u5355\u8c03\u7684\uff08monotonic\uff09\u8def\u5f84\u77e9\u9635A_hard\uff0c\u7136\u540e\u901a\u8fc7KL divergence\u8ba1\u7b97A_soft\u548cA_hard\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u6700\u540e\u5c06KL \u6563\u5ea6\u4e0e\u4e4b\u95f4\u7684forward sum\u7684loss\u76f8\u52a0\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u7684loss\u3002 </p> <p>Alignment Acceleration</p> <p>\u4e3a\u4e86\u52a0\u901f\u8bad\u7ec3\uff0c\u4e3alikelihood\u7684\u90e8\u5206\u52a0\u4e86\u4e00\u4e2aprior\uff0c\u53d8\u6210\u6c42posterior\u3002 </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#montreal-forced-aligner-trainable-text-speech-alignment-using-kaldi","title":"Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi","text":"<p>\u8fd9\u8bba\u6587\u6709\u70b9\u590d\u6742\uff0c\u601d\u60f3\u662fGMM+HMM\uff0c\u5177\u4f53\u7684\u8fd8\u6ca1\u770b\u3002 \u4e0b\u9762\u8fd9\u4e2a\u7b80\u8981\u7684\u4ecb\u7ecd\u6765\u81eaMFA\u7684\u6587\u6863\uff1a https://montreal-forced-aligner.readthedocs.io/en/latest/user_guide/index.html</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#pipeline-of-training","title":"Pipeline of training","text":"<p>The Montreal Forced Aligner by default goes through four primary stages of training.  - The first pass of alignment uses monophone models, where each phone is modelled the same regardless of phonological context.  - The second pass uses triphone models, where context on either side of a phone is taken into account for acoustic models.  - The third pass performs LDA+MLLT (feature space Maximum Likelihood Linear Regression (fMLLR)) to learn a transform of the features that makes each phone\u2019s features maximally different.  - The final pass enhances the triphone model by taking into account speaker differences, and calculates a transformation of the mel frequency cepstrum coefficients (MFCC) features for each speaker. </p> <p>See the Kaldi page on feature transformations for more detail on these final passes.</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#synthesis","title":"\u7eafsynthesis","text":""},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#fastspeech-2-fast-and-high-quality-end-toend-text-to-speech","title":"FASTSPEECH 2: FAST AND HIGH-QUALITY END-TOEND TEXT TO SPEECH","text":"<p>ICLR 2021</p> <p>1) directly training the model with ground-truth target instead of the simplified output from teacher 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. </p> <p>\u56de\u987eFastSpeech\uff1a  </p> <p></p> <p>encoder\u90e8\u5206\uff0c\u4f7f\u7528\u4e00\u4e9b\u989d\u5916\u7684\u6a21\u578b\u3001\u65b9\u6cd5\u5206\u522b\u5f97\u5230pitch\u3001duration\u548cenergy\u3002 \u7136\u540e\u8bad\u7ec3\u65f6\u5c06\u8fd9\u4e9b\u4fe1\u606f\u52a0\u5230latent embedding\u4e2d\uff0c\u540c\u65f6\u7528\u8fd9\u4e9b\u4fe1\u606f\u8bad\u7ec3\u5404\u81ea\u7684predictor\u3002 \u6d4b\u8bd5\u65f6\u76f4\u63a5\u7528predictor\u9884\u6d4b\u51fa\u8fd9\u4e9b\u4fe1\u606f\uff0c\u52a0\u5230latent embedding\u4e2d\u3002</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#mixer-tts-non-autoregressive-fast-and-compact-text-to-speech-model-conditioned-on-language-model-embeddings","title":"Mixer-TTS: Non-Autoregressive, Fast and Compact Text-to-Speech Model Conditioned on Language Model Embeddings","text":"<p>\u8fd9\u7bc7\u7684introduction\u90e8\u5206\u5199\u5f97\u5f88\u597d\uff0c\u4fe1\u606f\u91cf\u5f88\u5927\u3002 Non-autoregressive models can generate speech two orders of magnitude faster than auto-regressive models with similar quality. </p> <p>Glow-TTS [10] proposed a flowbased algorithm for unsupervised alignment training. This algorithm has been improved in RAD-TTS [11] and modified for non-autoregressive models in [12]. This new alignment framework greatly simplifies TTS training pipeline \u8fd9\u4e2a12\u4e4b\u540e\u8981\u770b\u4e0b\u3002 R. Badlani, A. Lancucki, K. Shih, R. Valle, W. Ping, and B. Catanzaro, \u201cOne TTS alignment to rule them all,\u201d arXiv:2108.10447, 2021.</p> <p> \u4e0a\u9762\u89e3\u91ca\u4e86\u7528external tool\u6c42pitch\u7684\u91cd\u8981\u6027\u3002 \u987a\u4fbf\u63d0\u4e86\u4e00\u4e0b\u7528BERT\u7684\u597d\u5904\u3002</p> <p>\u8fd9\u7bc7\u6587\u7ae0\u662f\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u7684MLP-Mixer\u7684\u8fc1\u79fb\u3002 - backbone\uff1aMLP-Mixer\uff0cnon auto regressive - uses an explicit duration predictor, which is trained by the unsupervised alignment framework proposed in [12] - has an explicit pitch predictor - adds token embeddings from an external pre-trained LM to improve speech prosody and pronunciation.  \u5e76\u4e14\u6307\u51fa\u5982\u679c\u53ea\u7528BERT\u7684token\uff0c\u800c\u4e0d\u7528BERT\u4f5cinference\u7684\u8bdd\uff0c\u5e76\u6ca1\u6709\u589e\u52a0\u591a\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u6548\u679c\u5f88\u660e\u663e\u3002\uff08\u672c\u6587\u4f7f\u7528\u7684\u662fhuggingface\u7684ALBERT\u3002\uff09 </p> <p>  \u8fd9\u91cc\u8fd9\u4e2a\u9700\u8981\u8be6\u7ec6\u770b\u4e00\u4e0b12\u548c\u8fd9\u91cc\u7684\u4ee3\u7801\uff0c\u6ca1\u51c6\u80fd\u7528\u5230\u3002 we train the speech-to-text alignments jointly with the decoder by using adaptation of unsupervised alignment algorithm [12] which was proposed in implementation of FastPitch 1.13 https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#fastpitch-parallel-text-to-speech-with-pitch-prediction","title":"Fastpitch: Parallel Text-to-Speech with Pitch Prediction","text":"<p>https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#prosodysynthesis","title":"\u6709\u5173prosody\u7684synthesis","text":""},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#towards-end-to-end-prosody-transfer-for-expressive-speech-synthesis-with-tacotron","title":"Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron","text":"<p>^7f06d5</p> <p>2018  </p> <p>\u8bad\u7ec3\u65f6\uff0creference audio\u5c31\u662ftarget audio\u3002 \u4e3a\u4e86\u907f\u514dcopy\u95ee\u9898\uff0c\u91c7\u7528\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\uff1a \u4f7f\u7528bottleneck\u7ed3\u6784\uff0c\u5f3a\u5236\u8ba9\u6a21\u578b\u5b66\u5230\u4f4e\u7ef4representation\u3002 inference\u65f6\uff0creference audio\u65e0\u9650\u5236\uff0c\u4e0d\u4e00\u5b9a\u8981\u548c\u8bad\u7ec3\u6570\u636e\u662f\u540c\u4e00speaker\u3002</p> <p>\u6211\u7684\u95ee\u9898\uff1a \u600e\u6837\u8bbe\u8ba1\u624d\u80fd\u8ba9\u6a21\u578b\u7684\u4e0d\u540cmodule\u5404\u81ea\u5b66\u5230speaker\u548cprosody\uff1f \uff08\u8fd9\u7bc7\u6587\u7ae0\u7684speaker\u90e8\u5206\u5199\u5f97\u597d\u4e71\uff0c\u4e0d\u60f3\u770b\u4e86\uff09</p> <p>\u8ba1\u7b97prosody embedding\u7684encoder\uff0c\u4f7f\u7528\u7684\u662f\u51e0\u5c42conv+\u901a\u8fc7stride\u8fdb\u884cdownsample+GRU\u5f97\u5230\u4e00\u4e2a128dim\u7684embedding\u3002 \u7136\u540e\u518d\u7528\u51e0\u5c42fully connected layer\u5c06embedding\u8c03\u6574\u5230\u60f3\u8981\u7684\u7ef4\u6570\u3002</p> <p>prosody encoder\u7684input\u5f62\u5f0f\u4e5f\u6709\u8bb2\u7a76\uff0c\u4f7f\u7528\u7684\u662fmel-warped spectrum\u3002 </p> <p>\u6211\u4eec\u53ef\u4ee5\u6362\u4e2a\u89d2\u5ea6\uff0c\u628aprosody encoder\u8fd9\u4e2a\u5e26\u6709downsample\u7684convnet\u770b\u4f5c\u662fautoencoder\u7684encoder\uff0c\u6574\u4e2a\u6a21\u578b\u76f8\u5f53\u4e8e\u8f93\u5165audio\u8f93\u51faaudio\u7684antoencoder\u3002\u53ea\u4e0d\u8fc7decoder\u90e8\u5206\uff0c\u4e5f\u5c31\u662ftacotron\uff0c\u8fd8\u4f7f\u7528\u4e86text\u548cspeaker\u7684\u4fe1\u606f\u3002 </p> <p>\u53e6\u5916\uff0c\u5176\u5b9e\u53ef\u4ee5\u4e0d\u53ea\u4ea7\u751f\u4e00\u4e2aprosody embedding\uff0c\u800c\u662f\u53ef\u53d8\u957f\u5ea6\uff0c\u4ea7\u751f\u5f88\u591a\u4e2a\u3002\u8fd9\u6837\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u5f88\u957f\u7684\u53e5\u5b50\uff0c\u4f46\u4e5f\u6709\u574f\u5904\u3002 </p> <p>\u7136\u540e\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u51e0\u4e2ametric\u3002 </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#robust-and-fine-grained-prosody-control-of-end-to-end-speech-synthesis","title":"ROBUST AND FINE-GRAINED PROSODY CONTROL OF END-TO-END SPEECH SYNTHESIS","text":"<p>2019  \u8fd9\u7bc7\u6587\u7ae0\u662f\u76f4\u63a5\u6253\u4e0a\u9762\u8fd9\u7bc7\u7684\u3002 two limitations - controlling the prosody at a specific moment of generated speech is not clear. \u4e3a\u4e86\u5f15\u5165\u53ef\u53d8\u957f\u5ea6\u4e2a\u6570\u4e2aprosody embedding\u800c\u4e0d\u662f\u53ea\u7528\u4e00\u4e2a\u3002 - inter-speaker prosody transfer is not robust if the difference between the pitch range of the source speaker and the target speaker is significant speaker\u4e4b\u95f4\u5982\u679cpitch\u5dee\u592a\u591a\uff0cinference\u6548\u679c\u4e0d\u597d\u3002</p> <p>contribution\uff1a 1. \u53ef\u53d8\u957f\u5ea6\u4e2aprosody embedding\uff0c\u957f\u5ea6\u53ef\u4ee5\u7b49\u4e8ereference audio\u4e5f\u53ef\u4ee5\u7b49\u4e8etext\u3002 2. \u5bf9prosody embedding\u505anormalization\u6709\u597d\u5904\u3002 </p> <p>\u4ecb\u7ecdGST-Tacotron\uff0c </p> <p>Method\uff1a Variable-length prosody embedding \u5c06prosody embedding\u7684\u4e2a\u6570downsample\u5230\u4e0etext side\uff08encoder attention\u957f\u5ea6\uff09\u6216speech side\uff08decoder attention\u957f\u5ea6\uff09\u76f8\u5339\u914d\u3002</p> <p>prosody encoder\u4e2d\u7684convnet\uff0c\u7b2c\u4e00\u5c42\u662f\u7528Coordconv\u6765\u4fdd\u7559positional information\u3002</p> <p></p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#cross-speaker-style-transfer-with-prosody-bottleneck-in-neural-speech-synthesis","title":"Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis","text":"<p>\u73b0\u72b6limitation\uff1a 1. the style embedding extracted from single reference speech can hardly provide fine-grained and appropriate prosody information for arbitrary text to synthesize 2. the content/text, prosody, and speaker timbre are usually highly entangled, it\u2019s therefore not realistic to expect a satisfied result when freely combining these components, such as to transfer speaking style between speakers.</p> <p>contribution\uff1a \u4f7f\u7528prosody bottleneck\uff0cdisentangles the prosody from content and speaker timbre</p> <p>\u57fa\u4e8etransformer-TTS \u6ca1\u4ed4\u7ec6\u770b</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#prosospeech-enhancing-prosody-with-quantized-vector-pre-training-in-text-to-speech","title":"PROSOSPEECH: ENHANCING PROSODY WITH QUANTIZED VECTOR PRE-TRAINING IN TEXT-TO-SPEECH","text":"<p>(ICASSP 2022)  Based on FastSpeech</p> <p>\u73b0\u72b6limitation\uff1a 1. some works use external tools to extract pitch contour. However, the extracted pitch has inevitable errors 2. Some works extract prosody attributes (e.g., pitch, duration and energy) from speech and model them separately. However, these prosody attributes are dependent on each other and produce natural prosody together. 3. Prosody has very high variability and varies from person to person and word to word. It can be very difficult to shape the full distribution of prosody using the limited amount of high-quality TTS data.</p> <p>Method\uff1a  </p> <p> \u6211\u8ba4\u4e3a\u7684\u5173\u952e\u4fe1\u606f\uff1a - \u57fa\u4e8efastspeech\uff0c\u9700\u8981\u9884\u6d4bcharacter\u5bf9\u5e94\u7684\u65f6\u957f - \u4ecetext\u4e2d\u7528encoder\u63d0\u53d6\u51fatext\u548cphoneme\u7684embedding - \u4ecereference audio\u7684mel-spectrogram\u7684\u4f4e\u9891\u90e8\u5206\u4e2d\u7528encoder\u63d0\u53d6\u51faembedding\uff0c\u79f0\u4e3aLPV\u3002 - \u8fd9\u7bc7\u5de5\u4f5c\u7684\u5173\u952e\u5728\u4e8e\uff0ctraining\u7684\u65f6\u5019\uff0c\u4ee3\u8868prosody\u7684embedding LPV\u6765\u81ea\u4e8eground truth\u7684spectrogram\uff1binference\u7684\u65f6\u5019\uff0c\u5982\u679c\u6709reference audio\uff0c\u5219LPV\u6765\u81ea\u4e8eprosody decoder\uff1b\u5982\u679c\u6ca1\u6709reference audio\uff0c\u5219LPV\u6765\u81ea\u4e8e\u4e00\u4e2apretrained auto regressive predictor\u3002\u8fd9\u4e2apredictor\u63a5\u6536\u6587\u5b57\u548cspeaker embedding\uff0c\u8f93\u51faprosody embedding\u3002 - LPV predictor\u662fauto-regressive\u7684\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u4f7f\u7528auto-regressive\u6a21\u578b\u5e38\u7528\u7684teacher forcing\u3002  - pre-train  </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#discourse-level-prosody-modeling-with-a-variational-autoencoder-for-non-autoregressive-expressive-speech-synthesis","title":"Discourse-Level Prosody Modeling with a Variational Autoencoder for Non-Autoregressive Expressive Speech Synthesis","text":"<p> \u57fa\u4e8efastspeech2 </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#_2","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>Autoregressive\uff1a Tacotron2 and Transformer-TTS - However, these models suffer from the unsatisfactory robustness of the attention mechanism, especially when the training data is highly expressive. - low inference efficiency.</p> <p>Non-autoregressive\uff1a FastSpeech and Parallel Tacotron</p> <p>Acoustic\uff1a Recently, acoustic models based on Normalizing Flows and Diffusion Probabilistic Models have also pushed the naturalness of speech synthesis to a new level.</p> <p>one-to-many mapping\u5e26\u6765\u7684\u95ee\u9898\uff1a \u201cone-to-many mapping from phoneme sequences to acoustic features, especially the prosody variations in expressive speech with multiple styles\u201d</p> <ul> <li>Variational autoencoder (VAE)</li> <li>Another approach to address the one-to-many issue is utilizing the textual information in a context range wider than the current sentence, e.g., at the paragraph or discourse level. \uff08\u8bf4\u767d\u4e86\u5c31\u662fcontext\u3002\uff09  \u4ee5\u5f80\u8fd9\u4e24\u4e2a\u601d\u8def\u7528\u4e8eautoregressive model\uff0c\u672c\u6587\u5c06\u5176\u7528\u4e8enon autoregressive\u3002</li> </ul> <p>In this method, a VAE is combined with FastSpeech to extract phone-level latent prosody representations, i.e., prosody codes, from the fundamental frequency (F0), energy and duration of the speech. Then, a Transformer-based model is constructed to predict prosody codes, taking discourse-level linguistic features and BERT embeddings as input.</p> <p>FastSpeech1\uff1a usually results in over-smoothed output In order to alleviate this problem, FastSpeech1 introduces an autoregressive teacher model for knowledge distillation</p> <p>FastSpeech2\uff1a   FastSpeech2\u5b58\u5728\u7684\u95ee\u9898\uff1a - \u9700\u8981\u989d\u5916\u7684module - \u9700\u8981\u5c06pitch spectrogram\u7528CWT\u8fdb\u884c\u63d2\u503c\uff0c\u4e0d\u592a\u7b26\u5408\u76f4\u89c9\u3002 - \u4f7f\u7528frame level\u7684pitch\u4fe1\u606f\uff0c\u8303\u56f4\u4e0d\u591f\u5927\u3002</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#method","title":"Method","text":"<ul> <li> <p>\u5c06\u7528\u5176\u4ed6\u65b9\u6cd5\u63d0\u53d6\u5f97\u5230\u7684variation information (i.e., pitch, energy, and upsampled frame-level duration)\u9001\u5165VAE\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u9001\u5165spectrogram\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u4e13\u6ce8\u4e8eprosody\uff0c\u5ffd\u7565\u5176\u4ed6\u7ec6\u8282\uff0c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002 \u7591\u95ee\uff1avariation info\u4ece\u54ea\u6765\uff1f \u7b54\uff1aThe structures of the text encoder, duration predictor, length regulator and mel-spectrogram decoder follows the ones in FastSpeech1\u3002</p> </li> <li> <p>VAE encoder \u4e2dsample\u5f97\u5230\u7684latent prosody embedding\u548cBert\u7684text embedding\u8fdb\u884cconcat\uff0c\u7136\u540e\u9001\u5165decoder\u3002      VAE \u7684elbo\u7684KL\u6563\u5ea6\u9700\u8981\u7528\u5230\u8fd9\u4e24\u4e2a\u4e1c\u897f\uff0c\u8bb0\u4e00\u4e0b\uff1a     </p> </li> <li> <p>VAE encoder\u4e3a\u6bcf\u4e2aphone\u9884\u6d4b\u4e00\u4e2a\u591a\u7ef4\u7684\u03bc\uff0c\u8fd9\u4e2a\u5411\u91cf\u79f0\u4e3a\u8fd9\u4e2aphone\u7684prosody code of this phone\u3002</p> </li> <li> <p>\u4f7f\u7528\u6587\u672c\u7684context\uff1a\u7528pretrained BERT\u63d0\u53d6context\u7684embedding\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2aencoder\uff0c\u63a5\u53d7\u8fd9\u4e9bcontext embedding\uff0c\u9884\u6d4b\u4e4b\u524d\u5f97\u5230\u7684prosody code\u3002 </p> </li> <li>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u771f\u6b63inference\u7684\u65f6\u5019\uff0c\u662f\u6ca1\u6709VAE\u7684\u3002VAE\uff08reference encoder\uff09\u53ea\u662f\u7528\u6765\u751f\u6210prosody code\uff0c\u7136\u540e\u7528\u8fd9\u4e2aprosody code\u6765\u8bad\u7ec3\u4e00\u4e2a\u4ececontext\u4e2d\u9884\u6d4bprosody code\u7684encoder\u3002\u6700\u540einference\u7528\u7684\u662f\u8fd9\u4e2acontext encoder\u9884\u6d4b\u51fa\u6765\u7684prosody\u4fe1\u606f\u3002 </li> </ul>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#_3","title":"\u603b\u7ed3\uff1a","text":"<p>\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u7528VAE\u751f\u6210\u4e00\u4e2a\u4ee3\u8868prosody\u7684latent \u5206\u5e03\uff0c\u7136\u540e\u5e76\u6ca1\u6709\u76f4\u63a5\u7528\u8fd9\u4e2alatent\u4fe1\u606f\u6765inference\u3002\u800c\u662f\u7528\u8fd9\u4e2alatent\u4fe1\u606f\u8bad\u7ec3\u4e86\u4e00\u4e2a\u201c\u8f93\u5165context text\uff0c\u8f93\u51faprosody latent \u4fe1\u606f\u201d\u7684encoder\u3002\u6700\u540e\u7528\u8fd9\u4e2aencoder\u9884\u6d4b\u7684prosody\u6765\u6307\u5bfcfastspeech\u7684inference\u3002 \u8fd9\u4e2a\u4f7f\u7528VAE\u8fdb\u884clatent\u5efa\u6a21\uff0c\u7136\u540e\u518d\u7528latent\u8bad\u7ec3\u5176\u4ed6module\u7684\u601d\u8def\u53ef\u4ee5\u5b66\u4e60\u4e00\u4e0b\u3002\u6ca1\u51c6\u53ef\u4ee5\u628a\u5176\u4e2d\u7684\u54ea\u4e00\u90e8\u5206\u6362\u6210visual\u7684encoder\u3002</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#improving-emotional-speech-synthesis-by-using-sus-constrained-vae-and-text-encoder-aggregation","title":"Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation","text":"<p>In recent studies, Variational AutoEncoder(VAE) [8] shows stronger capabilities in disentanglement, scaling and interpolation for expression modeling [9] and style control[10].</p> <p>\u601d\u60f3\uff1a - \u5728VAE\u4e2d\uff0cInstead of conventional KL-divergence regularizer, the new constraint expects the means of the embedding vectors are on the surface of the unit sphere while all dimensions have a uniform standard deviation. \u4e5f\u5c31\u662f\u4f7f\u7528\u4e86\u4e00\u4e2a\u65b0\u7684\u7ea6\u675f/loss\u3002 - \u5c06emotion embedding\u4f5c\u4e3aquery\u9001\u5165encoder\u800c\u4e0d\u662fdecoder\u3002\u540c\u65f6\u4e5f\u4f7f\u7528\u4e00\u4e9b\u5176\u4ed6text embedding\u4f5c\u4e3aencoder\u7684query\u3002\u8fd9\u6837\u5c31\u7ec4\u6210\u4e86\u4e00\u4e2amulti-query attention\u3002</p> <p>It contains  a self-attention-based text encoder,  an RNN-based auto-regressive decoder,  a GMMbased attention[14] bridging them,  a VAE-based emotion encoder and  an emotion classifier.  WaveRNN[15] is adopted to convert mel spectrogram to waveforms. </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#sus-constrained-vae","title":"SUS-constrained VAE","text":"<p>\u8fd9\u91cc\u8bb2\u4e86\u4e00\u4e0bVAE\u7684posterior collapse\u7684\u95ee\u9898\uff1a  \u6838\u5fc3\u601d\u60f3\uff1a The critical problem here, we think, is the distances between the means should be in the similar order with their standard deviations.</p> <p>If the distance between means is much bigger than their standard deviation, latent vectors will collapse to the means. Conversely, if the distance between means is much smaller, latent vectors will collapse to be independent on the input. \uff08\u611f\u89c9\u8fd9\u91cc\u7684independent\u662f\u4e0d\u662f\u5e94\u8be5\u662fdependent\uff0c\u6ca1\u51c6\u5199\u9519\u4e86\uff1f\uff09</p> <p>Inspired by it, this paper restricts the means approaching to the Surface of the Unit Sphere (SUS) while set the standard deviations to be an appropriate constant for all dimensions, such as 1. \u8bbestd\u4e3a\u5e38\u6570\uff0c\u7ea6\u675fmean\u5230\u5355\u4f4d\u5706\u7684\u8868\u9762\u3002</p> <p>\u4e0a\u9762\u7684\u601d\u8def\u7684\u6545\u4e8b\u8bb2\u5f97\u5f88\u597d\uff0c \u653e\u5230\u5b9e\u9645\u4ee3\u7801\u4e2d\uff0c\u5176\u5b9e\u5c31\u662f\u76f8\u5f53\u4e8e\u62c9\u683c\u6717\u65e5\u7ea6\u675f\uff0c\u7ea6\u675flatent vector\u7684mean\u7684norm\u4e3a1. \u7136\u540e\u624b\u52a8\u8bbe\u7f6estd\u4e3a\u4e00\u4e2a\u5e38\u6570\u3002 </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#weighted-aggregation","title":"Weighted Aggregation","text":"<p>multi-query attention \u8bf4\u767d\u4e86\u5c31\u662f\u628aencoder\u4e2d\u6bcf\u4e2alayer\u7684\u8f93\u51fa\u62ff\u8fc7\u6765\u9001\u5165\u540c\u4e00\u4e2aattention\uff0c\u8ba1\u7b97\u4e00\u4e2a\u6574\u4f53\u7684\u4fe1\u606f\uff0c\u6765\u56ca\u62ec\u5404\u4e2a\u5c3a\u5ea6\u7684\u4fe1\u606f\u3002 \u8bf4\u767d\u4e86\u76f8\u5f53\u4e8eobject detection\u7684FPN\u3002</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#combined-multi-query","title":"Combined multi-query","text":"<p> \u5176\u5b9e\u5c31\u662f\u628aemotion embedding\u4e5f\u9001\u5165\u8fd9\u4e2a\u7c7b\u4f3c\u4e8eFPN\u7684multi-query attention\u3002\u5177\u4f53\u7ec6\u8282\u4e0d\u770b\u4e86\u3002</p> <ul> <li>\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e2a\u53c2\u8003\u7684\u5730\u65b9\uff1a\u5c55\u793a\u5bf9emotion\u7684\u805a\u7c7b\uff08latent embedding\uff09\u4f1a\u7528\u5230t-SNE\u3002</li> <li>\u8fd8\u6709\u51e0\u4e2a\u9700\u8981\u770b\u7684\u4e1c\u897f\uff1a</li> <li>In recent studies, Variational AutoEncoder(VAE) [8] shows stronger capabilities in disentanglement, scaling and interpolation for expression modeling [9] and style control[10].</li> <li>In our previous work[11], we utilize the contexts extracted form the stacked layers to do self-learned multi-query attention over an expressive corpus.</li> <li>posterior collapse. Many attempts have been made to address this puzzle, such as annealing strategy in [10].</li> <li>\u8fd8\u6709\u6536\u85cf\u5939\u91cc\u7684\u51e0\u7bc7\u5173\u4e8eposterior collapse\u7684\u6587\u7ae0\u3002</li> <li>GMMv2 attention</li> <li>layer normalization</li> <li>multi-query attention</li> </ul>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#video","title":"Video\u76f8\u5173","text":""},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#visagesyntalk-unseen-speaker-video-to-speech-synthesis-via-speech-visage-feature-selection","title":"VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection","text":"<p>ECCV 2022 \u8fd9\u7bc7\u7684\u4efb\u52a1\u662flip speech\u3002</p> <p>Nevertheless, video-to-speech synthesis is considered as challenging since it is expected to represent not only the speech content but also the identity characteristics (e.g., voice) of the speaker</p> <p></p> <p></p> <p></p> <p>\u89c6\u9891\u7ecf\u8fc7visual encoder\u4e4b\u540e\uff0c\u7528speech-visage feature selection module\u63d0\u53d6speech content\u4fe1\u606f\uff0c\u8fc7\u6ee4\u6389speaker\u7684\u5916\u8c8c\u4fe1\u606f\u3002  speech-visage feature selection module\u539f\u6587\u8bf4\u7684\u633a\u82b1\u54e8\uff0c\u5176\u5b9e\u5c31\u662f\u7528lstm+softmax\u8ba1\u7b97attention weight\uff0c\u7528\u8fd9\u4e9bweight\u5bf9encoder\u5f97\u5230\u7684visual feature\u8ba1\u7b97embedding\u3002</p> <p>\u611f\u89c9\u4e0b\u9762\u8fd9\u4e00\u6b65\u662f\u8fd9\u7bc7\u6587\u7ae0\u7684\u5173\u952e\u3002 \u6211\u4eec\u5047\u8bbefeature\u4e2d\u7684\u4fe1\u606f\u53ea\u6709speech content\u548cspeaker\u5916\u8c8c\u4fe1\u606f\u4e24\u79cd\u3002 \u90a3\u4e48\u8ba1\u7b97\u4e86speaker\u5916\u8c8c\u7684attention weight\u4e4b\u540e\uff0c1-weight\u5f97\u5230\u7684\u5c31\u662fspeech\u4fe1\u606f\u7684weight\u3002 </p> <p>\u7136\u540e\u5bf9visual\u548cspeech\u7684feature\u90fd\u505amulti-head attention\u3002</p> <p>systhesis\u90e8\u5206\uff1a  \u7c7b\u4f3cstyle-transfer\u7684\u601d\u60f3\u3002</p> <p>\u8fd9\u91cc\u4f7f\u7528\u7684style transfer generator\u51fa\u81ea\uff1a'A style-based generator architecture for generative adversarial networks.' cvpr2019 \u5c06speech\u548cvisual\u7684\u4fe1\u606f\u540c\u65f6\u9001\u5165\u8fd9\u4e2agenerator\uff0c\u5176\u4e2d\u4f7f\u7528visual\u4fe1\u606f\u6765\u5b9a\u4e49style\u3002\u6700\u7ec8\u8f93\u51famel-spectrogram\u3002</p> <p>\u6700\u540e\u540e\u5904\u7406\u7684\u90e8\u5206\uff0c\u5148\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u5c06mel-spectrogram\u8f6c\u6210linear spectrogram\uff0c\u7136\u540e\u65bd\u52a0Griffin-Lim algorithm \u5f97\u5230waveform\u3002\u8fd9\u91cc\u8bad\u7ec3\u8fd9\u4e2a\u540e\u5904\u7406\u7f51\u7edc\u7684\u65f6\u5019\u5bf9\u5e94\u7740\u4e00\u4e2areconstruction loss\u3002</p> <p></p> <p>\u5bf9speech\u548cvisual\u7279\u5f81\u5404\u81ea\u52a0\u4e0a\u4e86\u4e00\u4e9b\u5b50\u7f51\u7edc\u6765\u6307\u5f15\u4e4b\u524d\u7684attention\u7684\u8bad\u7ec3\u3002\u8bf4\u767d\u4e86\u5c31\u662f\u76f8\u5f53\u4e8emulti-task\u3002</p> <p>\u9488\u5bf9visual feature\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff1a \u8fd9\u91cc\u8fd9\u4e2a\u601d\u8def\u4e5f\u5f88\u597d\uff1a  \u9664\u4e86\u5bf9\u5f97\u5230\u7684visual feature\u8fdb\u884c\u4e00\u4e2amulti class classification\u6765\u9884\u6d4bspeaker\uff0c\u6211\u4eec\u8fd8\u60f3\u8ba9\u540c\u4e00\u4e2aspeaker\u8bf4\u4e0d\u540c\u5185\u5bb9\u7684\u4e24\u4e2a\u89c6\u9891\uff0c\u62bd\u53d6\u51fa\u6765\u7684visual feature\u5c3d\u53ef\u80fd\u7c7b\u4f3c\u3002 \u4e5f\u5c31\u662f\u5728classifier\u7684\u8f93\u51falogit\u548c\u8f93\u5165feature map\u5c42\u9762\u5404\u81ea\u8ba1\u7b97\u4e00\u4e2aloss\u3002  \u6700\u540e\uff0c\u6211\u4eec\u60f3\u8ba9speech\u548cvisual\u7684feature\u5404\u53f8\u5176\u804c\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u5c06speech content\u7684feature\u8f93\u5165\u8fdb\u4e0a\u9762\u63d0\u5230\u7684\u9884\u6d4bspeaker\u7684classifer\uff0c\u6211\u4eec\u5e0c\u671b\u5176\u9884\u6d4b\u7ed3\u679c\u5f88\u5dee\u3002\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u5e0c\u671b\u8fd9\u4e00\u90e8\u5206\u7684loss\u8d8a\u5927\u8d8a\u597d\u3002 \u539f\u6587\u662f\u901a\u8fc7reverse gradient\u5b9e\u73b0\u7684\uff0c\u6211\u611f\u89c9\u5176\u5b9e\u76f4\u63a5\u5728\u8fd9\u4e2aloss\u4e4b\u524d\u76f4\u63a5\u52a0\u4e2a\u8d1f\u53f7\u4f5c\u7528\u4e5f\u5dee\u4e0d\u591a\u3002  </p> <p>\u9488\u5bf9speech content feature\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff1a </p> <p>\u8fd9\u91cc\u6709\u4e00\u4e2apretrain\u7684\u6a21\u578b\uff0c\u8f93\u5165spectrogram\uff0c\u8f93\u51faspeaker id\u3002 \u6211\u4eec\u5c06\u539f\u672c\u6a21\u578b\u8f93\u51fa\u7684spectrogram\u9001\u5165\u8fd9\u4e2apretrained model\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u4e0eground truth speaker id\u7b97\u4e00\u4e2aloss\u3002</p> <p>\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8c03\u6362\u4e24\u4e2a\u5bf9\u5e94\u7740\u4e0d\u540cspeaker \u7684visual feature\uff0c\u540c\u65f6\u4f7f\u7528\u539f\u672c\u5404\u81ea\u7684speech content feature\uff0c\u671f\u5f85\u8fd9\u4e2apretrain\u7684\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u51fa\u8fd9\u4e24\u4e2aspeaker\u7684\u8eab\u4efd\u4e92\u6362\u4e86\u3002</p> <p>\u6a21\u578b\u5176\u5b9e\u8fd8\u63a5\u4e86\u4e00\u4e2aGAN\u7684discriminator\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u8fd9\u4e2a\u6a21\u578b\u5176\u5b9e\u540c\u65f6\u4f7f\u7528\u4e86\u5bf9\u4e8emel-spectrogram\u7684\u91cd\u6784\u635f\u5931\u4ee5\u53cadiscriminator\u7684\u5206\u7c7b\u635f\u5931\u3002 </p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#_4","title":"\u603b\u7ed3\uff1a","text":"<p>\u611f\u89c9VisageSynTalk\u4e2d\u7684\u8fd9\u4e2a\u5c06\u4fe1\u606fdisentangle\u4e3aw\u548c1-w\u4e24\u90e8\u5206\u7684\u60f3\u6cd5\u53ef\u80fd\u80fd\u7528\u4e0a\u3002\u6ca1\u51c6\u4ee5\u540e\u53ef\u4ee5\u5c06video\u4e2d\u7684\u4fe1\u606f\u89e3\u8026\u4e3a\u4e0espeech\u76f8\u5173\u7684\u4fe1\u606f\u548c\u4e0espeech\u65e0\u5173\u7684\u4fe1\u606f\u3002\u6216\u8005\u89e3\u8026\u4e3a\u4e0espeecher\u76f8\u5173\u548c\u65e0\u5173\u7684\u4e24\u90e8\u5206\uff0c\u6216\u8005\u89e3\u8026\u4e3a\u4e0eevent\u76f8\u5173\u548c\u65e0\u5173\u7684\u4e24\u90e8\u5206\u3002\u53cd\u6b63\u8fd9\u4e2a\u601d\u8def\u53ef\u4ee5\u7ee7\u7eed\u5ef6\u4f38\u3002</p>"},{"location":"NLP/Speech%20Synthesis/Speech%20Papers/#svts-scalable-video-to-speech-synthesis","title":"SVTS: Scalable Video-to-Speech Synthesis","text":"<p>INTERSPEECH 2022</p> <p></p> <p>It consists of a video-to-spectrogram predictor followed by a spectrogram-to-waveform synthesizer.</p> <p>ResNet18+conformer network + pre-trained neural vocoder</p> <p>contribution: \u9664\u4e86\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u6a21\u578b\u4e4b\u5916\uff0c\u8fd9\u7bc7\u6587\u7ae0\u7684\u6d88\u878d\u5b9e\u9a8c\u4e5f\u633a\u503c\u5f97\u6ce8\u610f\u3002 </p> <p>conformer\uff1a A. Gulati, J. Qin, C. Chiu, et al., \u201cConformer: Convolutionaugmented transformer for speech recognition,\u201d in Interspeech, H. Meng, B. Xu, and T. F. Zheng, Eds., ISCA, 2020</p> <p></p> <p></p> <p></p>"},{"location":"PGM/course/lecture01-Introduction/","title":"lecture01-Introduction","text":""},{"location":"PGM/course/lecture01-Introduction/#measure-of-association-between-two-random-variables","title":"Measure of association between two random variables","text":"<p>  \u5982\u679c\u6bcf\u4e00\u4e2a\u53d8\u91cf\u90fd\u662fgaussian\uff0c\u5219joint\u5c31\u662f\u4e00\u4e2amultivariate gaussian\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u53ef\u4ee5\u7b80\u5355\u5730\u7528\u8fd9\u4e2agaussian\u7684covariance matrix\u8868\u793a </p> <p> </p>"},{"location":"PGM/course/lecture02-MRFrepresentation/","title":"lecture02-MRFrepresentation","text":"<p> joint\u53ef\u4ee5\u8868\u793a\u4e3a\u6bcf\u4e2aclique\u4e0a\u7684potential\u7684\u4e58\u79ef\uff0c\u518d\u52a0\u4ee5\u6b63\u5219\u5316 potential\u662fpre-probabilistic\u7684\uff0c\u4e5f\u5c31\u662f\u6ca1\u6709\u6b63\u5219\u5316</p> <p> H\u4e3a\u4e00\u4e2a\u65e0\u5411\u56fe\u3002\u5982\u679cP\u662f\u4e00\u4e2aGibbs-distribution over H\uff0c\u4e5f\u5c31\u662f\u8bf4P\u7684factor\u4e0d\u662f\u4efb\u9009\u7684\uff0c\u800c\u662f\u9009\u81eagraph H\u4e2d\u7684clique\uff0c\u7136\u540e\u518d\u52a0\u4ee5normalization\uff0c\u90a3\u4e48\u6839\u636esoundness theorem\uff0cH is an I-map of P\u3002\u6362\u53e5\u8bdd\u8bf4\uff0cH\u6240\u80fd\u8868\u793a\u7684\u4e1c\u897f\uff0c\u662fP\u6240\u80fd\u8868\u793a\u7684\u4e1c\u897f\u7684\u5b50\u96c6\u3002\u8fd9\u4e5f\u5c31\u65b9\u4fbf\u6211\u4eec\u4ece\u4e00\u4e2agraph H\u4e2d\uff0c\u5f97\u5230\u5b83\u7684distribution(P)\uff0c\u800c\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u81f3\u5c11\u4e0d\u4f1a\u635f\u5931independences\u3002</p> <p></p> <p>Independence properties:  </p> <p>\u603b\u7ed3\uff1a independence set\u662fgraph\u4e2d\u7684first citizen\uff0c \u7136\u540e\u6211\u4eec\u5229\u7528gibbs distribution\u4eceindependence set\u4e2d\u63d0\u53d6\u51fa\u6982\u7387\u5206\u5e03</p> <p>\u53cd\u4e4b\uff1a  \u5982\u679c\u6709\u4e09\u4e2avariable setX,Y,Z\uff0c\u5e76\u4e0d\u6ee1\u8db3global markov property\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u5efa\u7acb\u67d0\u4e9bP\u4f7f\u5f97\u4ed6\u4eec\u53ef\u4ee5\u5728\u4e0e\u4e4b\u5bf9\u5e94\u7684\u56feH\u4e0a\u8868\u73b0\u5f97independent\u3002(\u6bd4\u5982\uff0c\u5728\u7279\u5b9a\u7684\u53d6\u503c\u4e0b\uff0c\u6709\u53ef\u80fd\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u53ef\u4ee5\u770b\u4f5c\u72ec\u7acb\uff0c\u4f46\u7a0d\u5fae\u6539\u4e00\u4e0b\u6570\u5b57\uff0c\u53ef\u80fd\u5c31\u4e0d\u6ee1\u8db3\u72ec\u7acb\u7684\u5f0f\u5b50\u4e86)</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u7528\u4e0b\u9762\u8fd9\u4e2a\u5b9a\u7406\u6765\u89e3\u91caGibbs distribution\u7684\u5fc5\u8981\u6027\uff1a </p> <p>\u5f53H\u548cP\u5b8c\u5168\u7b49\u4ef7\u7684\u65f6\u5019\uff0c\u79f0H\u4e3aperfect map(\u5f88\u5c11\u89c1) </p> <p>\u56e0\u4e3apotential\u8981\u6c42positive\uff0c\u6240\u4ee5\u4ece\u5b9e\u8df5\u7684\u89d2\u5ea6\u6211\u4eec\u7528potential\u53cd\u6620\u8fd9\u4e2aunconstrained form\uff0c\u5728potential\u5916\u9762\u5957\u4e00\u4e2aexp(-x)\u53cd\u6620\u6211\u4eec\u60f3\u8981\u7684\u201cpositive\u7684potential\u201d\uff0c\u8fd9\u4e2aexp(-x)\u79f0\u4e3aenergy function\u3002 \u4e3a\u4e86\u65b9\u4fbf\uff0c\u6211\u4eec\u628a\u8fd9\u4e2a\u65bd\u52a0\u4e86energy function\u7684constrain\u4e86\u7684\u4e1c\u897f\u79f0\u4e3apotential\u3002 \u6b64\u65f6\u8fd9\u4e2anormalization form\u79f0\u4e3afree energy\u3002</p> <p></p> <p> \u8fd9\u4e2a\u6a21\u578b\u7684\u4f5c\u7528\uff1a\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u6a21\u578b\uff0c\u4ecedata\u4e2dlearn\u51fa\u4e00\u4e2agraph\u3002</p> <p>\u5f53data\u662fsparse\u7684\uff0c\u5bf9\u4e8eboltzman machine\uff0c\u5c31\u6709\u5f88\u591a\u53c2\u6570\u4e3a0\uff0c\u5bf9\u5e94\u5230graph\u4e2d\uff0c\u5c31\u6709\u5f88\u591aedge\u662f\u7f3a\u5931\u7684\u3002\u8fd9\u5c31\u5f97\u5230\u4e86ising-model\uff0c\u5b83\u5176\u5b9e\u662fboltzman machine\u7684\u7279\u6b8a\u60c5\u51b5\u3002 </p> <p> \u53ef\u4ee5\u770b\u5230RBM\u4e2d\u7528\u5230\u4e86singleton energy\u548cpairwise energy\u3002 RBM\u4e2d\u7684weight \u03b8\u662f\u672a\u77e5\u7684\uff0c\u6211\u4eec\u8981\u4ecedata\u4e2d\u5b66\u4e60\u51fa\u8fd9\u4e9bweight\u3002</p> <p>\u6027\u8d28\uff1a 1. visible unit\u6ca1\u6709observed\u65f6\uff0chidden factor\u90fd\u662fdependent\u7684 2. \u5f53\u6240\u6709\u7684visible unit\u90fdobserved\u65f6\uff0chidden factor\u4e4b\u95f4\u72ec\u7acb\u3002\u53cd\u4e4b\u4e5f\u6210\u7acb\u3002</p> <p>(\u53ef\u4ee5\u7528head to head\u6765\u89e3\u91ca)</p> <p> \u53ef\u4ee5\u628apairwise-energy\u5b9a\u4e49\u4e3asingleton energy\u7684\u4e58\u79ef\u518d\u4e58\u4e0a\u6743\u91cd\uff0c\u6765\u8fdb\u4e00\u6b65\u7b80\u5316\u6a21\u578b\u3002</p>"},{"location":"PGM/course/lecture03-BNrepresentation/","title":"lecture03-BNrepresentation","text":"<p> \u8fd9\u91cc\u6709\u4e00\u4e2a\u5f88\u597d\u7684head to head\u7684\u4f8b\u5b50\uff1a \u4e00\u4e2aparent\u8868\u793a\u949f\u8868\u662f\u5426\u574f\u6389\u4e86\uff0c\u53e6\u4e00\u4e2aparent\u8868\u793a\u662f\u5426\u5835\u8f66\uff0cchild\u8868\u793a\u662f\u5426\u8fdf\u5230\u3002\u672a\u89c2\u6d4b\u5230child\u65f6\u4e24\u4e2aparent\u662f\u72ec\u7acb\u7684\uff0c\u5f53\u89c2\u6d4b\u5230\u5df2\u7ecf\u8fdf\u5230\u65f6\uff0c\u4e24\u4e2aparent\u5219\u4f1a\u201c\u7ade\u4e89\u201d\u5f15\u8d77\u8fdf\u5230\u7684\u539f\u56e0\u3002\u5f53\u6211\u4eec\u53d1\u73b0\u5835\u8f66\u53d1\u751f\u4e86\uff0c\u90a3\u4e48\u5f88\u6709\u53ef\u80fd\u949f\u8868\u6ca1\u6709\u574f\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002</p> <p></p> <p> ancestral gragh\uff1a\u53ea\u4fdd\u7559node of interest\u548c\u5b83\u4eec\u7684\u7956\u5148 moralize\uff1a\u628a\u8fb9\u53d8\u4e3a\u65e0\u5411\u8fb9\uff0c\u7136\u540e\u8fde\u63a5coparent</p> <p>\u53e6\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u53ea\u9700\u8981\u539f\u672c\u7684gragh\uff0c\u4e0d\u9700\u8981moralize\u4e4b\u7c7b\u7684\uff1a bayes ball </p> <p>  \u548c\u65e0\u5411\u56fe\u7684\u7ed3\u8bba\u76f8\u4f3c\uff1a \u5982\u679c\u5bf9\u7740\u4e00\u4e2agraph\u5199\u51fadistribution P\uff0cgraph\u4e2d\u7684independence\u4e00\u5b9a\u4f1a\u51fa\u73b0\u5728P\u4e2d\u3002 \u5982\u679c\u6211\u4eec\u4eceP\u4e2d\u201c\u62fc\u51d1\u51fa\u201d\u4e00\u4e2aindependence\uff0c\u5219\u4e0d\u80fd\u4fdd\u8bc1\u8fd9\u4e9b\u53d8\u91cf\u5728graph\u4e2d\u53ef\u4ee5d-seperate\uff0c\u56e0\u4e3a\u53ef\u80fd\u53ea\u662f\u6570\u5b57\u4e0a\u7684\u5de7\u5408\u3002 \u6216\u8005\u8bf4\uff0c\u5982\u679c\u5728graph\u4e2d\u53d8\u91cf\u4e0d\u80fd\u88abd-seperate\uff0c\u4e0d\u4ee3\u8868\u5728P\u4e2d\u5c31\u4e00\u5b9adependent</p> <p></p>"},{"location":"PGM/course/lecture04-ExactInference/","title":"lecture04-ExactInference","text":"<p>\u6211\u4eec\u5728eliminate\u67d0\u4e00\u4e2a\u53d8\u91cfa\u7684\u65f6\u5019\uff0c\u5148\u628a\u6240\u6709\u4e0ea\u76f8\u5173\u7684term\u653e\u5230\u4e00\u8d77\uff0c\u7136\u540e\u628a\u8fd9\u4e9bterm\u5bf9a\u8fdb\u884csummation\uff0c\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u5173\u4e8e\u201c\u5269\u4f59term\u201d\u7684function m(b,c)\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3amessage  </p> <p>Variable Elimination \uff1a \u6211\u4eec\u53ef\u4ee5\u628a\u4e0a\u9762\u7684\u8fc7\u7a0b\u63a8\u5e7f\u5230graph\uff0c\u5c31\u662fVariable Elimination\u548csum product   \u3002\u3002\u3002 </p> <p> sum-product\u7684\u590d\u6742\u5ea6\u53d6\u51b3\u4e8eclique tree\u4e2d\u6700\u5927\u7684clique\u3002</p> <p> </p> <p> clique\u53ea\u8ddfelimination\u7684\u987a\u5e8f\u6709\u5173\uff0c\u800c\u4e0equery\u65e0\u5173\u3002\u56e0\u6b64\u6211\u4eec\u5728\u9009\u5b9a\u4e00\u4e2aelimination\u7684\u987a\u5e8f\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u628amessage\u5b58\u8d77\u6765\uff0c\u53ef\u4ee5\u91cd\u590d\u4f7f\u7528\u3002  </p> <p></p>"},{"location":"PGM/course/lecture05-ParameterEst/","title":"lecture05 ParameterEst","text":"<p> Note: natual parameter\u548csufficient statistic\u662f\u6700\u91cd\u8981\u7684\u90e8\u5206\uff0c\u5b83\u4eec\u4e4b\u95f4\u505a\u7684\u662finner product\u64cd\u4f5c\uff0c\u662flinear dependent\u7684\u3002 \u91cd\u8981\u6027\uff1a\u5173\u4e8e\u03b7\u7684parameter estimation\u53ea\u4e0eT(x)\u6709\u5173</p> <p> </p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9A\u6c42\u5bfc\u5f97\u5230T(x)\u7684moment\uff1a \u7136\u540emoment parameter\u4e5f\u5f80\u5f80\u4e0emoment\u6709\u5173\uff0c\u56e0\u6b64\u8fd9\u4e2a\u6027\u8d28\u6709\u52a9\u4e8e\u6211\u4eec\u8fdb\u884cparameter estimation  </p> <p> moment parameter \u03bc\u53ef\u4ee5\u7531natural parameter A\u5f97\u5230\uff0c \u7136\u540e\u89c2\u5bdf\u4e8c\u9636\u5bfc\u6570\uff0c\u56e0\u4e3a\u5b83\u5bf9\u5e94\u65b9\u5dee(\u5927\u4e8e0)\uff0c\u56e0\u6b64A\u662f\u4e00\u4e2a\u51f8\u51fd\u6570(\u4e00\u9636\u5bfc\u6570\u5355\u8c03\u9012\u589e)\u3002\u56e0\u6b64\u4e00\u9636\u5bfc\u6570\u6240\u5bf9\u5e94\u7684\u03bc\u53ef\u4ee5\u4e0e\u03b7\u4e00\u4e00\u5bf9\u5e94\u3002  \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u628a\u03b7\u548c\u03bc\u4e4b\u95f4\u5efa\u7acb\u4e00\u4e2a1to1\u7684\u51fd\u6570</p> <p>\u5728\u5bf9exponential family\u7684MLE\u4e2d\uff0c\u6211\u4eec\u4eec\u53ef\u4ee5\u76f4\u63a5\u8fd0\u7528\u4e0a\u9762\\(\\frac{dA(\\eta)}{d\\eta}=\\mu\\) \u8fd9\u4e00\u6027\u8d28\uff0c\u7b80\u4fbf\u5730\u5f97\u5230\\(\\mu_{MLE}\\). </p> <p>\u4e0b\u9762\u6765\u8bb2\u89e3\u628aexponential family\u7edf\u4e00\u5230natural parameter\u7684\u4f5c\u7528\uff1a</p> <p>\u56e0\u4e3adata\u548cparameter\u662flinear dependent\u7684\uff0c\u8fd9\u4e00\u53d8\u6362\u53ef\u4ee5explicitly\u5c55\u793a\u51fadata\u8981\u50cf\u8868\u73b0\u51fa\u8fd9\u4e2adistribution\u8981\u7ecf\u8fc7\u600e\u6837\u7684transformation\u3002(\u6bd4\u5982\u662f\u5426\u9700\u8981\u4fdd\u5b58\u6240\u6709\u7684data)</p> <p> \u5982\u4f55\u63cf\u8ff0Y\u7684\u5206\u5e03\uff1f \u901a\u5e38\u5bf9\u4e8eX\u65bd\u52a0\u4e00\u4e2aresponse function f()\uff0c\u7136\u540e\u5bf9\u4e8e\\(\\mu=f(X)\\)\u65bd\u52a0\u4e00\u4e2aexponential family\uff0c\u6765\u63cf\u8ff0Y \u5bf9\u4e8elinear regression\uff0cf\u4e3a\\(\\theta^TX+b\\) \u5bf9\u4e8elogistic regression\uff0cf\u4e3a\\(\\frac{1}{1+e^{-\\theta^TX}}\\)</p> <p>\u6211\u4eec\u628aconditional mean \u03bc\u8bbe\u4e3aresponse f(x)\uff0c\\(E_p(Y)=\\mu\\) </p> <p>\u5982\u679c\u6211\u4eec\u628af\u548c\\(\\psi\\)\u8bbe\u4e3a\u9006\u51fd\u6570\uff0c\u5219\u6b64\u65f6\\(\\theta^TX\\)\u5c31\u5bf9\u5e94\u7740\\(\\eta\\) </p> <p>\u6ce8\u610f\u4e0b\u9762\u8fd9\u4e9bmodel\u4f7f\u7528\u7684\u90fd\u662fcanonical response function  \u4e5f\u5c31\u662f\u8bf4\uff0c\u9690\u85cf\u7684f\u548c\u03c8\u5c31\u662f\u4e0a\u56fe\u4e2d\u7684\u5f62\u5f0f\uff0c\u53ea\u4e0d\u8fc7\u6700\u540e\u62b5\u6d88\u4e86\uff0c\u7136\u540e\u5c31\u6709\\(\\eta(x)=\\theta^Tx\\).</p> <p> \u81f3\u4e8e\u8fd9\u91cc\u867d\u7136\u4e0a\u9762\u90a3\u4e9bdistribution\u90fd\u5bf9\u5e94\u7740\\(\\eta(x)=\\theta^Tx\\)\uff0c\u4f46\u662f\u5728\u4e0a\u56fe\u4e2d\u7684likelihood\u4e2d\uff0ch(y)\u548cA(\u03b7)\u8fd8\u662f\u4e0d\u540c\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e9b\u4f53\u73b0\u51fadist\u7684\u4e0d\u540c\u3002</p> <p>\u5f53\u4f7f\u7528\u725b\u987f\u6cd5\u65f6\uff0c\u5229\u7528\u4e8c\u9636\u5bfc\u6570\uff0c\u6b64\u65f6\u6211\u4eec\u5fc5\u987b\u77e5\u9053\\(\\psi\\)\uff0c\u6ca1\u6cd5\u62b5\u6d88\u6389\u4e86\uff0c\u597d\u5728\u53ef\u4ee5\u5229\u7528\u4e4b\u524d\u7684\u90a3\u5f20\u8868\u627e\u5230\u5e38\u89c1\u7684dist\u5bf9\u5e94\u7684\\(\\psi\\)</p> <p> </p> <p></p> <p> \u6ce8\u610f\u4e0a\u56fe\u4e2d\u5982\u679c\u4e00\u4e2anode\u6709\u4e24\u4e2aparent\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2anode\u7684\u5f71\u54cd\u5f80\u5f80\u8fd8\u662f\u53ef\u4ee5\u5199\u6210\u76f8\u4e58\u7684\u5173\u7cfb\uff0c\u90a3\u4e48\u5728log likelihood\u4e2d\u5c31\u6210\u4e3a\u4e86\u76f8\u52a0\u7684\u5173\u7cfb\u3002</p>"},{"location":"PGM/course/lecture05-ParameterEst/#em","title":"EM","text":"<p>\u7528\u4e8epartial observed model In the e-step, we replace the sufficient statistic of hidden variable with their expection In the m-step, we just treat everything as observed\uff0c\u7136\u540e\u8fdb\u884cMLE</p>"},{"location":"PGM/course/lecture06-HMMCRF/","title":"lecture06 HMMCRF","text":""},{"location":"PGM/course/lecture06-HMMCRF/#hmm","title":"HMM","text":"<p>\u4e4b\u524d\u8bb2\u7684mixture model\u4e2d\uff0c\u6211\u4eec\u4e00\u6b21\u62e5\u6709\u5168\u90e8\u7684data \u73b0\u5728\u5047\u8bbe\u6211\u4eec\u9010\u4e2a\u5f97\u5230\u6570\u636e\uff1a </p> <p> \u6ce8\u610f\u4e0b\u56fe\u4e2d\u7684\u63a8\u5bfc\uff0c\u7531\u4e8emarkov property\uff0c\u89c2\u6d4b\u5230\\(y_t\\)\u4e4b\u540e\uff0ct\u4e4b\u524d\u7684x\u548ct\u4e4b\u540e\u7684x\u5c31\u662f\u6761\u4ef6\u72ec\u7acb\u7684 </p> <p>\u5982\u679c\u6211\u4eec\u60f3inference\u67d0\u4e00\u4e2a\u65f6\u523b\u7684\u6982\u7387given\u4e00\u6574\u4e2asequence\uff0c\u90a3\u4e48\u53ef\u4ee5\u76f4\u63a5\u628a\u524d\u5411\u548c\u540e\u5411\u7684message \u03b1\u548c\u03b2\u76f4\u63a5\u76f8\u4e58  \u5982\u679c\u6211\u4eec\u60f3\u5f97\u5230\u67d0\u4e00\u4e2apair\u7684joint\uff0cgiven\u6574\u4e2asequence\uff1a\\(p(y_t^i,y_{t+1}^i|x_1,...,x_T)\\) </p> <p>\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u7b97\u5355\u4e00latent variable\u7684MPA </p> <p>\u5982\u679c\u6211\u4eec\u60f3\u6c42\u4e00\u4e2apair\u7684joint\u7684MPA\uff0c\u90a3\u4e48\u4e0d\u80fd\u5206\u522b\u6c42t\u548ct+1\u7684MPA\uff0c\u56e0\u4e3a\u8fd9joint\u7684configuration\u548c\u5355\u72ec\u7684configuration\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u6bd4\u5982\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50 </p> <p>\u5982\u679c\u6211\u4eec\u60f3\u6c42\u6240\u6709hidden state\u7684posterior\uff0c\u7406\u8bba\u4e0a\u6211\u4eec\u5c31\u662f\u60f3\u6c42\\(p(y_1,...,y_T|x_1,...,x_T)\\)\uff0c\u4f46\u662f\u8fd9\u4e2a\u4e1c\u897f\u6ca1\u6cd5\u5b58\u3002 </p> <p></p> <p>  MLE\u7684\u7f3a\u70b9\u662fover fitting\uff0c\u5f53\u67d0\u4e00\u4e2a\u7ec4\u5408\u5728data\u4e2d\u6ca1\u6709\u51fa\u73b0\uff0c\u5c31\u4f1a\u5bfc\u81f4\u6027\u80fd\u5f88\u5dee\u3002\u56e0\u6b64\u6709pseudocounting\uff0c\u4e5f\u5c31\u662f\u589e\u52a0\u4e00\u5b9a\u7684\u6570\u76ee\uff0c\u4f7f\u5f97count\u4e0d\u4f1a\u4e3a0. \u800c\u8fd9\u76f8\u5f53\u4e8e\u65bd\u52a0conjugate prior  </p>"},{"location":"PGM/course/lecture06-HMMCRF/#crf","title":"CRF","text":"<p>\u6211\u4eec\u53d1\u73b0\u5728learning\u4e2d\uff0c\u628alikelihood\u5bf9\u4e8e\u53c2\u6570\u6c42\u5bfc\u4e4b\u540e\u4f1a\u53d1\u73b0\u4e00\u4e2a\u5947\u602a\u7684\u7ed3\u679c\u3002\u539f\u672c\u6211\u4eec\u662ffully observed\u7684\uff0cx\u548cy\u90fd\u77e5\u9053\uff0c\u4f46\u5728\u68af\u5ea6\u4e2d\uff0c\u7b2c\u4e00\u9879\u662fcounting\u7684\u90e8\u5206\uff0c\u7b2c\u4e8c\u9879\u662fcounting\u7684\u671f\u671b\uff0c\u800c\u8fd9\u91cc\u6211\u4eec\u8981\u5047\u88c5\u6211\u4eec\u4e0d\u77e5\u9053y\uff0c\u7136\u540e\u53bb\u6c42\u8fd9\u4e2a\u671f\u671b\u3002\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u5176\u5b9e\u8981\u505a\u4e00\u4e2ainference\uff0c\u4e3a\u4e0d\u5149\u662f\u7b80\u5355\u7684counting\u3002 </p> <p> \u6700\u7ec8\u7684\u6c42\u89e3\u53ef\u4ee5\u7528gradient ascent\uff0c\u5176\u4e2d\u671f\u671b\u53ef\u4ee5\u7528sum-product\u3002 \u800c\u6574\u4f53\u4e0a\u770b\uff0c\u5176\u5b9e\u76f8\u5f53\u4e8e\u4f7f\u7528\u4e86\u4e00\u4e2aEM\uff0c\u5c3d\u7ba1y\u5176\u5b9e\u662fobserved\u7684</p>"},{"location":"PGM/course/lecture07-VI1/","title":"lecture07-VI1","text":""},{"location":"PGM/course/lecture08-VI2/","title":"lecture08 VI2","text":""},{"location":"PGM/course/lecture08-VI2/#theory-of-variational-inference","title":"Theory of Variational Inference","text":"<p>variational\u5c31\u662f\u628a\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\u7684\u65b9\u6cd5\u3002 \u5728graphical model\u4e2d\uff0cinference\u6216\u8005\u8bf4partition function\u5f80\u5f80\u7279\u522b\u96be\u7b97\uff0c\u7ecf\u5e38\u662f\u6307\u6570\u7ea7\u522b\u7684\u590d\u6742\u5ea6\uff0c\u56e0\u6b64\u6211\u4eec\u5c1d\u8bd5\u628apartition function\u5199\u6210variational\u7684\u5f62\u5f0f </p> <p>\u8fd9\u4e2a\u65f6\u5019\u6211\u4eec\u53ef\u4ee5\u63a5\u89e6exponential family\u7684\u6027\u8d28\uff0c\u56e0\u4e3aexponential\u7684\u8868\u8fbe\u4e2d\u5c31\u6709\u4e00\u4e2a\u73b0\u6210\u7684normalization term\uff0cA</p> <p> \u5176\u4e2d\u03b8\u8981\u4fdd\u8bc1\u4e0d\u4f1a\u4f7fA\u8d8b\u4e8e\u65e0\u7a77\uff0c\u56e0\u6b64\u03b8\u5728\u4e00\u4e2a\u6709\u9650\u7684\u96c6\u5408\u4e4b\u5185 </p> <p>\u4e3a\u4ec0\u4e48\u8981\u9009exponential family\uff1f \u56e0\u4e3a\u03bc=sufficient statistic\u7684\u671f\u671b=x\u7684margin\uff0c\u800cmargin\u6b63\u662f\u6211\u4eec\u60f3\u7b97\u7684\u3002</p> <p> \u6b64\u5916normalization term\u4e5f\u6709\u6240\u5bf9\u5e94 </p> <p>\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u76f4\u63a5\u5bf9\u7740X\u6c42\u671f\u671b\u5f97\u5230\u03bc\u5c31\u597d\u4e86\uff1f \u56e0\u4e3a\u901a\u5e38\u4e3a\u6307\u6570\u7ea7\u8fd0\u7b97\uff0c\u6211\u4eec\u60f3\u8981\u7528\u4e00\u4e2aincremental\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u56e0\u6b64\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u628a\u8fd9\u4e2a\u95ee\u9898\u5199\u6210variational\u7684\u5f62\u5f0f\u3002</p> <p> \u6ce8\u610f\u4e0a\u9762\u8fd9\u4e2a\u8868\u8fbe\u5f0f\u5176\u5b9e\u5728PRML\u4e2d\u5df2\u7ecf\u89c1\u8fc7\uff1a  </p> <p>\u63a5\u4e0b\u6765\u7684\u6240\u6709\u5de5\u4f5c\uff0c\u90fd\u662f\u8981\u628aA(\u03b8)\u5199\u6210dual of dual\u7684\u5f62\u5f0f  \u4f46\u662f\u6211\u4eec\u8981\u77e5\u9053\u600e\u4e48\u6c42A*(\u03bc)\uff0c\u5426\u5219\u6ca1\u6cd5\u7b97</p> <p>\u4e0b\u9762\u4e3e\u4e00\u4e2a\u6c42Bernoulli\u7684A*(\u03bc)\u7684\u4f8b\u5b50\uff1a \u5982\u679c\u901a\u8fc7\u6b63\u5e38\u7684inference\uff0c\u6211\u4eec\u4f1a\u5f97\u5230\u4e0b\u9762\u7684\u7ed3\u679c\uff1a  \u4e0b\u9762\u901a\u8fc7conjugate\uff0c\u6c42stationary condition\uff1a  \u4f46\u662f\u6ce8\u610f\u8fd9\u91cc\u7684\u03bc\u662f\u6709\u503c\u57df\u7684\uff1a \u7136\u540e\u6211\u4eec\u5c31\u5f97\u51fa\u4e86conjugate\u7684\u5b8c\u6574\u5f0f\u5b50\uff1a  \u6709\u4e86conjugate\uff0c\u6211\u4eec\u5c31\u80fd\u628a\u539f\u672c\u7684A\u5199\u6210dual of dual\uff0c\u7136\u540e\u6c42\u89e3\uff0c\u6700\u540e\u6c42\u5f97\u7684\u7ed3\u679c\u8ddf\u76f4\u63a5\u7b97inference\u662f\u4e00\u6837\u7684\uff1a </p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u63a8\u5e7f\u5230\u6574\u4e2aexponential family\uff1a  \u7c7b\u6bd4\u521a\u624dBernoulli\u7684\u4f8b\u5b50\uff0c\u8fd9\u91cc\u7684\u03bc\u5176\u5b9e\u662f\u6709\u503c\u57df\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u73b0\u5728\u5c31\u6765\u786e\u5b9a\u4e00\u4e0b\u63a8\u5e7f\u4e4b\u540e\u503c\u57df\u600e\u4e48\u627e\u3002</p> <p>\u73b0\u5728\u5047\u8bbe\u03bc\u5728\u503c\u57df\u5185\uff0c\u662fvalid\u7684   \u89c2\u5bdf\u8fd9\u4e2a\u5f0f\u5b50\uff0c\u5b9e\u9645\u4e0a\u662f\u539f\u672cdistribution\u7684negative entropy\u3002 </p> <p>\u81f3\u6b64\uff0c\u6211\u4eec\u628a\u6c42\u03bc\u7684\u95ee\u9898\u8f6c\u5316\u4e3a\u4e86\u6c42A*(\u03bc)\u3001\u8fdb\u800c\u6c42A(\u03bc)\u7684\u95ee\u9898\u3002 \u4f46\u8fd9\u6837\u4e5f\u6ca1\u6709\u5b8c\u5168\u51cf\u5c11\u4ed6\u7684\u8ba1\u7b97\u91cf\uff0c\u539f\u672c\u662f\u79ef\u5206\u8ba1\u7b97\u91cf\u5927\uff0c\u73b0\u5728\u6c42inverse of gradient\u548c\u6c42entropy\u7684\u65f6\u5019\u8ba1\u7b97\u91cf\u4e5f\u5f88\u5927\u3002 </p> <p>\u63a5\u4e0b\u6765\u5173\u6ce8\u4e24\u4e2a\u95ee\u9898\uff1a 1. \u03bc\u4ec0\u4e48\u65f6\u5019\u6709\u89e3\uff1f 2. \u5982\u4f55approximate \u03bc\u7684\u89e3\u7a7a\u95f4\uff0c\u4f7f\u5f97\u6211\u4eec\u6c42\u89e3\u6574\u4e2a\u95ee\u9898\u65f6\u66f4\u597d\u7b97\uff1f</p> <p>\u03bc\u4ec0\u4e48\u65f6\u5019\u6709\u89e3\uff1f  \u03bc\u53ef\u4ee5\u770b\u4f5c\u662f\u03c6\u7684\u52a0\u6743\u6c42\u548c\uff0c\u7136\u540e\u6982\u7387\u53c8\u6ee1\u8db3\u4e00\u5b9a\u7684\u7ea6\u675f\uff0c \u4e8b\u5b9e\u4e0a\uff0c\u03bc\u662f\u03c6\u7684Convex combination(\u51f8\u7ec4\u5408\u6307\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u8981\u6c42\u6240\u6709\u7cfb\u6570\u90fd\u975e\u8d1f\u4e14\u548c\u4e3a1). \u56e0\u6b64\u8fd9\u4e2amarginal polytope\u5c31\u662fextreme points of sufficient statistics\u7684Convex hull </p> <p>\u6839\u636e\u8fd9\u4e2a\u5b9a\u7406\uff0c\u8fd9\u4e2apolytope\u53ef\u4ee5\u5199\u6210finite\u4e2alinear inequality\u7684\u7ec4\u5408  \u770b\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\uff0c \u8fd9\u91cc\u6211\u4eec\u628asufficient statistics\u8bbe\u4e3asingleton\u548cpairwise\u7684\u62fc\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6700\u7ec8\u7ed3\u679c\u662f\u03c6\u7684\u6781\u503c\u70b9\u6240\u6784\u6210\u7684convex hull\uff0c\u800c\u4e14\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5199\u62104\u4e2a\u4e0d\u7b49\u5f0f\u3002   \u4f46\u662f\u73b0\u5728\u7684\u95ee\u9898\u662f\uff0c\u4e4b\u524d\u90a3\u4e2a\u5b9a\u7406\u53ea\u4fdd\u8bc1linear inequality\u6709finite\u4e2a\uff0c\u7136\u800c\u8fd9\u4e2a\u4e2a\u6570\u5176\u5b9e\u662f\u5f88\u5927\u7684\u3002 \u53ef\u4ee5\u770b\u5230\u4e0b\u56fe\u4e2d\u7684\u7ed3\u8bba\uff0ctree graphical model\u7684\u7ea6\u6570\u4e2a\u6570\u662f\u968f\u7740graph size\u7ebf\u6027\u589e\u957f\u7684(\u8fd9\u4e5f\u89e3\u91ca\u4e86tree\u7ed3\u6784\u7684\u597d\u5904) </p> <p>\u56e0\u6b64\u4e0b\u9762\u5c31\u6765\u770b\u5982\u4f55approximate\u8fd9\u4e2apolytope</p> <p>\u5982\u4f55approximate \u03bc\u7684\u89e3\u7a7a\u95f4\uff1f </p>"},{"location":"PGM/course/lecture08-VI2/#mean-field-approximation","title":"Mean field approximation","text":"<p> \u6211\u4eec\u5462\u53ef\u4ee5\u53bb\u6389graph\u4e2d\u7684\u4e00\u4e9b\u8fb9\uff0c\u6216\u8005\u6240\u6709\u8fb9\uff0c\u7136\u540e\u7528\u65b0\u7684\u03bc\u7a7a\u95f4\u6765\u8fd1\u4f3c\u771f\u6b63\u7684\u03bc\u7a7a\u95f4\u3002</p> <p>\u53bb\u6389\u6240\u6709\u8fb9\u65f6\uff0c\\(\\theta_{ij}=0\\), \\(\\mu_{ij}=P(x_i,x_j)=P(x_i)P(x_j)=\\mu_i\\mu_j\\)</p> <p>\u4ece\u8fd9\u4e2a\u89c6\u89d2\uff0c\u6211\u4eec\u53ef\u4ee5\u628amarginal\u7684approximation\u8f6c\u5316\u4e3a\u5bf9\u03b8\u7684\u96c6\u5408\u7684\u8fd1\u4f3c\u6216\u8005\u5bf9\u03bc\u7684\u96c6\u5408(polytope)\u7684\u8fd1\u4f3c.</p> <p> \u53ef\u4ee5\u770b\u5230\uff0c\u7531\u4e8e\u8fd9\u65f6\u7684\u03b8\u53ea\u662f\u771f\u6b63\u7684\u03b8\u96c6\u5408\u7684\u5b50\u96c6(\u56e0\u4e3a\u589e\u52a0\u4e86\u7ea6\u675f)\uff0c\u56e0\u6b64\u5bf9\u5e94\u5230polytope\u4e0a\u5c31\u53d8\u6210\u4e86inner approximation\u3002</p> <p> \u5f53\u6211\u4eec\u53bb\u6389\u4e86\u6240\u6709\u7684\u8fb9\uff0c\u6211\u4eec\u5c31\u662f\u7528\u6240\u6709\u53d8\u91cf\u5728Q\u4e0a\u7684margin\u7684\u8fde\u4e58\u53bb\u8fd1\u4f3c\u6574\u4e2ajoint\uff0c\u7531\u6b64\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5199\u51faA*\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u8fd1\u4f3c\u7684entropy\uff0c\u5176\u5b9e\u7b49\u4e8e\u6240\u6709\u53d8\u91cf\u5728Q\u4e0a\u7684margin\u7684extropy\u4e4b\u548c\u3002 \u800c\u5728\u539f\u672c\u7684P\u4e0a\uff0c\u6211\u4eec\u662f\u6c42\u4e0d\u51fa\u6765\u6574\u4e2ajoint\u7684entropy\u7684\u3002</p> <p></p>"},{"location":"PGM/course/lecture08-VI2/#bethe-approximation-and-sum-product","title":"Bethe Approximation and Sum-Product","text":"<p>\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2agraph\uff0c  \u6ce8\u610f\u4e0a\u56fe\u4e2d\u7684\u8fd9\u4e24\u4e2a\u6761\u4ef6\uff1asingleton\u7684margin\u548c\u4e3a1\uff1bpairwise potential\u5728\u5bf9\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u91cfsum\u4e4b\u540e\u5f97\u5230\u53e6\u4e00\u4e2a\u53d8\u91cf\u7684margin \u6211\u4eec\u7684\u03bc\u8981\u4fdd\u8bc1local consistency\uff0c\u4f46\u662f\u6211\u4eec\u4e22\u6389global consistency\u548c\u5176\u4ed6\u7684\u5404\u79cd\u7ea6\u675f\u3002 \u56e0\u6b64\u6211\u4eec\u7684\u96c6\u5408\u5176\u5b9e\u662f\u6bd4marginal polytope\u5927\uff0c\u662f\u4e00\u4e2aouter bound\u3002 \u5728\u6c42A*(\u03bc)\u65f6\uff0c\u6211\u4eec\u7528tree\u7684entropy\u6765\u8fd1\u4f3c\u4efb\u610fgraph\u7684entropy </p>"},{"location":"PGM/course/lecture09-MC/","title":"lecture09-MC","text":"<p> \u7528sample\u7684\u65b9\u6cd5\u8868\u8fbe\u4e00\u4e2adistribution\uff0c\u5c31\u662f\u4fdd\u5b58\u4e00\u5806\u70b9\u4ee5\u53ca\u5176\u5bf9\u5e94\u7684\u51fd\u6570\u503c\u3002\u6c42\u671f\u671b\u7684\u8bdd\u5c31\u5bf9\u5e94\u8fd9\u4e9b\u70b9\u7684\u671f\u671b\u3002</p> <p></p> <p> \u6211\u4eec\u60f3\u8fd1\u4f3c\u4e00\u4e2a\u5206\u5e03\u03a0\uff0c\u4f46\u662f\u6211\u4eec\u53ea\u7528\u5b83\u7684unnormalized\u90e8\u5206\u03a0'(X)\u3002 \u7136\u540e\u6211\u4eec\u4ece\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u5206\u5e03Q(X)\u4e2d\u91c7\u6837\uff0c\u8ba1\u7b97 \\(\\Pi(x^*)/kQ(x^*)\\)\uff0ck\u662f\u4e00\u4e2a\u5b9a\u503c\u4f7f\u4e0a\u9762\u8fd9\u4e2a\u503c\u5904\u57280,1\u4e4b\u95f4\u3002</p> <p>\u7136\u540e\u901a\u8fc7\u4e0a\u56fe\u4e2d\u7684\u8bc1\u660e\uff0c\u53ef\u4ee5\u770b\u5230\uff1a\u6211\u4eec\u6700\u521d\u91c7\u6837\u7684x\u5176\u5b9e\u53ea\u6709\u4e00\u90e8\u5206\u80fd\u4fdd\u7559\u4e0b\u6765\uff0c\u800c\u4fdd\u7559\u4e0b\u6765\u7684\u8fd9\u4e9b\u91c7\u6837\u70b9\u6240\u5bf9\u5e94\u7684\u6982\u7387\u6b63\u597d\u7b49\u4e8e\u03a0</p> <p>\u7f3a\u70b9\uff1a  \u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u6211\u4eec\u7684Q\u4e0e\u771f\u6b63\u7684P\u5f88\u63a5\u8fd1\uff0c\u4e5f\u4f1a\u4f7f\u5f97k\u975e\u5e38\u5927</p> <p> \u628a \\(\\frac{P(x^*)}{Q(x^*)}\\) \u5f53\u4f5cweight\uff0c\u4e3a\u6bcf\u4e00\u4e2a\u91c7\u6837\u70b9\u90fd\u4fdd\u5b58\u8fd9\u6837\u4e00\u4e2aweight\u3002 \u7f3a\u70b9\uff1a\u9700\u8981\u4f7f\u7528normalized P\uff0c\u5bf9\u4e8emarkov random field\uff0c\u7528\u4e0d\u4e86\u3002</p> <p> \u6211\u4eec\u53ef\u4ee5\u7528unnormalized P'\u4ee3\u66ffP\uff0c\u03b1\u53ef\u4ee5\u7528\u4e0a\u9762\u7684\u516c\u5f0f\u6c42\u51fa\uff0c\u6b64\u65f6\u6211\u4eec\u7ecf\u8fc7\u63a8\u5bfc\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2anormalized\u7684weight</p> <p>\u7f3a\u70b9\uff1a\u4e4b\u524d\u7684\u7248\u672c\u53ef\u4ee5\u7528\u4f5conline algorithm\uff0c\u6bcf\u4e00\u4e2a\u70b9\u7684weight\u90fd\u53ef\u4ee5\u7acb\u5373\u5f97\u5230\u3002\u4f46\u662f\u5728\u8fd9\u4e2a\u7248\u672c\uff0c\u6211\u4eec\u9700\u8981sample\u82cf\u591f\u591a\u7684\u70b9\u4e4b\u540e\u624d\u80fd\u5f97\u5230weight</p> <p>importance sampling\u53ef\u80fd\u505c\u5728\u4e00\u4e2a\u5f88\u574f\u7684\u7ed3\u679c\u4e0a  \u5982\u679cP\u548cQ\u5dee\u522b\u5f88\u5927\uff0c\u5927\u90e8\u5206\u65f6\u5019\u6211\u4eec\u90fd\u5728Q\u5927\u7684\u5730\u65b9sample\uff0c\u4f46\u662f\u8fd9\u65f6P/Q\u5f88\u5c0f\uff0c\u4f7f\u5f97\u65b0\u7684\u70b9\u7684weight\u5f88\u5c0f\uff0c\u5bf9\u4e8e\u6574\u4e2a\u7ed3\u679c\u6ca1\u6709\u4ec0\u4e48\u5f71\u54cd\uff0c\u6700\u540e\u7ed3\u679c\u5c31\u5f88\u5dee\u3002</p> <p> \u901a\u8fc7\u5728sample\u4e0a\u518dsample\uff0cP/Q\u5c0f\u7684\u90e8\u5206\u5c31\u4f1a\u6709\u5f88\u5c0f\u7684\u51e0\u7387\u518d\u88absample\u4e86 </p> <p> </p> <p> </p> <p>Note\uff1a A usual choice is to let Q(x\u2223y) be a Gaussian distribution centered at y, so that points closer to  y are more likely to be visited next</p> <p></p> <p></p> <p>Notes: \u4eba\u4eec\u7ecf\u5e38\u4e22\u6389\u524d\u51e0\u5343\u4e2asample  \u56e0\u4e3a\u5f53t\u8fc7\u5c0f\u7684\u65f6\u5019\uff0c\u6574\u4e2aQ(x'|x)\u53ef\u80fd\u5361\u5728\u4e00\u4e2abad\u533a\u57df\uff0c\u6ca1\u6709\u5f00\u59cb\u5728\u6574\u4e2a\u7a7a\u95f4\u8fd0\u52a8</p> <p>\u4e3a\u4ec0\u4e48\u4f1a\u6536\u655b\uff1f </p> <p>\u5173\u4e8e\u6536\u655b\u7684\u8bc1\u660e\u9700\u8981\u7528\u5230markov chain\uff1a </p> <p>\u5728\u4e4b\u524d\u7684MC\u4e2d\uff0c\u6bcf\u4e00\u4e2asample\u62bd\u53d6\u81eaQ\uff0c\u4f46\u662f\u5b83\u4eec\u6700\u7ec8\u88ab\u62bd\u53d6\u7684\u6982\u7387\u4e5f\u7b49\u4e8eP\u3002\u56e0\u4e3aQ\u53ef\u4ee5\u88ab\u7ea6\u6389\uff0c\u6240\u4ee5\u5176\u5b9eQ\u957f\u4ec0\u4e48\u6837\u5e76\u6ca1\u6709\u5f88\u91cd\u8981\u3002 \u800c\u5728MCMC\u4e2d\uff0c\u6ce8\u610f\u5230\u6211\u4eec\u7684Q(x'|x)\u662f\u4e00\u76f4\u5728\u53d8\u5316\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5173\u6ce8Q\u7684\u72b6\u6001\u3002   \u5173\u4e8estationary distributions\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u5b83\u7c7b\u6bd4\u4e3a\u77e9\u9635\u4e58\u6cd5\u3002 \u5f53\u7cfb\u7edf\u4e0d\u518d\u53d8\u5316\uff0c\u4ee3\u8868\u7740 \\(\\pi=T\\cdot \\pi\\)\uff0c\u53ef\u4ee5\u770b\u5230\\(\\pi\\)\u53ef\u4ee5\u770b\u4f5c\u662feigenvector\u3002\u800c\u4e0d\u662f\u6240\u6709\u7684matrix\u90fd\u6709eigenvector\u3002\u56e0\u6b64\u4e0d\u662f\u6240\u6709\u7684transition\u90fd\u80fdlead to stationary\uff0c\u4f46\u662f\u67d0\u4e9b\u597d\u7684transition\u53ef\u80fd\u4f1a\u5bfc\u81f4stationary\u3002</p> <p>\u4e0b\u9762\u8981\u63d0\u5230\u51e0\u4e2aMC\u7684\u6982\u5ff5\uff1a  \u4e5f\u5c31\u662f\u8bf4\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u4ece\u4e00\u4e2astate\u53ef\u4ee5\u5728\u6709\u9650\u7684step\u4e4b\u540e\u79fb\u52a8\u5230\u53e6\u4e00\u4e2astate\u3002\u4fdd\u8bc1\u4e86\u6211\u4eec\u662f\u6709\u53ef\u80fd\u8fbe\u5230stationary\u7684\u3002</p> <p> MC\u53ef\u80fd\u4ecestate i\u5230state j\uff0c\u518d\u4ecestate j\u76f4\u63a5\u56de\u5230state i\uff0c\u800c\u4e0d\u662f\u7ecf\u8fc7\u4e00\u4e2a\u5f88\u957f\u7684cycle\u518d\u56de\u5230state i\u3002</p> <p> \u7136\u540e\u5b9a\u4e49\u4e00\u4e2a\u6982\u5ff5\uff0c\u79f0\u6ee1\u8db3\u4e0a\u9762\u4e24\u4e2a\u6027\u8d28\u7684MC\u4e3aergodic\u7684\u3002</p> <p> \u5982\u679c\u4e00\u4e2aMC\u6ee1\u8db3ergodic\uff0c\u90a3\u4e48\u5b83\u5c31\u53ef\u4ee5\u8fbe\u5230stationary(\u5982\u679c\u6709\u7684\u8bdd)</p> <p> \u5982\u679c\u4e00\u4e2aMC\u6ee1\u8db3detailed balance\uff0c\u90a3\u4e48\u5b83\u5c31\u6709stationary</p> <p>\u6240\u4ee5\uff0c\u4e00\u4e2aMC\u8981\u60f3work\uff0c\u6211\u4eec\u5c31\u8981\u6784\u9020\u5408\u9002\u7684T\uff0c\u4f7f\u5f97MC\u6709detailed property\uff0c\u7136\u540e\u8fd9\u4e2aMC\u5c31\u53ef\u4ee5converge somewhere\u3002\u6700\u597d\u662f\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u5408\u9002\u7684T\u4f7f\u5f97\u6700\u7ec8\u6536\u655b\u5230P\u3002</p> <p>Metropolis-Hastings\u6536\u655b\u7684\u8bc1\u660e\uff1a</p> <p> </p> <p></p> <p>Gibbs sampling:</p> <p>idea: My new example is all the same as the previous one except one dimention. And that dimention is draw from a conditional from the remaining dimentions.  \\(x_i^{t}\\sim P(x_i|x^{t-1}_{\\{-i\\}})\\)  \u7136\u540e\uff0c\u56e0\u4e3a\u6709markov blanket\uff0cdraw next sample\u7684\u8ba1\u7b97\u4f1a\u5f88\u7b80\u5355 </p> <p></p> <p> </p> <p> \u53ef\u4ee5\u7531\u6b64\u7b97\u51fa\u4e00\u4e2a\u7cfb\u6570\uff0c\u7531\u6b64\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff0c\u5f53\u6211\u4eecsample\u4e86100\u4e2a\u70b9\u65f6\uff0c\u5176\u5b9e\u76f8\u5f53\u4e8esample\u4e86\u591a\u5c11\u4e2aindependent\u7684\u70b9 </p> <p> \u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u662f\uff0c\u6bd4\u8f83\u591a\u6b21MC\u7684\u7ed3\u679c\uff0c\u5982\u679c\u7ed3\u679c\u5dee\u4e0d\u591a\u8bf4\u660e\u6536\u655b\u4e86  \u6216\u8005\u753b\u51falog likelihood </p> <p> </p>"},{"location":"PGM/course/lecture10-MCMC-opt/","title":"lecture10-MCMC-opt","text":"<p>MCMC\u6240\u5b58\u5728\u7684\u95ee\u9898\uff1a 1 \u56fe\u4e2d\uff0c\u4ece\u5f53\u524d\u4f4d\u7f6e\u8fdb\u884crandom walk\uff0c\u5f88\u5927\u6982\u7387\u4f1a\u8d70\u5230\u4e00\u4e2a\u4f4e\u6982\u7387\u7684\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u4f1a\u5bfc\u81f4reject\u5f88\u591asample </p> <p>2 </p> <p>Random walk can have poor acceptance rate </p>"},{"location":"PGM/course/lecture10-MCMC-opt/#using-gradient-information","title":"Using gradient information","text":"<p>Hamiltonian Monte Carlo</p> <p> \u6211\u4eec\u60f3\u5efa\u7acb\u4e00\u4e2a\u5173\u4e8e\u4f4d\u7f6ex\u7684\u5206\u5e03\uff0c\u4ee5\u8868\u793a\u5176\u80fd\u91cf\u3002\u4f46\u8fd9\u4e2adistribution\u4e0d\u597d\u5199\u3002 \u6211\u4eec\u5c31\u5148\u5f15\u5165hamitonian\uff0c\u7b97\u51fa\u67d0\u4f4d\u7f6e\u7684\u52a8\u80fd\u4e0e\u52bf\u80fd\u4e4b\u548c\uff0c\u7136\u540e\u5bf9\u4e8e\u4f4d\u7f6e\u548c\u901f\u5ea6\u5efa\u7acb\u4e00\u4e2adistribution\uff0c\u8fd9\u6837\u5206\u5e03\u7684\u5f62\u5f0f\u4f1a\u7b80\u5355\u4e9b\u3002</p> <p>  (\u6211\u4eec\u628a\u4f4d\u7f6e\u7684notation\u4ecex\u6362\u6210q\uff0c\u52a8\u91cf\u7684p\u8fd8\u662fp)</p> <p>\u6211\u4eec\u5f15\u5165\u8fd9\u4e2a\uff0c\u662f\u4e3a\u4e86\u628a\u4e0a\u56fe\u4e2d\u7684\u8fd9\u68af\u5ea6\u7528\u5230MCMC\u7684\u66f4\u65b0\u5f53\u4e2d\u3002\u539f\u672c\u4e0b\u4e00\u4e2asample\u53ea\u4e0eT(x)\u6709\u5173\uff0c\u73b0\u5728\u6211\u4eec\u60f3\u8ba9T(x)\u548c\u68af\u5ea6\u5171\u540c\u51b3\u5b9a\uff0c\u4ece\u800c\u63a7\u5236\u65b9\u5411\u3002</p> <p>\u4e0b\u9762\u662f\u4e00\u4e2a\u7269\u7406\u4e2dHamiltonian\u7684\u4f8b\u5b50\uff0c\u968f\u4fbf\u770b\u4e00\u4e0b\u5c31\u597d\uff1a  \u6ce8\u610f\u5728\u6211\u4eec\u7684\u4efb\u52a1\u4e2d\uff0cU(q)\u662ftrue target distribution P\uff0cK(p)\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u8f85\u52a9\u7684\u65b9\u7a0b\uff0c\u6211\u4eec\u5e38\u7528\\(p^2/2\\).  \u56e0\u6b64p\u53ea\u662f\u4e00\u4e2a\u8f85\u52a9\u91cf\uff0c\u6211\u4eec\u771f\u6b63\u5173\u6ce8\u7684\u662f \\(q_1,...,q_t,q_{t+1}\\) \u800c\u4e0a\u56fe\u4e2d\u4e00\u4e2a\u53d8\u91cf\u7684\u5fae\u5206\u53ef\u4ee5\u8f6c\u6362\u6210\u53e6\u4e00\u4e2a\u53d8\u91cf\u7684\u68af\u5ea6\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u4ea4\u53c9\u66f4\u65b0\u8fd9\u4e24\u4e2a\u53d8\u91cf\u4e86\u3002</p> <p> \u4f46\u662f\u4e0a\u9762\u8fd9\u79cd\u65b9\u6cd5\u662f\u53d1\u6563\u7684\u3002\u5b9e\u9645\u4e0a\u4e0a\u9762\u7684\u8fc7\u7a0b\u6240\u5bf9\u5e94\u7684\u77e9\u9635\uff0c\u884c\u5217\u5f0f&gt;1\uff0c\u56e0\u6b64\u5728\u8fde\u4e58\u4e4b\u540e\u4f1a\u7206\u70b8\u3002</p> <p>\u7a0d\u5fae\u6539\u8fdb\u4e00\u4e0b\u4e4b\u540e\uff0c\u5219\u53ef\u4ee5\u6536\u655b  </p> <p>  \u539f\u672c\u6211\u4eecdraw x' from Q(x'|x)\u65f6\uff0c\u6211\u4eec\u662f\u5728centered on x\u7684gaussian\u4e0a\u9762\u968f\u673a\u9009\u4e00\u4e2a\uff0c \u800c\u73b0\u5728\u6211\u4eec\u662f\u4ece  \u5f53\u4e2d\u62bd\u53d6\uff0c\u800c\u8fd9\u4e2a\u68af\u5ea6\u7531\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684q\u51b3\u5b9a\u3002  \u6ce8\u610f\u6211\u4eec\u5728test\u4e00\u4e2asample\u4e4b\u524d\uff0c\u53ef\u4ee5\u8fdb\u884cL\u6b21leapfrog</p> <p> \u9ad8\u7ef4\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u660e\u663e\u770b\u5230sample\u53d8\u5f97uncorrelated\u4e86  </p> <p>\u53ef\u89c1\u81f3\u6b64\u6211\u4eec\u628amcmc\u548copt(\u4f18\u5316\u7406\u8bba)\u7ed3\u5408\u4e86\u8d77\u6765\u3002 \u91c7\u6837\u7684\u8fc7\u7a0b - MCMC \u68af\u5ea6 - opt</p> <p>\u53d8\u79cd\uff1a \u53ea\u8fdb\u884c\u4e00\u6b21leapfrog\uff1a </p> <p> \u6ce8\u610fHMC\u4e0d\u80fd\u7528\u4e8e\u79bb\u6563\u53d8\u91cf\uff0c\u56e0\u4e3a\u79bb\u6563\u53d8\u91cf\u4e0d\u80fd\u6c42\u68af\u5ea6</p>"},{"location":"PGM/course/lecture10-MCMC-opt/#using-approximation-of-the-given-probability-distribution","title":"Using approximation of the given probability distribution","text":"<p>\u56de\u987e\uff1a\u5728\u4e4b\u524d\u7684\u5404\u79cd\u65b9\u6cd5\u4e2d\uff0c\u54ea\u91cc\u7528\u5230\u4e86true distribution\uff1f MC\uff1a\u7528P\u7b97importance MCMC\uff1a\u7b97acceptance\u65f6 HMC\uff1a\u7b97U(q)\u7684\u68af\u5ea6\u65f6\uff0cU\u5176\u5b9e\u5c31\u662fP\u3002\u6362\u4e00\u4e0bnotation\u7684\u8bdd\u5c31\u662f\\(\\frac{dP}{dx}\\)</p>"},{"location":"PGM/course/lecture10-MCMC-opt/#variational-mcmc","title":"variational MCMC","text":"<p> \u56de\u60f3\u4e4b\u524d\u7684variational inference\uff0c\u4e4b\u524d\u6211\u4eec\u53ea\u662f\u5f15\u5165\u03bb\uff0c\u7528q\u8fd1\u4f3cp(x|\u03b8) \u73b0\u5728\u6211\u4eec\u518d\u7ed9p(x|\u03b8)\u589e\u52a0\u4e00\u4e2avariational parameter\uff0c\u73b0\u5728p(x|\u03b8)\u4e5f\u662f\u8fd1\u4f3c\u7684\u4e86\u3002 \u6211\u4eec\u7684\u76ee\u6807\u5c31\u4e0d\u662f\u7528q\u8fd1\u4f3c\u771f\u5b9e\u7684p\uff0c\u800c\u662f\u627e\u8fd1\u4f3c\u7684q\u548c\u8fd1\u4f3c\u7684p\u80fd\u5426\u6c47\u5408\u5728\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u7ed3\u679c\u4e0a(\\(p^{est}\\))\u3002</p> <p>Variational MCMC:  \u5728variational inference\u4e2d\uff0c\u6211\u4eec\u5229\u7528KL(Q||P)\uff0c\u4f7f\u5f97Q\u903c\u8fd1P\u3002 \u800c\u5728variational MCMC\u4e2d\uff0c \u628a\u521a\u624d\u6211\u4eec\u63d0\u5230\u7684\u6536\u655b\u7684\u4f4d\u7f6e \\(p^{est}\\)\u4f5c\u4e3aproposal\uff0c\u4ece\u4e2d\u8fdb\u884c\u4e0b\u4e00\u4e2asample\u3002\u6700\u7ec8\u6211\u4eec\u6709\u5e0c\u671b\u7528\u6240\u6709sample\u903c\u8fd1P(x).</p> <p>\u8fd9\u91cc\u662f\u53e6\u4e00\u79cdMCMC\u7ed3\u5408opt\u7684\u601d\u8def\uff1a \u7528Q\u548cP\u7b97\\(p^{est}\\) - variational(opt) \u628a\\(p^{est}\\)\u5f53\u4f5cproposal\u8fdb\u884csample - MCMC</p>"},{"location":"PGM/course/lecture10-MCMC-opt/#idea","title":"\u4e24\u79cdidea\u7684\u6bd4\u8f83\uff1f","text":"<p>variational MCMC\u7684idea\u53ef\u4ee5\u7b80\u5316\u95ee\u9898\u7684structure\uff0c\u6bd4\u5982\u5047\u8bbe\u6211\u4eec\u7684\\(p^{est}\\)\u53ef\u4ee5factorize\u4e3a\u591a\u4e2aq\u7684\u4e58\u79ef HMC\u7684idea\u66f4\u4fa7\u91cd\u4e8e\u6539\u5584\u5177\u4f53\u7684sample\u7684\u6027\u80fd</p> <p>\u56e0\u6b64\u6211\u4eec\u5176\u5b9e\u53ef\u4ee5\u5148\u7528variational MCMC\u6765\u7b80\u5316\u95ee\u9898\u7684\u7ed3\u6784\uff0c\u6bd4\u5982\u628adiscrete\u8f6c\u5316\u4e3acontinuous\u6216\u8005\u65bd\u52a0mean field\uff0c\u7136\u540e\u518d\u7528HMC\u8fdb\u884c\u5177\u4f53\u7684proposal\u4e0a\u7684sample\u3002</p>"},{"location":"PGM/course/lecture10-MCMC-opt/#sequential-mc","title":"Sequential MC","text":"<p>\u5229\u7528\u4e4b\u524dresampling\u7684\u601d\u60f3\uff0c\u7136\u540e\u52a0\u4ee5\u63a8\u5e7f  \u5047\u8bbe\u5728\u4e00\u4e2aHMM\u4e2d\uff0cX\u4ee3\u8868latent variable\uff0cY\u4ee3\u8868data  \u6c42\\(p(X_t|Y_{1:t})\\)\u7684inference\uff0c\u5176\u5b9e\u53ef\u4ee5\u6539\u5199\u6210\u4ece\\(p(X_t|Y_{1:t-1})\\)\u4e2d\u8fdb\u884cweighted sample\u7684\u8fc7\u7a0b\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u73b0\u5728\u6765\u4e86\u4e00\u4e2a\u65b0\u7684\\(Y_{t+1}\\), \u60f3\u6c42\\(p(X_{t+1}|Y_{t+1}, Y_{1:t})\\)\u3002 \u6211\u4eec\u901a\u8fc7sample\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u65f6\u523b\uff0c\u9690\u53d8\u91cfX\u7684\u72b6\u6001\u3002\u5176\u4e2d\\(p(X_t|Y_{1:t})\\)\u5c31\u662f\u521a\u624dsample\u5f97\u5230\u7684\uff1a  \u7136\u540e\u6211\u4eec\u628a\u65b0\u7684data\u653e\u8fdb\u6765\uff0c\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\u8fdb\u884c\u4e0b\u4e00\u6b21\u7684sample </p> <p></p>"},{"location":"PGM/course/lecture11-NN/","title":"lecture11-NN","text":"<p>Similarities and differences between GMs and NNs  </p> <p>I: Restricted Boltzmann Machines  </p> <p>II: Sigmoid Belief Networks  \u56de\u5fc6baysian network\u4e2d\u7684head-to-head\uff0c\u5f53\u4e0b\u4e00\u5c42observed\u65f6\uff0c\u4e0a\u4e00\u5c42\u4f1a\u53d8\u5f97dependent\u3002\u56e0\u6b64\u6574\u4e2a\u7f51\u7edc\u5f88\u590d\u6742\uff0c\u56e0\u4e3a\u6bcf\u4e2alayer\u4e4b\u5185\u90fd\u662fcoupled\u7684 </p> <p>\"RBMs are infinite belief networks\" \u4e0b\u9762\u628aRBM\u548cSigmoid Belief Network\u8054\u7cfb\u8d77\u6765\uff1a \u5f53\u6211\u4eec\u5728RBM\u4e2d\u8ba1\u7b97\u68af\u5ea6\u65f6\uff0c\u8981\u8ba1\u7b97\u5bf9\u4e8ejoint\u7684expectation\uff0c\u8fd9\u5c31\u8981\u7528\u5230sample\u6765\u7b80\u5316\u8ba1\u7b97\u3002 \u5047\u8bbe\u6211\u4eec\u4f7f\u7528gibbs-sampling\uff0c\u6211\u4eec\u662f\u5728\u4ea4\u66ffcondition on \u4e24\u7ec4\u53d8\u91cf\uff0chidden\u548cobserved\u3002\u6bcf\u7b97\u4e00\u6b21joint\uff0c\u6211\u4eec\u90fd\u53ef\u4ee5\u770b\u4f5c\u8981\u8fdb\u884cinfinite\u6b21\u8fd9\u6837\u7684\u4ea4\u66ff\u8ba1\u7b97\u3002  \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u628aRBM\u4e2d\u5bf9\u4e8ejoint\u7684sample\uff0c\u770b\u4f5c\u662f\u4e00\u4e2ainfinite layer\u7684Sigmoid Belief Network\u7684forward propagation\u3002 \u4f46\u662f\u9700\u8981\u6ce8\u610f\u7684\u4e0a\u9762\u7684infinite\u6b21condition on\uff0c\u90fd\u53d1\u751f\u5728\u67d0\u4e00\u6b21\u8ba1\u7b97joint\u7684\u8fc7\u7a0b\u5f53\u4e2d\uff0c\u6ca1\u6709\u66f4\u65b0w\uff0c\u56e0\u6b64w\u662f\u4e0d\u53d8\u7684\uff0c\u56e0\u6b64  RBM\u5bf9\u5e94\u7684\u4e0d\u662f\u4efb\u610f\u7684Sigmoid Belief Network\uff0c\u5728\u8fd9\u4e2anetwork\u4e2d\u6bcf\u4e00\u5c42\u7684\u53c2\u6570\u90fd\u4e00\u6837\u3002  \u5f53\u6211\u4eec\u6765\u4e86\u65b0\u7684data\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u5728\u539f\u672c\u7684RBM\u6216\u8005\u539f\u672c\u7684network\u4e0a\u76f4\u63a5\u65b0\u589e\u4e00\u7ec4layer</p> <p>III: Deep Belief Nets  </p> <p>Deep Boltzmann Machines\u7684\u601d\u60f3\u4e5f\u5dee\u4e0d\u591a\uff0c\u4e5f\u662flayer wise\uff0c\u8fd9\u91cc\u5c31\u8df3\u8fc7\u4e86 </p> <p> \u53ef\u4ee5\u770b\u5230back-propagation\u7684error\u5f88\u5927\uff0c\u5e76\u6ca1\u6709inference\u51fahidden variable\u771f\u6b63\u7684\u5206\u5e03\u3002\u7136\u800c\u5176\u5b9e\u5b9e\u8df5\u4e2d\u5e76\u4e0d\u600e\u4e48\u5173\u6ce8\u8fd9\u4e2a\u3002</p> <p>\u6211\u4eec\u63d0\u5230\u5728RBM\u7684MCMC\u4e2d\uff0c\u5728\u7ecf\u8fc7infinite\u6b21sample\u4e4b\u540e\uff0c\u6211\u4eec\u80fd\u5f97\u5230true distribution\u3002 \u5728\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u5bf9unroll RBM\u5f97\u5230\u7684DBN\u7ecf\u8fc7infinite\u66f4\u65b0\u68af\u5ea6\u4e4b\u540e\uff0c\u5c31\u6709\u5e0c\u671b\u903c\u8fd1true distribution\u3002\u7ad9\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6709\u4e24\u70b9\u8981\u6ce8\u610f\uff1a\u6240\u6709\u7684w\u65f6\u56fa\u5b9a\u7684\uff1b\u6bcf\u5c42\u53ea\u8ba1\u7b97\u4e00\u6b21\uff0c\u7136\u540e\u8fdb\u5165\u4e0b\u4e00\u5c42\u3002</p> <p>\u4f46\u5176\u5b9e\u5728\u5b9e\u9645\u4e2d\uff0c\u6211\u4eec\u5bf9DBN\u6bcf\u5c42\u8fdb\u884c\u591a\u6b21\u8ba1\u7b97\uff0c\u7136\u540e\u6700\u7ec8\u4f7f\u7528finite\u5c42\u3002 \u6253\u4e2a\u6bd4\u65b9\uff0c\u6211\u4eec\u4e0d\u662f\u76f4\u63a5\u4fee\u4e00\u6761\u5230Rome\u7684\u8def\uff0c\u800c\u662f\u5148\u4fee\u901a\u5f80A\u7684\u8def\uff0c\u628a\u8def\u4fee\u5230perfect\uff0c\u518d\u4fee\u5230B,C,D... \u6700\u7ec8\u751a\u81f3\u53ef\u80fd\u5e76\u4e0d\u5173\u6ce8\u662f\u5426\u5230\u4e86Rome\u3002\u4f46\u8fd9\u4e2d\u95f4\u7684\u4efb\u4f55\u4e00\u6bb5\u90fd\u80fd\u4f5c\u4e3a\u4e00\u4e2a\u5355\u72ec\u7684task\uff0c\u6709\u4e00\u5b9a\u7528\u5904\u3002  \u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2astep\u4e4b\u5185\u7684\u591a\u6b21\u8ba1\u7b97optimize\u4e86\u6c42\u68af\u5ea6\u8fd9\u4e00\u8fc7\u7a0b\u3002\u518d\u6362\u53e5\u8bdd\u8bf4\uff0cwe optimized the optimization. \u6211\u4eec\u53ef\u4ee5\u4ece\u4e0a\u9762\u8fd9\u4e2a\u89d2\u5ea6\u6765justify \u53ea\u6709finite steps\u7684deep learning\u3002</p> <p> \u4ece\u8fd9\u4e2a\u89d2\u5ea6\uff0c\u6211\u4eec\u5176\u5b9e\u53ef\u4ee5\u628agraphic model\u7684inference\u8fc7\u7a0b\u5c55\u5f00\uff0c\u4ecebackprop\u7684\u89d2\u5ea6\u7406\u89e3\u6574\u4e2a\u95ee\u9898</p>"},{"location":"PGM/course/lecture12-DGM1/","title":"lecture12-DGM1","text":""},{"location":"PGM/course/lecture12-DGM1/#wake-sleep-algorithm","title":"Wake sleep algorithm","text":"<p>  \u5148\u56de\u987e\u4e00\u4e0bvariational inference\uff0c\u53ef\u4ee5\u770b\u5230VI\u4e2d\u7684Estep\u548cMstep\u90fd\u662f\u5bf9\u4e8efree energy\u8fdb\u884c\u4f18\u5316\u3002</p> <p> \u800c\u5728wake sleep algorithm\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230KL\u4e2dp\u548cq\u98a0\u5012\u4e86\uff0c\u8fd9\u5176\u5b9e\u662f\u4e3a\u4e86\u8ba1\u7b97\u4e0a\u7684\u65b9\u4fbf\u3002 wake sleep algorithm\u603b\u4f53\u4e0a\u8bad\u7ec3\u4e24\u4e2a\u5355\u72ec\u7684model\uff0cinference model\u548cgenerate model\uff0c\u5206\u522b\u5bf9\u5e94\\(p_{\\theta}(x|z)\\)\u548c\\(q_{\\phi}(z|x)\\).</p> <p> \u5728wake phase\uff0c\u6211\u4eec\u60f3\u5bf9\u4e8e\u03b8\u4f18\u5316\uff0c\u89c2\u5bdffree energy\u7684\u5f62\u5f0f\u53ef\u4ee5\u770b\u51fa\uff0c\u03b8\u53ea\u51fa\u73b0\u5728\u671f\u671b\u7684object\u4e2d\uff0c\u800c\u671f\u671b\u5bf9\u5e94\u7684\u5206\u5e03\u4e0e\u03b8\u65e0\u5173\uff0c\u53ea\u4e0e\u03c6\u6709\u5173\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5728q\u4e0a\u5bf9\u4e8ep\u8fdb\u884c\u91c7\u6837\uff0c\u4ece\u800c\u4f18\u5316\u03b8</p> <p> \u800c\u5728sleep phase\u4e2d\uff0c\u6211\u4eec\u60f3\u8981\u5bf9\u03c6\u4f18\u5316\uff0c\u4f46\u662f\u770b\u5230\u03c6\u51fa\u73b0\u5728\u4e86\u6211\u4eec\u60f3\u8981sample\u7684distribution\u4e2d\uff0c\u6ca1\u6cd5sample\uff0c\u6211\u4eec\u53ea\u80fdlearn\u8fd9\u4e2a\u03c6\u3002 \u800c\u5982\u679c\u7ecf\u8fc7\u4e00\u4e9b\u53d8\u6362\uff0c  \u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2alog P\u7684\u5f62\u5f0f\u3002\u56e0\u4e3aP\u53ef\u80fd\u662f\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0clogP\u5c31\u4f1a\u5f88\u5927\u3002\u603b\u4f53\u4e0alogP\u7684\u5c3a\u5ea6\u53d8\u5316\u5c31\u4f1a\u5f88\u5927\uff0c\u68af\u5ea6\u7684\u5c3a\u5ea6\u53d8\u5316\u4e5f\u4f1a\u5f88\u5927\uff0c\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u6ca1\u6cd5\u5b9e\u73b0\u3002</p> <p>\u56e0\u6b64\u5728wake sleep algorithm\u4e2d\uff0c\u5c31\u628aKL\u4e2d\u7684p\u548cq\u8c03\u6362\u4e86\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u6362\u4e86\u53e6\u4e00\u4e2aloss function\u3002</p> <p>\u5728\u8c03\u6362\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230  \u6211\u4eec\u540c\u6837\u53ef\u4ee5\u4f7f\u7528sample\u4e86\u3002\u5c3d\u7ba1\u6211\u4eec\u6709observed X\uff0c\u6211\u4eec\u8fd8\u662fignore\u6389X\u3002\u7136\u540e\u4ece\u4e4b\u524dgenerative model\u7684p(x, z)\u4e2d\uff0c'dream'\u51fax\u548cz\uff0c\u7136\u540e\u8fdb\u884csample\u3002 </p> <p></p>"},{"location":"PGM/course/lecture12-DGM1/#vae","title":"VAE","text":"<p>  \u6211\u4eec\u5bf9\u539f\u672c\u590d\u6742\u7684\u68af\u5ea6\u8fdb\u884creparameterization\uff0c\u76f4\u63a5\u5bf9\u4e8e\u68af\u5ea6\u5efa\u7acb\u4e00\u4e2adiscrimitive\u7684\u5206\u5e03\uff0c\u91cd\u65b0\u8bbe\u7f6e\u8fd9\u4e2a\u5206\u5e03\u7684\u5f62\u5f0f\uff0c\u7136\u540e\u53bb\u5b66\u4e60\u8fd9\u4e2a\u65b0\u7684\u5206\u5e03\u7684\u53c2\u6570\u3002\u7406\u8bba\u4e0a\u8fd9\u6837\u53ef\u4ee5\u89e3\u51b3\u539f\u672cvariance\u8fc7\u5927\u7684\u95ee\u9898\u3002  </p>"},{"location":"PGM/course/lecture12-DGM1/#gan","title":"GAN","text":"<p> \u5728GAN\u4e2d\uff0c\u6211\u4eec\u53ea\u6709generative model\uff0c\u6ca1\u6709inference model\u3002  \u6211\u4eec\u9700\u8981\u8fbe\u5230\u4e24\u4e2a\u76ee\u6807\uff1a  In practical\uff0c\u6211\u4eec\u4f7f\u7528\u4e0b\u9762\u8fd9\u4e2a\u5f0f\u5b50\u89e3\u51b3gradient vanishing\u7684\u95ee\u9898\u3002 </p> <p></p> <p></p>"},{"location":"PGM/course/lecture12-DGM1/#a-unified-view-of-deep-generative-models","title":"A unified view of deep generative models","text":"<p>\u5bf9\u4e8eGAN\uff0c\u6700\u666e\u901a\u7684\u6570\u5b66\u8868\u793a\u662f\u8fd9\u6837\u7684\uff0c\u7136\u800c\u8fd9\u4e2a\u8ddf\u4e4b\u524d\u7684variational EM\u6ca1\u6709\u4ec0\u4e48\u5171\u540c\u70b9\u3002 </p> <p> \u6ce8\u610f\u6b64\u65f6\u5728GAN\u4e24\u4e2astep\u4e2d\uff0c\u6211\u4eec\u8981\u4f18\u5316\u7684loss\u5176\u5b9e\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u591a\u4e86\u4e00\u4e2areverse\u3002  \u6ce8\u610f\u867d\u7136GAN\u662f\u7528\u6765\u751f\u6210\u56fe\u7247x\u7684\uff0c\u4f46\u771f\u6b63observed\u7684\u662flabel y\u3002\u56e0\u6b64\u6211\u4eec\u5728\u8fd9\u91cc\u628ax\u5f53\u4f5clatent\uff0c\u56e0\u6b64\u751f\u6210\u56fe\u7247\u7684\u8fc7\u7a0b\u662f\u5bf9\u4e8elatent variable x\u7684inference\uff0c\u800c\u4e0d\u662fgenerate\u3002 \u6ce8\u610fvariational EM\uff0clower bound \u53ef\u4ee5\u6709\u4e24\u79cd\u5199\u6cd5\uff0c\u4e4b\u524d\u63a8\u5bfc\u4e00\u76f4\u7528\u7684\u662f\u53f3\u4e0a\u89d2\u7684\u8fd9\u79cd\uff0c\u73b0\u5728\u5199\u6210\u53e6\u4e00\u79cd\uff0c\u4ee5\u4fbf\u89c2\u5bdf\u548cGAN\u7684\u76f8\u4f3c\u6027\u3002</p> <p> \u5728vEM\u4e2d\uff0cx\u662fobserved\u7684\uff0cz\u662flatent\u7684 \u5728GAN\u4e2d\uff0cy\u662fobserved\uff0c\u4ee3\u8868\u662f\u771f\u5b9edata\u8fd8\u662f\u751f\u6210\u7684data\uff1bx\u662flatent\u7684\uff0c\u7528\u6765\u8868\u793adata \u53ef\u4ee5\u770b\u5230\u6700\u5927\u7684\u533a\u522b\u662f\uff0cGAN\u4e2d\u6ca1\u6709\u548cprior\u7684KL\u3002\u56e0\u4e3aGAN\u672c\u8eab\u5c31\u6ca1\u6709prior\uff0cz\u662ftrivial\u7684\u3002</p> <p> \u5728GAN\u7684\u66f4\u65b0\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u68af\u5ea6\u5199\u6210KL\u51cf\u53bb\u4e00\u4e2aJSD\u7684\u5f62\u5f0f\u3002\u8fd9\u4e2aJSD\u6211\u4eec\u8fd9\u91cc\u4e0d\u7ba1\uff0c\u56e0\u4e3a\u53d8\u4e0d\u6210\u5176\u4ed6\u7684\u5f62\u5f0f\u3002 \u6211\u4eec\u8fd9\u91cc\u6765\u89c2\u5bdfKL\u7684\u6027\u8d28\u3002</p> <p>\u53ef\u4ee5\u770b\u5230\u8fd9\u4e2a\u8ddf\u6211\u4eec\u5728VEM\u4e2d\u60f3\u8981\u505a\u7684\u4e8b\u662f\u7c7b\u4f3c\u7684\uff0c\u90fd\u662f\u8ba9\u4e24\u4e2alatent variable\u7684posterior\u5c3d\u53ef\u80fd\u76f8\u8fd1\u3002GAN\u5176\u5b9e\u60f3\u8ba9generative p\u548cdiscrimitive\u7684posterior q\u5c3d\u53ef\u80fd\u76f8\u8fd1\u3002</p> <p>\u524d\u51e0\u884c\u7528\u4e8e\u8bf4\u660e\\(KL(p_{\\theta}(x|y=1)||q^r(x|y=1))\\)\u662f\u4e0d\u7528\u4f18\u5316\u7684\u3002\u7531\u5e26\u2605\u53f7\u7684\u5f0f\u5b50\u53ef\u4ee5\u770b\u5230\uff0c\\(q^r(x|y=0)\\)\u53ef\u4ee5\u770b\u4f5c\u662f\u771f\u5b9edata\u7684\u5206\u5e03\u548cgenerated data\u7684\u5206\u5e03\u7684mixture\u3002  \u6240\u4ee5learning\u7684\u8fc7\u7a0b\u5176\u5b9e\u662f\u8ba9\\(p_{g_{\\theta}}(x|y)\\)\u9760\u8fd1mixture\uff0c\u4e5f\u5c31\u76f8\u5f53\u4e8e\u9760\u8fd1\\(p_{data}(x)\\)\u3002</p> <p>\u7136\u540e\u6ce8\u610f\u5230KL\u7684\u975e\u5bf9\u79f0\u6027\uff0c\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u6027\u8d28\uff1a  \u8fd9\u4e00\u70b9\u5728PMRL\u4e2d\u5df2\u7ecf\u8bb2\u8fc7\u4e86\uff0c\u4e5f\u5c31\u662fp\u4f1a\u201c\u7f29\u5728\u201dq\u7684mode\u4e2d\u3002</p> <p> \u800cJSD\u662f\u5bf9\u79f0\u7684\uff0c\u5bf9\u6b64\u6ca1\u6709\u5f71\u54cd\u3002 \u56e0\u6b64\u7efc\u5408\u8d77\u6765\uff0cGAN\u4f1a\u4e22\u5931true distribution\u4e2d\u7684mode\u3002</p> <p>GAN\u7684image\u662fsharp\u7684\uff0c\u5c31\u8ddf\u8fd9\u4e2a\u4e0a\u9762\u8fd9\u4e2a\u6027\u8d28\u6709\u5173\u3002GAN\u5728learning\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u503e\u5411\u4e8e\u9009\u62e9\u4e00\u90e8\u5206\u66f4\u5927\u7684mode\uff0c\u800c\u4e22\u6389\u4e00\u4e9b\u5c0fmode\u3002\u56e0\u6b64\u5728\u751f\u6210style\u7684\u4efb\u52a1\u4e2d\uff0c\u52a0\u5165\u8f93\u516510\u5f20Astyle\uff0c1\u5f20Bstyle\uff0c\u90a3\u4e48\u8f93\u51fa\u7684image\u5f88\u53ef\u80fd\u4e0d\u4f1a\u6765\u81eaB\u3002</p> <p>\u6211\u4eec\u5c1d\u8bd5\u7528\u540c\u6837\u7684\u601d\u8def\u6765\u5206\u6790VAE\uff0c\u7136\u800cVAE\u4e2d\u6ca1\u6709GAN\u4e2d\u7684indicator y\u3002\u90a3\u6211\u4eec\u5c31\u5047\u8bbe\u4e00\u4e2ay\uff0c\u56e0\u4e3aVAE\u4e2d\u6240\u6709\u7684\u56fe\u7247\u90fd\u662f\u751f\u6210\u7684\uff0c\u56e0\u6b64\u6240\u6709\u56fe\u7247\u7684label y\u90fd\u662ffake\u3002 \u7136\u540e\u7ecf\u8fc7\u4e00\u7cfb\u5217\u63a8\u5bfc\u53ef\u4ee5\u5f97\u51fa\u4e0b\u9762\u7684\u5f0f\u5b50\uff0c\u53ef\u4ee5\u770b\u5230\u4e5f\u662f\u4e00\u4e2aKL\u7684\u5f62\u5f0f\u3002\u4f46\u662f\u4e0eGAN\u4e0d\u540c\u7684\u662f\uff0cVAE\u5bf9\u4e8elatent\u7684inference\u51fa\u73b0\u5728KL\u7684\u53f3\u8fb9\uff0c\u56e0\u6b64VAE\u503e\u5411\u4e8e\u628a\u6240\u6709\u7684mode\u90fd\u5305\u62ec\u8fdb\u6765\u3002\u8868\u73b0\u4e3aVAE\u751f\u6210\u7684\u56fe\u8c61\u6bd4\u8f83\u6a21\u7cca\u3002 </p> <p> </p>"},{"location":"PGM/pyro/Untitled/","title":"An Introduction to Models in Pyro","text":""},{"location":"PGM/pyro/Untitled/#primitive-stochastic-functions","title":"Primitive Stochastic Functions","text":"<p>compute the probability of the outputs given the inputs</p> <p>use PyTorch\u2019s distribution library</p>"},{"location":"PGM/pyro/Untitled/#a-simple-model","title":"A Simple Model","text":""},{"location":"PRML/chap1/chap1/","title":"Chap1","text":""},{"location":"PRML/chap1/chap1/#11-curve-fitting","title":"1.1. Curve Fitting","text":"<ol> <li>As we shall see in Chapter 3, the number of parameters is not necessarily the most appropriate measure of model complexity. \u53c2\u6570\u4e2a\u6570\u4e0d\u8db3\u4ee5\u8861\u91cf\u6a21\u578b\u7684\u590d\u6742\u5ea6</li> <li>We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood (discussed in Section 1.2.5), and that the over-fitting problem can be understood as Section 3.4 a general property of maximum likelihood \u6700\u5c0f\u4e8c\u4e58\u6cd5\u53ef\u4ee5\u770b\u4f5cMLE\uff0c\u8fc7\u62df\u5408\u662fMLE\u7684property</li> </ol>"},{"location":"PRML/chap1/chap1/#12-probability-theory","title":"1.2. Probability Theory","text":"<p> Bayes\u2019 theorem  We can view the denominator in Bayes\u2019 theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.</p> <p>If we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is provided by the probability p(B). We call this the prior probability because it is the probability available before we observe the identity of the fruit. </p> <p>Once we are told that the fruit is an orange, we can then use Bayes\u2019 theorem to compute the probability p(B|F), which we shall call the posterior probability because it is the probability obtained after we have observed F.</p> <p>We note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be independent. From the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X.</p>"},{"location":"PRML/chap1/chap1/#121-probability-densities","title":"1.2.1 Probability densities","text":"<p>The probability that x will lie in an interval (a, b) is then given by  \u6ee1\u8db3 </p> <p>\u4e0b\u9762\u8fd9\u91cc\u63d0\u5230\u4e86\u4e00\u4e2ajacobian factor\uff0c\u5927\u6982\u610f\u601d\u5c31\u662f\uff1a \u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u548c\u666e\u901a\u7684\u51fd\u6570\u662f\u4e0d\u540c\u7684\u3002 \u5728pdf\u4e2d\uff0c\u5982\u679c\u4e24\u4e2a\u968f\u673a\u53d8\u91cf\u6709\u975e\u7ebf\u6027\u7684\u51fd\u6570\u5173\u7cfb\uff0c\u8fd9\u4e24\u4e2a\u51fd\u6570\u7684pdf\u4e0d\u4f1a\u4fdd\u6301\u8fd9\u79cd\u5173\u7cfb\u3002 \u8fdb\u800c\u53ef\u4ee5\u63a8\u5e7f\u51fa\u7ed3\u8bba\uff0cpdf\u7684\u6700\u503c\u4e0evariable\u7684\u9009\u62e9\u6709\u5173 </p> <p>\u4e0b\u9762\u662f\u5bf9\u4e8e\u201cpdf\u7684\u6700\u503c\u4e0evariable\u7684\u9009\u62e9\u65e0\u5173\u201d\u7684\u8bf4\u660e\uff1a \u7531\u4e0b\u9762\u7684\u63a8\u5bfc\uff0c\u53ef\u77e5\u5bf9\u4e8e\u666e\u901a\u7684\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u6700\u503c\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u662f\u4fdd\u6301\u7684\uff1a </p> <p>\u4f46\u662f \u7531\u4e8e\u6709\u8fd9\u4e2a\u5f0f\u5b50\uff1a   \u5728(4)\u4e2d\uff0c\u56e0\u4e3a\u6709\u7b2c\u4e8c\u9879\uff0c\u6240\u4ee5\u5de6\u8fb9\u7b49\u4e8e0\u65f6 \\(p_{x}^{\\prime} (g(y))\\) \u4e0d\u4e00\u5b9a\u7b49\u4e8e0\uff0c\u56e0\u6b64\u4e24\u4e2a\u6781\u503c\u4e0d\u4e00\u5b9a\u540c\u65f6\u8fbe\u5230\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53 \\(x=g(y)\\) \u4e3a\u7ebf\u6027\u53d8\u5316\u65f6\uff0c\\(g^{\\prime\\prime}(y)=0\\) \uff0c\u56e0\u6b64\u4e0a\u9762\u5f0f\u5b50\u91cc\u7684\u7b2c\u4e8c\u9879\u5c31\u6ca1\u6709\u4e86\uff0c\u6b64\u65f6\u5173\u7cfb\u4fdd\u6301\u3002</p> <p> \u4e0a\u56fe\u4e2d\uff0c\u4ece\\(x\\) \u53d8\u6362\u5230 \\(y\\)\u7ecf\u5386\u4e86\u4e00\u4e2a\u975e\u7ebf\u6027\u53d8\u6362\u3002\u5982\u679c\u4e0d\u8003\u8651jacobian factor\uff0c\u5e94\u8be5\u662f\u7ea2\u7ebf\u8f6c\u79fb\u5230\u7eff\u7ebf\uff0c\u6700\u503c\u4fdd\u6301\u51fd\u6570\u5173\u7cfb\u3002\u4f46\u662f\u56e0\u4e3a\u6709jacobian factor\uff0c\u5b9e\u9645\u4e0a\u8f6c\u79fb\u5230\u4e86\u7d2b\u8272\u7684\u7ebf\uff0c\u6700\u503c\u5e76\u4e0d\u7b26\u5408\u51fd\u6570\u5173\u7cfb</p> <p>cumulative distribution function\uff1a </p> <p>The sum and product rules </p>"},{"location":"PRML/chap1/chap1/#122-expectations-and-covariances","title":"1.2.2 Expectations and covariances","text":"<p>Expectation of f(x)\uff1a   \u53ef\u4ee5\u7528\u6709\u9650\u7684\\(N\\)\u6b21sample\u8fd1\u4f3c\u6c42expectation\uff1a  We shall make extensive use of this result when we discuss sampling methods in Chapter 11. The approximation in (1.35) becomes exact in the limit \\(N\\to\\infty\\).</p> <p>\u7528\u4e0b\u6807\u8868\u793awhich variable is being averaged over\uff0c \u5728 \\(\\mathbb{E}_{x}f(x,y)\\) \u4e2d\uff0c\u662f\u5bf9 \\(x\\) \u53d6\u5e73\u5747\uff0c \\(\\mathbb{E}_{x}f(x,y)\\) will be a function of \\(y\\) .</p> <p>Conditional Expectation </p> <p>Variance:   If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation \\(cov[\\mathrm {x}]\\equiv cov[\\mathrm {x}, \\mathrm {x}]\\).  ## 1.2.3 Bayesian probabilities  \u5bf9\u4e8e\u4e0d\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\uff0c\u5982\u51b0\u5ddd\u4f1a\u4e0d\u4f1a\u878d\u5316\uff0c\u6211\u4eec\u4e0d\u80fd\u901a\u8fc7\u9891\u7387\u6765\u63cf\u8ff0uncertainty\uff0c\u8fd9\u65f6\u5019\u8981\u901a\u8fc7probability\u6765\u63cf\u8ff0\u3002\u6b64\u65f6\u6bcf\u5f53\u6211\u4eec\u638c\u63e1\u4e86\u4e00\u4e9b\u65b0\u7684\u8bc1\u636e\uff0c\u90fd\u4f1a\u5bf9\u539f\u6709\u7684\u4f30\u8ba1\u52a0\u4ee5\u4fee\u6b63\uff0c\u8fd9\u5c31\u662fbayesian\u7684\u601d\u8def\u3002</p> <p>prior, posterior, likelihood\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u4e2a\u5e94\u8be5\u770b\u4e86\u597d\u591a\u904d\u4e86\uff1a     \u5206\u6bcd\u662fnormalization term\uff0c\u56e0\u4e3a\u5982\u679c\u5de6\u53f3\u4e24\u8fb9\u540c\u65f6\u5bf9\\(\\mathrm{w}\\)\u79ef\u5206\uff1a  </p> <p>In a frequentist setting, \\(\\mathrm{w}\\) is considered to be a fixed parameter, whose value is determined by some form of \u2018estimator\u2019, and error bars on this estimate are obtained by considering the distribution of possible data sets \\(\\mathcal{D}\\).   By contrast, from the Bayesian viewpoint there is only a single data set \\(\\mathcal{D}\\) (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over \\(\\mathrm{w}\\).</p> <p>## 1.2.4 The Gaussian distribution    The square root of the variance \u03c3, is called the standard deviation  \u03b2 = 1/\u03c32, is called the precision </p> <p></p> <p>One common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function.   This might seem like a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters.</p> <p>MLE\u5f97\u5230\u7684\\(\\mu_{ML}\\)\u5c31\u662fsample mean\uff0c\\(\\sigma_{ML}\\)\u5c31\u662fsample variance   </p> <p>maximum likelihood approach systematically underestimates the variance of the distribution.  </p> <p>## 1.2.5 Curve fitting re-visited</p> <p>we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve    \u5199\u51falikelihood\uff1a    \u6c42log\uff0c\u540e\u4e24\u9879\u4e0e\\(\\mathrm{w}\\)\u65e0\u5173    the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution</p> <p>\u540c\u65f6\uff0c\u5bf9\\(\\beta\\)\u6c42\u5bfc\uff0c\u53ef\u4ee5\u5f97\u51fa    \u6b64\u65f6\u5c31\u53ef\u4ee5\u7528\u8fd9\u4e2a\u5206\u5e03\u8fdb\u884c\u9884\u6d4b\u4e86  </p> <p>introduce a prior distribution over the polynomial coefficients \\(\\mathrm{w}\\)   \u6b64\u65f6likelihood\uff1a    \u53ef\u4ee5\u770b\u5230\u76f8\u5f53\u4e8e\u52a0\u5165\u4e86\u6b63\u5219\u9879\uff1a    This technique is called maximum posterior, or simply MAP.</p> <p>## 1.2.6 Bayesian curve fitting  \u867d\u7136\u524d\u9762\u6c42\u51fa\u4e86\\(\\mathrm{w}\\)\u7684\u540e\u9a8c\uff0c\u4f46\u8fd9\u4e0d\u80fd\u7b97\u662f\u5b8c\u6210\u7684bayesian treatment\u3002we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of \\(\\mathrm{w}\\)\u3002</p> <p>We therefore wish to evaluate the predictive distribution \\(p(t|x, \\mathbf{x}, \\mathbf{t})\\)(\u8fd9\u91cc\u8bbe\\(\\alpha\\)\u548c\\(\\beta\\)\u5df2\u77e5)  \u5176\u4e2d  Here p(w|x, t) is the posterior distribution\uff0c\u901a\u8fc7\\(\\frac{prior\\times likelihood}{normalization\\space term}\\)\u5f97\u5230\uff0csection3.3\u4e2d\u53ef\u77e5\uff0c\u5bf9\u4e8ecurve fitting\uff0cposterior\u4e5f\u662f\u4e00\u4e2agaussian</p> <p>\u6b64\u5916\u66f4\u8fdb\u4e00\u6b65\u3002\u9884\u6d4b\u7ed3\u679c\u4e5f\u662f\u4e00\u4e2agaussian\uff1a  The first term in (1.71) represents the uncertainty in the predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through \\({\u03b2^{-1}_{ML}}\\). However, the second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment.</p>"},{"location":"PRML/chap1/chap1/#13-model-selection","title":"1.3. Model Selection","text":"<p>Akaike information criterion, or AIC chooses the model for which the quantity  is largest. Here p(D|wML) is the best-fit log likelihood, and M is the number of adjustable parameters in the model.</p> <p>section 4.4.1\u4e2d\u8981\u8bb2\u5230 Bayesian information criterion, or BIC Such criteria do not take account of the uncertainty in the model parameters,  however, and in practice they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully Bayesian approach where we shall see how complexity penalties arise in a natural and principled way.</p>"},{"location":"PRML/chap1/chap1/#15-decision-theory","title":"1.5. Decision Theory","text":"<p>\u4f5c\u7528\uff1aHere we turn to a discussion of decision theory that, when combined with probability theory, allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition.</p> <p>Determination of p(x, t) from a set of training data is an example of inference and is typically a very difficult problem whose solution forms the subject of much of this book</p> <p></p>"},{"location":"PRML/chap1/chap1/#153-the-reject-option","title":"1.5.3 The reject option","text":"<p>In some applications, it will be appropriate to avoid making decisions on the difficult cases in anticipation of a lower error rate on those examples for which a classification decision is made. This is known as the reject option.</p>"},{"location":"PRML/chap1/chap1/#154-inference-and-decision","title":"1.5.4 Inference and decision","text":"<p>We have broken the classification problem down into two separate stages</p> <p>inference stage in which we use training data to learn a model for p(Ck|x) decision stage in which we use these posterior probabilities to make optimal class assignments</p> <p>\u5982\u679c\u76f4\u63a5learn a function\uff0c\u628a\u8f93\u5165\u6620\u5c04\u5230\u51b3\u7b56\u4e2d\uff0c\u5c31\u662fdiscriminant function</p> <p>\u4e0b\u9762\u8fd9\u91cc\u8bb2\u4e86\u4e09\u79cd\u6a21\u578b\uff1agenerative, discrimitive\u548c\u76f4\u63a5\u6620\u5c04\u52300\u548c1 </p>"},{"location":"PRML/chap1/chap1/#155-loss-functions-for-regression","title":"1.5.5 Loss functions for regression","text":"<p>\u4e4b\u524d\u8bf4\u7684\u90fd\u662f\u5206\u7c7b\u95ee\u9898\uff0c\u5bf9\u4e8e\u56de\u5f52\u95ee\u9898\uff0c\u6211\u4eec\u60f3\u8981minimize\u7684loss\u7684\u671f\u671b\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a  \u6bd4\u5982\u8bf4square loss\uff1a  \u4e0b\u9762\u8fd9\u4e00\u6bb5\u662f\u5bf9\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u6c42\u5bfc\uff0c\u8981\u7528\u5230variational calculus\uff08\u5bf9\u4e8e\u51fd\u6570\u6c42\u5bfc\uff09\uff0c\u9700\u8981\u770b\u4e00\u4e0bappendix D </p> <p>which is the conditional average of t conditioned on x and is known as the regression function</p> <p>\u53ef\u4ee5\u770b\u5230regression function \\(\\mathbb{E}_{t}[t|x]\\)\uff0c\u53ef\u4ee5minimize square loss\u7684\u671f\u671b\uff0c\u662f\u7531\\(p(t|x)\\)\u7684\u5747\u503c\u5f97\u5230\u7684</p> <p>\u56e0\u4e3a\\(\\mathbb{E}_{t}[t|x]\\)\u662f\u6700\u4f18\u89e3\uff0c \\({y(x) \u2212 \\mathbb{E}[t|x]}\\)\u7684\u671f\u671b\u662f0\uff0c\u56e0\u6b64\u4ea4\u53c9\u9879\u6d88\u5931  \u800c\u7531(1.90)\u7684\u524d\u534a\u90e8\u5206\u4e5f\u53ef\u4ee5\u5f97\u51fa\u4e4b\u524d\u7684\u7ed3\u8bba\uff0c\u5373\u6700\u4f18\u89e3\u662f\u7531conditional mean \\(\\mathbb{E}_{t}[t|x]\\)\u5f97\u51fa\u7684</p> <p>\u7b2c\u4e8c\u9879\u5219\u662ftarget\u7684variance\uff0c\u56e0\u6b64\u9884\u6d4b\u51fa\u7684target\u7684\u4e0d\u540c\u53ef\u4ee5\u770b\u4f5c\u566a\u58f0\u3002\u800c\u56e0\u4e3a\u8fd9\u9879\u4e0ey(x)\u65e0\u5173\uff0c\u56e0\u6b64\u8fd9\u4e2a\u65b9\u5dee\u662f\u53bb\u4e0d\u6389\u7684</p> <p>\u7c7b\u6bd4\u5206\u7c7b\u95ee\u9898\uff0c\u56de\u5f52\u95ee\u9898\u4e5f\u53ef\u4ee5\u5206\u4e3a\u4e09\u7c7b\uff1a  </p>"},{"location":"PRML/chap1/chap1/#16-information-theory","title":"1.6. Information Theory","text":"<p>\u5f15\u51fa\\(h(x)\\): \u6211\u4eec\u7528\\(h(x)\\)\u63cf\u8ff0degree of surprise\uff0c\u663e\u7136\\(h(x)\\)\u4e0e\\(p(x)\\)\u76f8\u5173 \u5f53x\u548cy\u72ec\u7acb\u65f6\uff0c\u6211\u4eec\u89c2\u5bdfx\u7684surprise+\u89c2\u5bdfy\u7684surprise\u5e94\u8be5\u7b49\u4e8e\u540c\u65f6\u89c2\u5bdfx\u548cy\u7684surprise\uff0c\u5373\\(h(x,y)=h(x)+h(y)\\) \u800c\u53c8\u6709\\(p(x,y)=p(x)\\cdotp(y)\\) \u56e0\u6b64\u53ef\u4ee5\u5b9a\u4e49information </p> <p>\u4f20\u8f93\u4e00\u4e2a\u53d8\u91cfx\u7684\\(h(x)\\)\u7684\u671f\u671b\uff0c\u5c31\u662fx\u7684entropy </p> <p>\u4e4b\u540e\u7684\u8ba8\u8bba\u4e2d\uff0centropy\u7684\u5e95\u6570\u4e3ae</p> <p>multiplicity\uff1a \u628aN\u4e2a\u76f8\u540c\u7684\u7269\u4f53\u5206\u5230n\u4e2a\u7bb1\u5b50\u4e2d\u7684\u5206\u6cd5\uff1a \u9996\u5148\u9009\u7b2c\u4e00\u4e2a\u7269\u4f53\u6709N\u79cd\u9009\u6cd5\uff0c\u7b2c\u4e8c\u4e2a\u7269\u4f53\u6709N-1\u79cd\u9009\u6cd5\u3002\u4e00\u5171\u6709\\(N!\\)\u79cd\u9009\u6cd5 \u800cn\u4e2a\u7bb1\u5b50\u5185\u90e8\u672c\u8eab\u662f\u65e0\u5e8f\u7684\uff0c\u56e0\u6b64\u6700\u7ec8\u7ed3\u679c\u4e3a\uff1a  which is called the multiplicity</p> <p>\u800centropy\u5219\u662f logarithm of the multiplicity scaled by an appropriate constant </p> <p>\u79bb\u6563\u503c\u7684\u71b5\uff1a  \u5bf9\u4e8e\u8fde\u7eed\u503c\uff0cdifferential entropy\uff1a </p> <p>\u5bf9\u4e8e\u79bb\u6563\u53d8\u91cf\uff0c\u7528\u62c9\u683c\u6717\u65e5maximize extropy\u5f97\u5230uniform distribution</p> <p>\u5bf9\u4e8e\u8fde\u7eed\u53d8\u91cf\uff0c\u7528\u62c9\u683c\u6717\u65e5maximize extropy\u5f97\u5230Gaussian distribution</p> <p>conditional entropy\uff1a \u5982\u679c\u5bf9\u4e8e\u53d8\u91cfx\u548cy\uff0c\u6211\u4eec\u5148\u89c2\u5bdf\u5230\u4e86y\uff0c\u90a3\u4e48\u89c2\u5bdfx\u5f97\u5230\u7684\u4fe1\u606f\u91cf\u4e3a\u2212ln p(y|x)\uff0cx\u6b64\u65f6\u7684\u6761\u4ef6\u71b5\u4e3a\uff1a </p> <p>\u6761\u4ef6\u71b5\u6ee1\u8db3  where \\(H[x, y]\\) is the differential entropy of \\(p(x, y)\\) and \\(H[x]\\) is the differential entropy of the marginal distribution \\(p(x)\\)</p>"},{"location":"PRML/chap1/chap1/#161-relative-entropy-and-mutual-information","title":"1.6.1 Relative entropy and mutual information","text":"<p>Consider some unknown distribution \\(p(x)\\), and suppose that we have modelled this using an approximating distribution \\(q(x)\\).  If we use \\(q(x)\\) to construct a coding scheme for the purpose of transmitting values of \\(x\\) to a receiver, then the average additional amount of information (in nats) required to specify the value of \\(x\\) (assuming we choose an efficient coding scheme) as a result of using \\(q(x\\)) instead of the true distribution \\(p(x)\\) is given by  This is known as the relative entropy or Kullback-Leibler divergence,or KL divergence KL(p||q) &gt;=0 with equality if, and only if, p(x)= q(x).</p> <p>\u540e\u9762\u8981\u7528\u5230jensen\u4e0d\u7b49\u5f0f\uff0c\u56e0\u6b64\u5148\u8bf4\u660e\u51f8\u51fd\u6570\u7684\u5b9a\u4e49\uff1a  This is equivalent to the requirement that the second derivative of the function be everywhere positive</p> <p></p> <p>we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x).</p> <p>KL divergence\u5b9e\u9645\u4e0a\u6c42\u4e0d\u51fa\u6765\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u77e5\u9053\u771f\u6b63\u7684p(x). \u4f46\u662f\u6211\u4eec\u77e5\u9053\u7531p(x)\u4ea7\u751f\u7684N\u4e2ax \u56e0\u6b64p(x)\u53ef\u4ee5\u7528\\(p(\\mathrm{x})\\)\u8fd1\u4f3c</p> <p></p> <p> Thus we see that minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function</p> <p>Mutual information</p> <p>\u5bf9\u4e8ejoint distribution\u4e2d\u7684\u4e00\u7ec4\u53d8\u91cfx\u548cy\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u72ec\u7acb\uff0c\u5c31\u4e0d\u80fd\u8868\u793a\u4e3ap(x, y)= p(x)p(y) \u4f46\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u7528KL divergence\u8861\u91cf\u5b83\u4eec\u4e4b\u95f4\u201c\u6709\u591a\u4e48\u4e0d\u72ec\u7acb\u201d</p> <p> I(x, y) &gt;= 0 with equality if, and only if, x and y are independent</p> <p>mutual information is related to the conditional entropy through  \u63a8\u5bfc\uff1a </p> <p>Thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y (or vice versa).  \u6211\u4eec\u53ef\u4ee5\u628amutual information\u770b\u4f5c\u662f\u89c2\u5bdf\u5230y\u4e4b\u540e\uff0c\u5bf9\u4e8ex\u7684uncertainty\u51cf\u5c11\u4e86\u591a\u5c11\uff08\u901a\u8fc7entropy\u548cconditional entropy\u6765\u8861\u91cf\uff09 \u6216\u8005\u6211\u4eec\u53ef\u4ee5\u628ap(x)\u770b\u4f5c\u5148\u9a8c\uff0cp(x|y)\u770b\u4f5c\u662f\u540e\u9a8c\u3002mutual information\u8868\u793a\u89c2\u6d4b\u5230y\u4e4b\u540ethe reduction in uncertainty about x</p>"},{"location":"PRML/chap10/chap10/","title":"10. Approximate Inference","text":"<p>motivation: \u6211\u4eec\u60f3\u8981\u6c42posterior distribution p(Z|X) of the latent variables Z given the observed (visible) data variables X \u4f46\u662f\uff0c \u5bf9\u4e8e\u8fde\u7eed\u53d8\u91cf\uff0cthe required integrations may not have closed-form analytical solutions, while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration \u5bf9\u4e8e\u79bb\u6563\u53d8\u91cf\uff0cthe marginalizations involve summing over all possible configurations of the hidden variables. There may be exponentially many hidden states so that exact calculation is prohibitively expensive</p> <p>\u56e0\u6b64\u6211\u4eec\u9700\u8981approximation schemes\uff0c\u800c\u8fd9\u5206\u4e3a\u4e24\u7c7b\uff1astochastic or deterministic approximations Stochastic techniques such as Markov chain Monte Carlo, \u63a8\u5e7f\u4e86Bayesian methods\u7684\u4f7f\u7528\u3002Bayesian methods\u5728\u6709\u65e0\u9650\u7684\u8fd0\u7b97\u8d44\u6e90\u65f6\u80fd\u7ed9\u51fa\u7cbe\u786e\u89e3\u3002\u800c\u8fd9\u4e9b\u8fd1\u4f3c\u65b9\u6cd5\u53ef\u4ee5\u5728\u6709\u9650\u7684\u65f6\u95f4\u5185\u7ed9\u51fa\u89e3\u3002In practice, sampling methods can be computationally demanding, often limiting their use to small-scale problems. Also, it can be difficult to know whether a sampling scheme is generating independent samples from the required distribution. \u8fd9\u7ae0\u8bb2\u7684\u662fdeterministic approximation schemes, some of which scale well to large applications. \u4f46\u5b83\u4eec\u4e0d\u80fd\u7ed9\u51fa\u7cbe\u786e\u89e3</p>"},{"location":"PRML/chap10/chap10/#101-variational-inference","title":"10.1. Variational Inference","text":"<p>a functional is a mapping that takes a function as the input and that returns the value of the functional as the output. \u4e00\u4e2a\u4f8b\u5b50\u5c31\u662fentropy \\(H[p]\\)\uff0c\u63a5\u53d7p(x)\u4f5c\u4e3ainput\uff0c\u8fd4\u56de\u4e00\u4e2a\u91cf\u4f5c\u4e3a\u8f93\u51fa  a functional derivative expresses how the value of the functional changes in response to infinitesimal changes to the input function</p> <p>variational methods\u672c\u8eab\u6ca1\u6709\u4ec0\u4e48\u8fd1\u4f3c\u7684\u5143\u7d20\u3002\u4f46\u662f\u901a\u5e38\u4f1a\u5728\u6c42\u89e3\u7684\u8fc7\u7a0b\u4e2d\u9650\u5236function\u7684\u79cd\u7c7b\u548c\u8303\u56f4, \u6bd4\u5982\u53ea\u8003\u8651quadratic functions\u6216\u8005\u53ea\u8003\u8651\u67d0\u4e9bfixed basis functions\u7684\u7ebf\u6027\u7ec4\u5408. \u5728probabilistic inference\u4e2d\uff0c\u53ef\u80fd\u4f1a\u6709factorization assumptions</p> <p>variational optimization\u7528\u4e8einference\u7684\u4f8b\u5b50\uff1a \u6211\u4eec\u6709\u4e00\u4e2afully Bayesian model\uff0c\u6240\u6709latent variable\u8bb0\u4f5cZ\uff0c\u6240\u6709observed variables\u8bb0\u4f5cX\uff0cX\u7531N\u4e2ai.i.d\u6570\u636e\u7ec4\u6210\u3002 \u6211\u4eec\u8bbe\u4e86joint distribution p(X, Z), \u76ee\u6807\u662f\u5f97\u5230\u8fd1\u4f3c\u7684posterior p(Z|X)\u548c\u8fd1\u4f3c\u7684evidence p(X)</p> <p>\u4eff\u7167\u4e4b\u524dEM\u7684\u505a\u6cd5\uff0c\u6211\u4eec\u4ecelog marginal\u4e2d\u62c6\u51fa\u6765KL\u6563\u5ea6\uff0c\u552f\u4e00\u4e0e\u4e4b\u524d\u4e0d\u540c\u7684\u662f\uff0c\u6211\u4eec\u8fd9\u91cc\u6ca1\u6709\u51fa\u73b0theta\uff0c\u56e0\u4e3a\u5728EM\u4e2d\uff0ctheta\u90fd\u662fobserved\u7684\uff08\u53ef\u4ee5\u53bb\u770bGMM\u5bf9\u5e94\u7684graph\uff09\uff0c\u800c\u5728\u8fd9\u91cc\uff0cthe parameters are now stochastic variables and are absorbed into Z\u3002 </p> <p>\u548c\u4e4b\u524d\u7684Estep\u4e00\u6837\uff0c\u6211\u4eec\u5173\u4e8eq\u5bf9L\u8fdb\u884cmaximize\uff08\u987a\u4fbf\u4e00\u63d0\u56e0\u4e3a\u6ca1\u6709\u03b8\uff0c\u6240\u4ee5\u6ca1\u6709M-step\u4e86\uff09\u3002\u5982\u679c\u6211\u4eec\u5bf9\u4e8eq(Z)\u6ca1\u6709\u9650\u5236\uff0c\u9009\u5565\u90fd\u884c\u7684\u8bdd\uff0clower bound\u8fbe\u5230\u6700\u5927\u65f6\uff0cKL\u6563\u5ea6\u4e3a0\u8fbe\u5230, q(Z)\u5c31\u7b49\u4e8e\u771f\u6b63\u7684posteriorp(Z|X). \u8fd9\u6837\u7684\u7ed3\u679c\u867d\u7136\u662f\u5bf9\u7684\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u5bfc\u81f4p(Z|X)\u592a\u590d\u6742\uff0c\u6ca1\u6cd5\u7b97\u3002</p> <p>We therefore consider instead a restricted family of distributions q(Z) and then seek the member of this family for which the KL divergence is minimized. \u6211\u4eec\u60f3\u8ba9\u5bf9p(Z)\u52a0\u4e00\u4e9b\u9650\u5236\uff0c\u4f7f\u5f97\u5b83\u4eec\u90fd\u201c\u597d\u7b97\u201d\uff0c\u540c\u65f6\u53c8\u80fd\u5bf9\u771f\u6b63\u7684posterior\u8db3\u591f\u8fd1\u4f3c (\u4e0b\u9762\u8fd9\u6bb5\uff0c\u56e0\u4e3a\u6a21\u578b\u7b80\u5316\uff0c\u6240\u4ee5\u6cdb\u5316\u80fd\u529b\u53d8\u5f3a\uff0c\u56e0\u6b64\u5c31\u89e3\u51b3\u4e86\u8fc7\u62df\u5408\u7684\u95ee\u9898) In particular, there is no \u2018over-fitting\u2019 associated with highly flexible distributions. Using more flexible approximations simply allows us to approach the true posterior distribution more closely.</p> <p>\u5176\u4e2d\u4e00\u79cd\u7ea6\u675fq\u7684\u65b9\u6cd5\u662f\u4f7f\u7528parametric distribution\uff0c\u5982\u679cq(Z|\u03c9) governed by a set of parameters \u03c9\uff0c\u90a3\u4e48L\u5c31\u6210\u4e86\u03c9\u7684function\u3002\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u5e38\u7528\u7684\u4f18\u5316\u65b9\u6cd5\u7528\u5230L\u4e0a\u9762\u4e86</p>"},{"location":"PRML/chap10/chap10/#1011-factorized-distributions","title":"10.1.1 Factorized distributions","text":"<p>\u73b0\u5728\u4ecb\u7ecd\u53e6\u4e00\u79cd\u9650\u5236q(Z)\u7684\u65b9\u6cd5\uff1a \u8fd9\u91cc\u7684Z\u5176\u5b9e\u662f\\(\\mathrm{Z}\\), \u662f\u6240\u6709\u7684latent variable\uff0c\u8fd9\u91cc\u5c31\u7b80\u5199\u4e86 Suppose we partition the elements of Z into disjoint groups that we denote by \\(Z_i\\) where i =1,...,M. We then assume that the q distribution factorizes with respect to these groups, so that </p> <p>\u6ce8\u610f\u6211\u4eec\u5bf9\u4e8e\\(q_i(Z)\\)\u7684\u5177\u4f53\u5f62\u5f0f\u6ca1\u6709\u8fdb\u4e00\u6b65\u7684\u5047\u8bbe\uff0c\u5c31\u53ea\u662f\u5047\u8bbeq(Z)\u80fd\u62c6\u5f00\u800c\u5df2 This factorized form of variational inference corresponds to an approximation framework developed in physics called mean field theory</p> <p>\u5728\u6240\u6709\u201c\u80fd\u62c6\u201d\u7684q(Z)\u4e2d\uff0c\u6211\u4eec\u8981\u627e\u80fd\u4f7fL(\u4e5f\u5c31\u662flower bound)\u6700\u5927\u7684\u3002\u6211\u4eec\u53ef\u4ee5\u5206\u522b\u5bf9\u6bcf\u4e00\u4e2a\\(q_i(Z)\\)\u90fd\u8f6e\u6d41\u4f18\u5316\uff0c\u4e5f\u5c31\u662f\u4f18\u5316\u67d0\u4e00\u4e2a\uff0c\u56fa\u5b9a\u5176\u4ed6\u7684\uff0c\u7136\u540e\u8f6e\u6d41\u8fdb\u884c\u3002</p> <p>\u628a\\(q_i(Z)\\)\u8fde\u4e58\u4ee3\u5165\u5230L\u7684\u5b9a\u4e49\u5f0f\u4e2d\uff0c\u53ef\u4ee5\u5f97\u5230  \u8fd9\u91cc\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7684distribution\\(\\hat{p}(X,Z_j)\\)\uff1a  \u7136\u540e\u53c8\u5b9a\u4e49\u4e86\u4e00\u4e2a\\(E_{i\\neq j}[...]\\)\uff0c\u4ee3\u8868\u8fd9\u4e2a\u671f\u671b\u5728\u7b97\u7684\u8fc7\u7a0b\u4e2d\u628aj\u8fd9\u9879\u53bb\u6389\u4e86\u3002\u6ce8\u610f\u4e0b\u9762\u8fd9\u4e2a\u5f0f\u5b50\u7b97lnp(X,Z)\u7684\u201c\u671f\u671b\u201d\uff0c\u662f\u5bf9\u4e8eZ\u800c\u8a00\u7684\uff0c\u56e0\u6b64\u4e58\u7684\u662fZ\u7684\u5206\u5e03q </p> <p>\u800c\u5177\u4f53\u56fa\u5b9a\u4f4f\u5176\u4ed6\u7684\u3001\u53ea\u4f18\u5316j\u7684\u8fc7\u7a0b\u4e5f\u6709\u7b80\u4fbf\u8ba1\u7b97\uff0c\u6211\u4eec\u60f3maximize L\uff0c\u53ef\u4ee5\u770b\u5230L\u7684\u8fd9\u4e2a\u5f62\u5f0f\uff1a  \u5c31\u662f\u4e00\u4e2anegative Kullback-Leibler divergence between \\(q_j(Z_j)\\) and \\(\\hat{p}(X,Z_j)\\)\uff0c\u56e0\u6b64\\(q_j(Z_j)=\\hat{p}(X,Z_j)\\)\u7684\u65f6\u5019KL\u6563\u5ea6\u6700\u5c0f\u4e3a0\uff0cL\u6700\u5927</p> <p>\u6b64\u65f6\u6211\u4eec\u5f97\u5230\u4e00\u4e2aoptimal solution\\(q^*_j(Z_j)\\)\u7684\u901a\u7528\u7ed3\u8bba\uff1a  Note: \u89c2\u5bdf\u4e00\u4e0b\u8fd9\u4e2a\u5f0f\u5b50\uff0cIt says that the log of the optimal solution for factor \\(q_j\\) is obtained simply by considering the log of the joint distribution over all hidden and visible variables and then taking the expectation with respect to all of the other factors \\(\\{q_i\\}\\space for\\space i\\neq j\\). \u800c\u4e0a\u9762\u7684\u8fd9\u4e2a\u5e38\u6570\u9879\u662fnormalization term\uff0c\u901a\u5e38\u4e0d\u4f1a\u786c\u7b97\u8fd9\u4e2aconst\uff0c\u800c\u662f\u5148\u7b97\u524d\u9762\u8fd9\u9879\uff0c\u7136\u540e\u89c2\u5bdf\u51faconst </p> <p>\u57fa\u672c\u6d41\u7a0b\u662f\u5148\u9002\u5f53\u5730\u521d\u59cb\u5316\u5404\u4e2a\\(q_i(Z_i)\\)\uff0c\u7136\u540e\u5f00\u59cb\u5faa\u73af\uff0c\u4f9d\u6b21\u66f4\u65b0\\(q_i(Z_i)\\) Convergence is guaranteed because bound is convex with respect to each of the factors \\(q_i(Z_i)\\)</p>"},{"location":"PRML/chap10/chap10/#1012-properties-of-factorized-approximations","title":"10.1.2 Properties of factorized approximations","text":"<p>(Let us consider for a moment the problem of approximating a general distribution by a factorized distribution)</p>"},{"location":"PRML/chap10/chap10/#factorized-gaussian","title":"factorized Gaussian","text":"<p>\u5047\u8bbe\u6709  two correlated variables z =(z1,z2) in which the mean and precision have elements  \u5e76\u4e14\u56e0\u4e3a\u662f\u5bf9\u79f0\u77e9\u9635\uff0c\\(\\Lambda_{12}=\\Lambda_{21}\\)</p> <p>\u73b0\u5728\u6211\u4eec\u60f3\u8981\u7528q(z)= q1(z1)q2(z2)\u8fd1\u4f3cp(z) \u60f3\u8981\u6c42\\(q^*_j(Z_j)\\)\uff0c\u76f4\u63a5\u4ee3\u5165\u4e4b\u524d\u7684\u901a\u7528\u8868\u8fbe\u5f0f\uff0c\u6ce8\u610fE\u4e2d\u53ea\u7559\u4e0b\u4e0e\\(z_1\\)\u6709\u5173\u7684\u90e8\u5206\uff0c\u5176\u4ed6\u7684\u6254\u5230const\u91cc </p> <p>\u7136\u540e\u6211\u4eec\u53d1\u73b0\\(lnq^*_j(Z_j)\\)\u662fquadratic\u7684\uff0c\u56e0\u6b64\\(q^*_j(Z_j)\\)\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u4e2agaussian\u3002 \u6ce8\u610f\uff1a\u6211\u4eec\u6ca1\u6709\u5047\u8bbeq(Z)\u662fGaussian\uff0cbut rather we derived this result by variational optimization of the KL divergence over all possible distributions q(zi).</p> <p>\u7528\u914d\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f97\u5230\\(q_1^*\\)\u548c\\(q_2^*\\)\u5bf9\u5e94\u7684Gaussian\u7684\u53c2\u6570\uff1a </p> <p>Note that these solutions are coupled, so that q(z1) depends on expectations computed with respect to q(z2) and vice versa. </p> <p>\u901a\u5e38\u6211\u4eec\u4f1a\u5faa\u73af\u66f4\u65b0\u8fd9\u4e24\u4e2a\uff0c\u76f4\u5230\u4ed6\u4eec\u6536\u655b\u3002 \u4f46\u662f\u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u95ee\u9898\u5f88\u7b80\u5355\uff0c\u53ef\u4ee5\u76f4\u63a5\u89c2\u5bdf\u51faclose form solution\uff1a</p> <p>In particular, because \\(E[z1]= m1\\) and \\(E[z2]= m2\\), we see that the two equations are satisfied if we take \\(E[z1]= \u00b51\\) and \\(E[z2]= \u00b52\\), and it is easily shown that this is the only solution provided the distribution is nonsingular.</p> <p> \u53ef\u4ee5\u770b\u5230\u6211\u4eec\u8fd1\u4f3c\u51fa\u4e86\u6b63\u786e\u7684\u03bc\uff0c\u4f46\u662fvariance\u8fc7\u5c0f\u4e86\u3002 \u901a\u5e38\u6765\u8bf4factorized variational approximation\u4f1a\u7ed9\u51fa\u4e00\u4e2a\u8fc7\u4e8ecompact\u7684distribution</p> <p>\u76f8\u53cd\uff0c\u5982\u679c\u6211\u4eecminimize reverse Kullback-Leibler divergence KL(p||q)\u5462\uff1f \u8fd9\u79cd\u65b9\u6cd5\u7528\u4e8e\u53e6\u4e00\u79cd\u8fd1\u4f3c\u7684framework\uff1aexpectation propagation</p> <p>\u6b64\u65f6KL\u6563\u5ea6\u53ef\u4ee5\u5199\u6210\uff1a  \u6b64\u65f6\u6211\u4eec\u53ef\u4ee5\u628a\\(i\\neq j\\)\u7684\u90e8\u5206\u5f53\u4f5c\u5e38\u6570\uff0c\u7136\u540e\u56e0\u4e3a\u6709q_j\u79ef\u5206\u4e3a1\uff0c\u56e0\u6b64\u6709\u7ea6\u675f\uff0c\u8981\u7528lagrange  </p> <p>\u6211\u4eec\u53d1\u73b0\uff0cqj(Zj)\u7684\u6700\u4f18\u89e3\u53ea\u4e0ep(Z)\u6709\u5173\uff0c\u800c\u4e14\u662fclose form\uff0c\u4e0d\u9700\u8981iteration</p> <p> We see that once again the mean of the approximation is correct, but that it places significant probability mass in regions of variable space that have very low probability.</p> <p>\u5206\u6790\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e0d\u540c\u7684\u539f\u56e0\uff1a</p> <p>\u5bf9\u4e8eKL(q||p)\uff1a here is a large positive contribution to the Kullback-Leibler divergence  from regions of Z space in which p(Z) is near zero unless q(Z) is also close to zero. \u6362\u53e5\u8bdd\u8bf4\uff1a\u9996\u5148\u6ce8\u610f\u5230ln\u51fd\u6570\u8d8a\u9760\u8fd10\uff0c\u8d8a\u9661\u3002\u5f53\u5206\u5b50p\u63a5\u8fd10\uff0c\u5206\u6bcdq\u4e0d\u63a5\u8fd10\u65f6\uff0c\u6574\u4e2aln\u5c31\u5f88\u5c0f\uff0cKL\u6563\u5ea6\u5c31\u5f88\u5927\u3002\u800c\u6211\u4eecminimize KL\u6563\u5ea6\uff0c\u5c31\u904f\u5236\u8fd9\u79cd\u60c5\u51b5\u53d1\u751f\uff0c\u4e5f\u5c31\u662f\u8bf4\uff1aq\u8981\u7f29\u5728p\u7684\u5185\u90e8\uff0c\u5bf9\u5e94\u4e0a\u9762\u7684\u56fea \u5bf9\u4e8eKL(p||q)\uff1a \u5c31\u6b63\u76f8\u53cd\uff0cq\u8981\u7f29\u5230p\u7684\u5185\u90e8\uff0c\u5bf9\u5e94\u56feb</p> <p> \u5bf9\u4e8e\u4e00\u4e2amultimodal\u7684\u5206\u5e03\uff0c\u6211\u4eecminimize KL\u6563\u5ea6\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2aunimodal\u7684\u8fd1\u4f3c\u5206\u5e03\uff0c\u800c\u6b64\u65f6\u9009\u7528\u8fd9\u4e24\u79cdKL\u6563\u5ea6\u5c31\u4f1a\u6709\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u4e0e\u4e4b\u524d\u6211\u4eec\u5f97\u5230\u7684\u7ed3\u8bba\u4e00\u81f4\uff0cKL(p||q)\u4f1a\u5f97\u5230a\uff0c\u5305\u5728\u771f\u5b9e\u5206\u5e03p\u7684\u5916\u9762\uff0cKL(q||p)\u4f1a\u5f97\u5230b\u6216c\uff0c\u7f29\u5728\u771f\u5b9e\u5206\u5e03\u4e4b\u5185\u3002 \u800cKL(p||q)\u5f97\u5230\u7684\u8fd9\u4e2a\u5206\u5e03\u901a\u5e38\u8868\u73b0\u4e0d\u662f\u5f88\u597d\uff0c\u56e0\u4e3abecause the average of two good parameter values is typically itself not a good parameter value \u4e0d\u8fc7KL(p||q)\u4f1a\u572810.7\u8ba8\u8bbaexpectation propagation\u65f6\u53d1\u6325\u4f5c\u7528\u3002</p> <p>alpha family of divergences</p> <p></p> <p>\\(\\alpha\\to 1\\)\u5bf9\u5e94KL(p||q) \\(\\alpha\\to -1\\)\u5bf9\u5e94KL(q||p) \u5bf9\u4e8e\u6240\u6709\u7684\\(\\alpha\\)\u90fd\u6709\\(D_\\alpha(p||q)&gt;0\\) if and only if p(x)=q(x) For \u03b1&lt;=\u22121 the divergence is zero forcing, so that any values of x for which p(x)=0 will have q(x)=0, and typically q(x) will under-estimate the support of p(x) and will tend to seek the mode with the largest mass. Conversely for \u03b1&gt;=1 the divergence is zero-avoiding, so that values of x for which p(x) &gt; 0 will have q(x) &gt; 0, and typically q(x) will stretch to cover all of p(x), and will over-estimate the support of p(x). \u03b1 =0\u65f6\u7684\u60c5\u51b5\uff0cwe obtain a symmetric divergence that is linearly related to the Hellinger distance given by </p>"},{"location":"PRML/chap10/chap10/#1013-example-the-univariate-gaussian","title":"10.1.3 Example: The univariate Gaussian","text":""},{"location":"PRML/chap10/chap10/#102-illustration-variational-mixture-of-gaussians","title":"10.2. Illustration: Variational Mixture of Gaussians","text":"<p>\u9996\u5148\u6839\u636e\u4e0a\u4e00\u7ae0\u7684\u8ba8\u8bba\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e0b\u9762\u8fd9\u4e24\u4e2a\u5f0f\u5b50\uff1a \uff08\u8fd9\u91cc\u53ea\u4e0d\u8fc7\u628aN\u4e2a\u6837\u672c\u5408\u5230\u4e86\u4e00\u8d77\uff09   \u6ce8\u610f\u8fd9\u91cc\u7528precision matrices rather than covariance matrices\u6765\u7b80\u5316\u8868\u8fbe</p> <p>\u7136\u540e\u6211\u4eec\u7ed9\\(\\pi\\)\u8bbe\u4e00\u4e2a\u5148\u9a8c\uff0c\u56e0\u4e3aZ\u7684\u5206\u5e03\u662fmultinomial\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u7528dirichlet  where by symmetry we have chosen the same parameter \u03b10 for each of the components, and C(\u03b10) is the normalization constant for the Dirichlet distribution  As we have seen, the parameter \u03b10 can be interpreted as the effective prior number of observations associated with each component of the mixture.  If the value of \u03b10 is small, then the posterior distribution will be influenced primarily by the data rather than by the prior.</p> <p>Similarly, we introduce an independent Gaussian-Wishart prior governing the mean and precision of each Gaussian component, given by  because this represents the conjugate prior distribution when both the mean and precision are unknown  \u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0clatent variable\u548cparameter\u6700\u5927\u7684\u533a\u522b\u5c31\u662flatent variable\u5728plate\u4e2d\uff0c\u4e2a\u6570\u968f\u7740\u6570\u636e\u589e\u957f\u800c\u589e\u957f\u3002\u800c\u5728graph\u5c42\u9762\uff0c\u5176\u5b9e\u4e0eparameter\u6ca1\u6709\u6839\u672c\u4e0a\u7684\u533a\u522b\u3002</p>"},{"location":"PRML/chap10/chap10/#1021-variational-distribution","title":"10.2.1 Variational distribution","text":"<p>\u603b\u7684joint\u53ef\u4ee5\u5199\u4f5c  Note that only the variables X are observed.</p> <p>\u7136\u540e\u6211\u4eec\u73b0\u5728\u5c31\u6765\u8bbe\u4e00\u4e2a\u53ef\u4ee5factorize\u7684variational distribution  \u8fd9\u91cc\u6211\u4eecfactorize between the latent variables and the parameters </p> <p>It is remarkable that this is the only assumption that we need to make in order to obtain a tractable practical solution to our Bayesian mixture model. \u6709\u4e86\u8fd9\u4e2afactorize\u7684\u5047\u8bbe\uff0cq\u7684\u5f62\u5f0f\u5c31\u80fd\u81ea\u52a8\u5b9a\u4e0b\u6765</p>"},{"location":"PRML/chap10/chap10/#qz","title":"q(Z)","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u76f4\u63a5\u5957\u7528\u4e4b\u524d\u7684\u6700\u4f18\u89e3\u7684\u7ed3\u8bba\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a  \u7136\u540e\u6211\u4eec\u53ea\u4fdd\u7559Z\u76f8\u5173\u7684\u90e8\u5206\uff0c\u5176\u4ed6\u7684\u653e\u5230const\u4e2d </p> <p>\u4ee3\u5165\u4e4b\u524d\u7684\u4e24\u4e2ap\u7684\u5b9a\u4e49\uff0c\u5f97\u5230  where D is the dimensionality of the data variable x.</p> <p>\u540c\u65f6\u53d6\u5bf9\u6570  \u7136\u540e\u518dnormalize\u4e00\u4e0b\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a  </p> <p>We see that the optimal solution for the factor q(Z) takes the same functional form as the prior p(Z|\u03c0). \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u671f\u671b\uff1a\uff08\u7c7b\u6bd4multinomial\u7684\u671f\u671b\uff09  \u8fd9\u4e2a\\(r_{nk}\\)\u5176\u5b9e\u5c31\u8d77\u7740responsibility\u7684\u4f5c\u7528</p> <p>\u89c2\u5bdf\u5230q*(Z)\u7684\u6700\u4f18\u89e3\u4e2d\u6709\u5176\u4ed6\u53d8\u91cf\u7684\u671f\u671b\uff0c\u56e0\u6b64\u6211\u4eec\u8fd8\u662f\u8981\u7528iterative\u7684\u65b9\u6cd5\u6c42\u89e3</p>"},{"location":"PRML/chap10/chap10/#q","title":"q(\u03c0, \u00b5, \u039b)","text":"<p>\u4e0b\u9762\u5b9a\u4e49\u4e09\u4e2astatistics\uff0c\u7528\u4e8e\u7b80\u5316\u8868\u8fbe\uff1a  Note that these are analogous to quantities evaluated in the maximum likelihood EM algorithm for the Gaussian mixture model</p> <p>\u9996\u5148\u4ee3\u5165\u6700\u4f18\u89e3\uff1a  \uff08\u8fd9\u91cc\u89c2\u5bdf\u5230\u53f3\u8fb9\u7684\u9879\u8981\u4e48\u53ea\u6709\u03c0\uff0c\u8981\u4e48\u53ea\u6709\u00b5\u548c\u039b\u3002\u4f53\u73b0\u4e86q(\u03c0, \u00b5, \u039b)\u5728\u8fd9\u91cc\u5206\u89e3\u4e3aq(\u03c0)q(\u00b5, \u039b)\uff09 \u800c\u4e14\u6211\u4eec\u89c2\u5bdf\u5230\u542b\u6709\u00b5\u548c\u039b\u7684\u9879\u90fd\u6709\u6c42\u548c\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u628a\u6574\u4e2afactorization\u5199\u6210\uff1a </p> <p>\u5148\u6c42\u5173\u4e8e\u03c0\u7684\uff1a \u628a\u4e0e\u03c0\u65e0\u5173\u7684\u90fd\u4e22\u5230const\u4e2d\uff0c\u53ef\u4ee5\u5f97\u5230  Taking the exponential of both sides, we recognize q*(\u03c0) as a Dirichlet distribution </p> <p>\u518d\u6c42\u5173\u4e8e\u00b5\u548c\u039b\u7684\uff1a the variational posterior distribution \\(q*(\u00b5k, \u039bk)\\) does not factorize into the product of the marginals, but we can always use the product rule to write it in the form \\(q*(\u00b5k, \u039bk)= q*(\u00b5k|\u039bk)q*(\u039bk)\\) .</p> <p>\u7ed3\u679c\u4e5f\u662f\u4e00\u4e2aGaussian-Wishart distribution\uff1a(\u8fd9\u91cc\u63a8\u5bfc\u5c31\u8df3\u4e86)  These update equations are analogous to the M-step equations of the EM algorithm for the maximum likelihood solution of the mixture of Gaussians.  We see that the computations that must be performed in order to update the variational posterior distribution over the model parameters </p> <p>\u7136\u800c\u95ee\u9898\u8fd8\u6ca1\u89e3\u51b3\uff0c \u4e0a\u9762\u7684\u6700\u4f18\u89e3\u4e2d\u9700\u8981\u7528\u5230\\(r_{nk}\\), \\(r_{nk}\\)\u662f\u7531\\(\\rho _{nk}\\)normalize\u5f97\u5230\u7684\uff0c\\(\\rho _{nk}\\)\u4e2d\u53c8\u8981\u7528\u5230\\(E[ln\\pi_k]\\),\\(E[ln\\Lambda_k]\\)\u548c   We see that this expression involves expectations with respect to the variational distributions of the parameters, and these are easily evaluated to give  </p> <p>variational EM\u4e2d\uff0c  MLE EM\u4e2d\uff0c  \u53ef\u4ee5\u770b\u5230\u5f62\u5f0f\u662f\u5f88\u76f8\u8fd1\u7684</p> <p>\u603b\u7ed3\uff1a * In the variational equivalent of the E step,     we use the current distributions over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66) and hence evaluate E[znk]= rnk. * in the subsequent variational equivalent of the M step\uff0c     we keep these responsibilities fixed and use them to re-compute the variational distribution over the parameters using (10.57) and (10.59).</p> <p>\u540c\u65f6\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u5230\uff0c\u6211\u4eec\u6c42\u5f97\u7684variational posterior q\u4e0e\u6211\u4eec\u7684\u5047\u8bbep\u7684\u51fd\u6570\u5f62\u5f0f\u662f\u4e00\u6837\u7684\uff08dirichlet\u3001Gaussian-Wishart\uff09\u3002This is a general result and is a consequence of the choice of conjugate distributions.</p> <p> Components that take essentially no responsibility for explaining the data points have rnk\\(\\to\\) 0 and hence Nk\\(\\to\\) 0. From (10.58), we see that \u03b1k\\(\\to\\) \u03b10 and from (10.60)\u2013(10.63) we see that the other parameters revert to their prior values</p> <p>In fact if we consider the limit N \u2192\u221ethen the Bayesian treatment converges to the maximum likelihood EM algorithm.</p> <p>\u8ba1\u7b97\u91cf\u5927\u7684\u90e8\u5206\u4e3b\u8981\u662fthe evaluation of the responsibilities, together with the evaluation and inversion of the weighted data covariance matrices\uff0c\u800c\u8fd9\u4e9b\u5728MLE EM\u4e2d\u4e5f\u90fd\u6709\u3002\u56e0\u6b64variational\u7684\u65b9\u6cd5\u5e76\u6ca1\u6709\u589e\u5927\u591a\u5c11\u8ba1\u7b97\u91cf\u3002 \u4f18\u70b9\uff1a * \u89e3\u51b3\u4e86singlarity\u7684\u95ee\u9898\uff0cthese singularities are removed if we simply introduce a prior and then use a MAP estimate instead of maximum likelihood * there is no over-fitting if we choose a large number K of components in the mixture * the variational treatment opens up the possibility of determining the optimal number of components in the mixture without resorting to techniques such as cross validation</p>"},{"location":"PRML/chap10/chap10/#1022-variational-lower-bound","title":"10.2.2 Variational lower bound","text":"<p>\u5728\u6c42\u89e3\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u7b97\u4e00\u4e0b(10.3)\u7ed9\u51fa\u7684lower bound\uff0c\u7528\u4e8e\u9a8c\u8bc1\u7a0b\u5e8f\u662f\u5426\u6b63\u786e\u6216\u8005\u662f\u5426\u6536\u655b\u3002 variational GMM\u7684lower bound\uff1a   Note that the terms involving expectations of the logs of the q distributions simply represent the negative entropies of those distributions</p>"},{"location":"PRML/chap10/chap10/#1023-predictive-density","title":"10.2.3 Predictive density","text":"<p>\u5728\u9884\u6d4b\u4e2d\uff0c\u6211\u4eec\u8981\u5229\u7528X\u5efa\u6a21\uff0c\u7136\u540e\u4e3a\u4e86\u4e00\u4e2a\u65b0\u6765\u7684\\(\\hat{x}\\)\u627e\u5230\u5b83\u7684\\(\\hat{z}\\)  where p(\u03c0, \u00b5, \u039b|X) is the (unknown) true posterior distribution of the parameters</p> <p>\u7136\u540e\u5bf9\\(\\hat{z}\\)\u8fdb\u884csummation\uff1a </p> <p>\u7136\u540e\u63a5\u4e0b\u6765\u7684\u79ef\u5206\u5c31\u6ca1\u6cd5\u7b97\u4e86\uff0c\u7136\u540e\u7528variational approximation q(\u03c0)q(\u00b5, \u039b)\u6765\u4ee3\u66ffp(\u03c0, \u00b5, \u039b|X)</p> <p></p>"},{"location":"PRML/chap10/chap10/#1024-determining-the-number-of-components","title":"10.2.4 Determining the number of components","text":"<p>\u8fd8\u6709\u4e00\u4e2a\u9700\u8981\u5f3a\u8c03\u7684\u5730\u65b9\uff0c For any given setting of the parameters in a Gaussian mixture model , there will exist other parameter settings for which the density over the observed variables will be identical.</p> <p>\u50cfMLE\u4e00\u6837\uff0c\u6700\u7ec8\u6211\u4eec\u4f1a\u5f97\u5230K!\u4e2a\u7b49\u4ef7\u7684\u7ed3\u679c\u3002 \u5728MLE\u4e2d\uff0c\u6ca1\u4ec0\u4e48\u6240\u8c13\uff0c\u56e0\u4e3a\u6211\u4eec\u7ed9\u53c2\u6570\u4e00\u4e2a\u521d\u59cb\u503c\uff0c\u7136\u540e\u6536\u655b\u5230\u4e00\u4e2a\u89e3\u4e0a\uff0c\u5176\u4ed6\u7684\u89e3\u662f\u6ca1\u6709\u7528\u7684\u3002 \u5728variational\u4e2d\u4e5f\u662f\u4e00\u6837\uff0cif the true posterior distribution is multimodal, variational inference based on the minimization of KL(q||p) will tend to approximate the distribution in the neighbourhood of one of the modes and ignore the others.</p> <p>\u7136\u800c\u8ba9\u6211\u4eec\u8981\u6bd4\u8f83\u4e0d\u540c\u7684K\u7684model\u65f6\uff0c\u5c31\u8981\u8003\u8651multimodal\u3002\u6b64\u65f6\uff0c\u53ef\u4ee5add a term lnK! onto the lower bound when used for model comparison and averaging</p> <p>MLE\u7684likelihood\u7684\u503c\u968f\u7740K\u7684\u589e\u52a0\u5355\u8c03\u589e\u52a0\uff0c\u56e0\u6b64\u4e0d\u80fd\u7528\u4f5c\u6a21\u578b\u6bd4\u8f83\uff0c\u800cbaysian\u53ef\u4ee5\u3002</p> <p>\u53e6\u5916\u4e00\u79cd\u7b97\u5408\u9002\u7684K\u7684\u65b9\u6cd5\u662f\uff0ctreat the mixing coefficients \u03c0 as parameters and make point estimates of their values by maximizing the lower bound with respect to \u03c0 instead of maintaining a probability distribution over them as in the fully Bayesian approach \u4e5f\u5c31\u662f\u589e\u52a0\u4e00\u6b65\uff1a </p>"},{"location":"PRML/chap10/chap10/#1025-induced-factorizations","title":"10.2.5 induced factorizations","text":"<p>\u9664\u4e86\u6211\u4eec\u9884\u60f3\u7684factorization\uff0c\u6211\u4eec\u5728\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u81ea\u52a8\u4ea7\u751f\u4e00\u4e9b\u989d\u5916\u7684factorization\u3002\u8fd9\u662f\u4e0egraph\u7684\u7ed3\u6784\u6709\u5173\u7684\u3002 \u6bd4\u5982q(\u00b5, \u039b)\u53ef\u4ee5\u5206\u89e3\u4e3aK\u4e2acomponent\u7684\u4e58\u79ef\uff0cq(Z)\u53ef\u4ee5\u5206\u89e3\u4e3aN\u4e2a\\(q*(Z_n)\\)\u7684\u4e58\u79ef\u3002 \u8fd9\u79cd\u88ab\u989d\u5916\u5f15\u5165\u7684factorization\u79f0\u4e3ainduced factorizations\u3002 Such induced factorizations can easily be detected using a simple graphical test based on d-separation\u3002 \uff08\u5177\u4f53\u5c31\u8df3\u4e86\uff09</p>"},{"location":"PRML/chap10/chap10/#103-variational-linear-regression","title":"10.3. Variational Linear Regression","text":"<p>\u5c3d\u7ba1baysian linear regression\u4e0b\u7684intergration\u662fintractable\u7684\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u8ba8\u8bba\u4e00\u4e0b\u5bf9\u5e94\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002</p> <p>\u8fd9\u91cc\u6211\u4eec\u5047\u8bbeprecision \u03b2\u662f\u5df2\u77e5\u7684\uff0c\u5e76\u4e14fix\u5230\u5b83\u7684\u771f\u5b9e\u503c\u4e0a\uff0c\u7528\u4e8e\u7b80\u5316\u8ba1\u7b97\u3002</p> <p>\u5bf9\u4e8elinear regression model\u8fd9\u4e2a\u4f8b\u5b50\uff0cevidence framework\u548cvariational framework\u662f\u7b49\u4ef7\u7684\u3002\u4f46\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u8ba8\u8bba\u4e00\u4e0b\uff0c\u4e3a\u4e4b\u540e\u7684variational logistic regression \u4f5c\u94fa\u57ab\u3002</p> <p>\u5148\u56de\u5fc6\u4e00\u4e0b\u4e4b\u524d\u7684baysian regression\uff0c\u957f\u8fd9\u6837\uff1a  </p>"},{"location":"PRML/chap10/chap10/#1031-variational-distribution","title":"10.3.1 Variational distribution","text":"<p>\u6211\u4eec\u7684\u76ee\u6807\u662f\u7ed9posterior p(w,\u03b1|t)\u627e\u4e00\u4e2a\u8fd1\u4f3c </p> <p>\u76f4\u63a5\u628a\u03b1\u4ee3\u5165\u6700\u4f18\u89e3\u7684\u8868\u8fbe\u5f0f\uff0c\u5f97\u5230\uff1a   \u7136\u540e\u518d\u628aw\u4ee3\u5165\uff1a   \u5229\u7528gamma\u548cgaussian\u7684\u6027\u8d28\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a </p> <p>\u53c2\u6570\u7684\u6c42\u89e3\u65b9\u6cd5\u8fd8\u662f\u7ed9\u03b1\u548cw\u4e00\u4e2a\u521d\u59cb\u503c\uff0c\u7136\u540e\u5faa\u73af\u6c42\u89e3</p>"},{"location":"PRML/chap10/chap10/#1032-predictive-distribution","title":"10.3.2 Predictive distribution","text":""},{"location":"PRML/chap10/chap10/#1033-lower-bound","title":"10.3.3 Lower bound","text":""},{"location":"PRML/chap10/chap10/#104-exponential-family-distributions","title":"10.4. Exponential Family Distributions","text":"<p>\u5728\u8fd9\u4e00\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u628alatent variable\u8fdb\u4e00\u6b65\u5206\u4e3aintensive latent variable Z \u548c extensive parameter \u03b8 \u5176\u4e2dintensive\u8868\u793a\u4f1a\u968f\u7740data size\u53d8\u5316\uff0cextensive\u5219\u4e0d\u4f1a\u3002</p> <p>Now suppose that the joint distribution of observed and latent variables is a member of the exponential family, parameterized by natural parameters \u03b7 so that </p>"},{"location":"PRML/chap10/chap10/#105-local-variational-methods","title":"10.5. Local Variational Methods","text":"<p>\u4e4b\u524d\u8bb2\u7684\u65b9\u6cd5\u53ef\u4ee5\u770b\u4f5c\u662fglobal method\uff0cit directly seeks an approximation to the full posterior distribution over all random variables \u63a5\u4e0b\u6765\u4ecb\u7ecd\u4e00\u4e2alocal approach\uff1afinding bounds on functions over individual variables or groups of variables within a model \u6bd4\u5982\uff0c\u6211\u4eec\u4f1a\u627e\u4e00\u4e2aconditional distribution p(y|x)\u7684bound, \u7136\u800c\u8fd9\u53ea\u662fa much larger probabilistic model\u4e2d\u7684\u4e00\u4e2afactor This local approximation can be applied to multiple variables in turn until a tractable approximation is obtained</p>"},{"location":"PRML/chap10/chap10/#example-exp-x","title":"example: exp{-x}","text":"<p>\u8fd9\u91cc\u7528\u4e00\u4e2a\u4f8b\u5b50\u6765\u5c55\u793alocal method\uff1af(x)=exp{-x}\uff0c\u8fd9\u4e2a\u662f\u4e00\u4e2aconvex function Our goal is to approximate f(x) by a simpler function, in particular a linear function of x</p> <p>\u5f53\u8fd9\u4e2a\u7ebf\u6027\u51fd\u6570\u4e3af(x)\u7684\u5207\u7ebf\u65f6\uff0c\u53ef\u4ee5\u770b\u5230\u6b64\u65f6\u8fd9\u4e2a\u5207\u7ebf\u5c31\u662ff(x)\u7684\u4e00\u4e2alower bound\u3002 \u7531\u6cf0\u52d2\u5c55\u5f00\u53ef\u4ee5\u5f97\u5230f(x)\u5728\\(\\xi\\) \u5904\u7684\u5207\u7ebf\uff1a </p> <p>\\(y(x)\\le f(x)\\) with equality when x = \\(\\xi\\)</p> <p>\u7136\u540e\u6211\u4eec\u628af(x)\u6362\u6210exp(-x)\uff0c\u7136\u540e\u4ee4\\(\\lambda =-exp\\{-\\xi\\}\\)\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a  </p> <p>\u4e0d\u540c\u7684\u03bb\u5bf9\u5e94\u4e0d\u540c\u7684\u5207\u7ebf\uff0c\u6bcf\u4e00\u6761\u5207\u7ebf\u90fd\u662ff(x)\u7684lower bound\uff0c\u540c\u65f6\u03bb\u4e5f\u662f\u8fd9\u4e00\u6761\u5207\u7ebf\u7684\u659c\u7387\u3002\u5207\u7ebfparameterized by \u03bb\u3002 \u56e0\u4e3a\\(f(x)\\ge y(x, \\lambda)\\)\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230 </p> <p>\u7136\u540e\u6211\u4eec\u5c31\u7528\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\\(y(x, \\lambda)\\)\u8fd1\u4f3c\u4e86f(x)\uff0c\u4f46\u540c\u65f6\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u53c2\u6570\u03bb</p>"},{"location":"PRML/chap10/chap10/#generalize","title":"generalize","text":"<p>\u4e0b\u9762\u7528convex duality\u5bf9\u4e0a\u9762\u7684\u8fd9\u79cd\u65b9\u6cd5\u8fdb\u884c\u63a8\u5e7f\uff1a</p> <p> \u5728\u5de6\u56fe\u4e2d\uff0c\\(\\lambda x\\)\u662fconvex funxtion f(x)\u7684\u4e00\u4e2alower bound\uff0c\u4f46\u662f\u5e76\u4e0d\u662f\u6700tight\u7684\uff0c\u56e0\u6b64\u8981\u628a\u76f4\u7ebf\u5e73\u79fb\u5230\u4e0ef(x)\u76f8\u5207\u7684\u4f4d\u7f6e\u3002\u800c\u9700\u8981\u5e73\u79fb\u7684\u8ddd\u79bb\u53ef\u4ee5\u7531\\(min\\{f(x)-\\lambda x\\}\\)\u5f97\u5230\uff0c\u56e0\u6b64\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\\(g(\\lambda)\\):  \u521a\u624d\u6211\u4eec\u662ffixing \u03bb and varying x\uff0c\u73b0\u5728consider a particular x and then adjust \u03bb until the tangent plane is tangent at that particular x \u73b0\u5728\u6709\u4e86\\(g(\\lambda)\\)\uff0c\u6211\u4eec\u5df2\u7ecf\u80fd\u4fdd\u8bc1\u76f4\u7ebf\u4e0ef(x)\u76f8\u5207\u4e86\u3002\u73b0\u5728\u6211\u4eec\u60f3\u8981\u7ed9\u5b9a\u67d0\u4e00\u4e2ax\uff0c\u8c03\u6574\u03bb\uff0c\u4f7f\u5f97\u76f4\u7ebf\u4e0ef(x)\u5728x\u5904\u76f8\u5207\u3002 consider\u4e00\u4e2a\\(max_{\\lambda}(\\lambda x-g(\\lambda))\\)\uff0c\u5c1d\u8bd5\u6240\u6709\u7684\u03bb\uff0c\u6211\u4eec\u4f1a\u5f97\u5230\u4e00\u7cfb\u5217\u7684\u201c\u5207\u7ebf\u5728x\u5904\u7684y\u5750\u6807\u201d\u3002\u800c\u8fd9\u5176\u4e2d\u6700\u5927\u7684\uff0c\u5c31\u662f\u5207\u7ebf\u4e0ef(x)\u5728x\u5904\u76f8\u5207\u7684\u60c5\u51b5\uff0c\u6b64\u65f6max\u5f97\u5230\u7684\u8fd9\u4e2a\u503c\uff0c\u6070\u597d\u7b49\u4e8ef(x)\u5728x\u5904\u7684\u51fd\u6570\u503c\uff0c  \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e2a\u201c\u5de7\u5408\u201d\u8868\u793a\u4e3a\uff1a </p> <p>\u89c2\u5bdf\u8fd9\u4e24\u4e2a\u5f0f\u5b50\u7684\u5f62\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8fd9\u4e24\u4e2a\u5f0f\u5b50play a dual role \\(g(\\lambda)=max_{x}(\\lambda x-f(x))\\) \\(f(x)=max_{\\lambda}(\\lambda x-g(\\lambda))\\)</p> <p>\u7c7b\u6bd4\uff0c\u5bf9\u4e8econcave function\uff0c\u6211\u4eec\u53ef\u4ee5\u627eupper bound\uff0c\u6b64\u65f6\u8981\u628a\u4e0a\u8fb9\u5f0f\u5b50\u4e2d\u7684max\u90fd\u6362\u6210min\uff1a </p> <p>\u5982\u679cfunction\u65e2\u4e0d\u662fconvex\u4e5f\u4e0d\u662fconcave\uff0cwe can first seek invertible transformations either of the function or of its argument which change it into a convex form. We then calculate the conjugate function and then transform back to the original variables.</p> <p>\u4f8b\u5b50\uff1alogistic sigmoid function\uff1a  upper bound\uff1a sigmoid\u65e2\u4e0d\u662fconvex\u4e5f\u4e0d\u662fconcave\uff0c\u4f46\u662f\u7531\u4e8c\u9636\u5bfc\u6570\u53ef\u77e5\uff0csigmoid\u7684logarithm\u662fconcave\u7684\u3002\u56e0\u6b64\u6211\u4eec\u4ee4f(x)=sigmoid\u7684\u5bfc\u6570\u3002\u4ee4\\(f'(x)=\\lambda\\),\u6c42\u51fa\\(\\xi\\)\uff0c\u518d\u4ee3\u5165g(\u03bb)\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a  \u6211\u4eec\u53d1\u73b0\u8fd9\u5c31\u662fbinary entropy\uff0cfor a variable whose probability of having the value 1 is \u03bb</p> <p>\u7136\u540e\u6211\u4eec\u4ee3\u5165\u5230f(x)\u7684\u8868\u8fbe\u5f0f\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230sigmoid\u7684upperbound\uff1a  </p> <p>lower bound\uff1a We can also obtain a lower bound on the sigmoid having the functional form of a Gaussian  \\(f(x)=-ln(e^{x/2}+e^{-x/2})\\) is a convex function of the variable \\(x^2\\) \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u4e00\u4e2a\u5173\u4e8e\\(x^2\\)\u7684linear function\u4f5c\u4e3alower bound  \u6c42\u89e3\u65f6\uff0c\u4ee4\u03bb\u7b49\u4e8e\u659c\u7387\uff1a  \u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2a\u5f0f\u5b50\uff0c\u518d\u5229\u7528tanh\u548csigmoid\u7684\u5173\u7cfb\uff0c\u53ef\u4ee5\u505a\u4e00\u4e0b\u8f6c\u6362\uff1a </p> <p>\u4e0b\u9762\u8981\u8fdb\u884c\u4e00\u4e2a\u6bd4\u8f83\u5fae\u5999\u7684\u6539\u52a8\uff0c\uff08\u56e0\u4e3aPRML\u5c31\u8fd9\u4e48\u6539\u7684\uff09\uff0c\u6211\u4eec\u628a\u4e4b\u524d\u8ba8\u8bba\u4e2d\u7684\u6240\u6709\\(\\lambda\\)\u6539\u4e3a\\(\\eta\\)\uff0c\u7136\u540e\u5b9a\u4e49\\(\\lambda=-\\eta\\) \u6b64\u65f6\u4e0a\u9762\u7684(10.141)\u5c31\u53d8\u6210\u4e86  \u800c\u672c\u8d28\u4e0a\u5176\u5b9e\u4ec0\u4e48\u90fd\u6ca1\u6709\u6539\uff0c\u53ea\u662f\u6362\u4e86\u4e00\u4e0bnotation</p> <p>Instead of thinking of \u03bb as the variational parameter, we can let \u03be play this role as this leads to simpler expressions for the conjugate function, which is then given by  \u7136\u540e\u6211\u4eec\u5c31\u5f97\u5230\u4e86f(x)\u7684bound\uff1a   where \u03bb(\u03be) is defined by (10.141).</p> <p>\u4f7f\u7528\u8fd9\u4e9bbound\u7684\u4f8b\u5b50\uff1a</p> <p> where \u03c3(a) is the logistic sigmoid, and p(a) is a Gaussian probability density \u5728Bayesian models\u7684predictive distribution\u4e2d\uff0c\u6211\u4eec\u4f1a\u9047\u5230\u4e0a\u9762\u7684\u8fd9\u4e2a\u79ef\u5206\uff0c\u5176\u4e2dp(a)\u662f\u4e00\u4e2aposterior parameter distribution \u8fd9\u4e2a\u79ef\u5206\u6ca1\u6cd5\u7b97\uff0c\u56e0\u6b64\u6211\u4eec\u5229\u7528(10.144)\u8fd9\u4e2a\u8fd1\u4f3c\uff0c\u8fd9\u91cc\u6211\u4eec\u8bb0\u4f5c\\(\\sigma(a)\\ge f(a,\\xi)\\) where \u03be is a variational parameter</p> <p>The integral now becomes the product of two exponential-quadratic functions and so can be integrated analytically to give a bound on I </p> <p>\u6b64\u65f6\u6211\u4eec\u5c06\u5176\u8f6c\u5316\u4e3a\u4e86\u4e00\u4e2a\u5173\u4e8e\\(\\xi\\)\u7684\u4f18\u5316\u95ee\u9898\uff0cwhich we do by finding the value \u03be that maximizes the function F(\u03be) The resulting value F(\u03be) represents the tightest bound within this family of bounds and can be used as an approximation to I.</p>"},{"location":"PRML/chap10/chap10/#106-variational-logistic-regression","title":"10.6. Variational Logistic Regression","text":""},{"location":"PRML/chap10/chap10/#1061-variational-posterior-distribution","title":"10.6.1 Variational posterior distribution","text":"<p>\u4e0b\u9762\u6211\u4eec\u4f7f\u7528variational approximation based on the local bounds\u3002 This allows the likelihood function for logistic regression, which is governed by the logistic sigmoid, to be approximated by the exponential of a quadratic form. \u548c\u7b2c4\u7ae0\u4e00\u6837\uff0c\u6211\u4eec\u8fd8\u662f\u5148\u7ed9w\u8bbe\u4e00\u4e2agaussian\u7684prior  \u8fd9\u91cc\u6211\u4eec\u5148\u5047\u5b9a\\(m_0\\)\u548c\\(S_0\\)\u8fd9\u4e24\u4e2ahyperparameter\u90fd\u662ffixed constants\u3002\u572810.6.3\u4e2d\u518d\u62d3\u5c55\u5230unknown\u7684\u60c5\u51b5\u3002</p> <p>\u6211\u4eec\u5148\u5199\u51fabaysian logistic regression\u7684marginal\uff0c\u7136\u540e\u5229\u7528variational\u7684\u65b9\u6cd5\uff0c\u6c42\u51falower bound\u4ece\u800c\u5bf9\u5176\u8fdb\u884c\u8fd1\u4f3c\u3002</p> <p>baysian LR\u4e2d\uff0cmarginal likelihood\u662f\u8fd9\u6837\u7684\uff1a </p> <p>\u5bf9\u4e8e\u6bcf\u4e00\u4e2at\uff0c\u90fd\u6709\uff1a  where a = \\(w^T\\phi\\)</p> <p>\u56de\u5fc6\u6211\u4eec\u4e4b\u524d\u6c42\u7684sigmoid\u7684lower bound\uff1a  \u4ee3\u5165\u5f97\u5230\uff1a </p> <p>\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6bcf\u7b14data\u90fd\u5bf9\u5e94\u4e00\u4e2a\\(\\xi\\)\u3002 \u7136\u540e\u6211\u4eec\u628aa = \\(w^T\\phi\\)\u548clower bound\u4ee3\u5165joint\uff0c\u53ef\u4ee5\u770b\u5230\u6211\u4eec\u53ef\u4ee5\u7528\\(h(w,\\xi)\\)\u8fd1\u4f3cconditional likelihood </p> <p>Note that the function on the right-hand side cannot be interpreted as a probability density because it is not normalized.</p> <p>\u56e0\u4e3aln\u51fd\u6570\u5355\u8c03\u9012\u589e\uff0c\u6240\u4ee5\u4e0a\u9762\u7684\u4e0d\u7b49\u5f0f\uff0c\u4e24\u8fb9\u540c\u65f6\u53d6ln\uff0c\u4e0d\u7b49\u5f0f\u65b9\u5411\u4e0d\u53d8\uff1a </p> <p>\u6700\u540e\u4ee3\u5165prior p(w)\uff0c\u4e0d\u7b49\u5f0f\u53f3\u8fb9\u770b\u4f5c\u662fw\u7684function\uff1a  \u7136\u540e\u6211\u4eec\u53d1\u73b0ln posterior\u662f\u4e00\u4e2a\u5173\u4e8ew\u7684quadratic\uff0c\u56e0\u6b64\u53ef\u4ee5\u628aw\u7684\u5206\u5e03\u5199\u6210\u4e00\u4e2agaussian q(w)\uff0c\u7528\u8fd9\u4e2aq(w)\u6765\u8fd1\u4f3cp(t|w)p(w) \u7531\u6b64\u6211\u4eec\u5f97\u5230variational gaussian posterior: </p>"},{"location":"PRML/chap10/chap10/#1062-optimizing-the-variational-parameters","title":"10.6.2 Optimizing the variational parameters","text":"<p>\u5728\u505a\u9884\u6d4b\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u6c42\u89e3variational parameters {\u03ben} by maximizing the lower bound on the marginal likelihood.</p> <p>\u6211\u4eec\u628a\u4e4b\u524d\u7684\u5f97\u5230\u7684\u4e0d\u7b49\u5f0f\u4ee3\u5165marginal p(t)\u7684\u8868\u8fbe\u5f0f\uff1a </p> <p>\u56e0\u6b64\u6211\u4eec\u8981\u5229\u7528\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50maximize over \u03be\uff0c\u8fd9\u662f\u5c31\u53ef\u4ee5\u9009\u4e24\u79cd\u65b9\u6cd5\uff1a * we recognize that the function L(\u03be) is defined by an integration over w and so we can view w as a latent variable and invoke the EM algorithm * we integrate over w analytically and then perform a direct maximization over \u03be. Let us begin by considering the EM approach</p> <p>EM algorithm:</p>"},{"location":"PRML/chap2/chap2/","title":"Chap2","text":"<p>In a frequentist treatment, we choose specific values for the parameters by optimizing some criterion, such as the likelihood function.  By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes\u2019 theorem to compute the corresponding posterior distribution given the observed data.</p>"},{"location":"PRML/chap2/chap2/#21-binary-variables","title":"2.1. Binary Variables","text":"<p>Bernoulli Distribution:</p> <p></p> <p>\u5f53\u6211\u4eec\u6709N\u4e2a\u6570\u636e\uff0c\u72ec\u7acb\u540c\u5206\u5e03iid\uff0c\u5c31\u53ef\u4ee5\u5199\u51falikelihood\uff1a  </p> <p>\u5bf9\u4e8efrequentist\u89c6\u89d2\uff0c\u81ea\u7136\u8981\u6c42\u89e3\u03bc\u4f7f\u5f97likelihood\u6700\u5927\uff1a  \u4f46\u662f\u8fd9\u91cc\u6709\u4e2a\u9700\u8981\u6ce8\u610f\u7684\u70b9\uff1a \u5728log likelihood\u4e2d\uff0c\u03bc\u548c1-\u03bc\u90fd\u662f\u5e38\u91cf\uff0c\u7ed3\u679c\u5c31\u662flikelihood\u53ea\u53d6\u51b3\u4e8e\\(\\Sigma_{n}x_{n}\\)\u6709\u5173\u3002\u77e5\u9053\u8fd9\u4e2a\u503c\uff0c\u5c31\u53ef\u4ee5\u6c42\u5f97likelihood \\(\\Sigma_{n}x_{n}\\)\u8fd9\u4e2a\u503c\uff0c\u6211\u4eec\u79f0\u4e3asufficient statistic \u5bf9\u03bc\u6c42\u5bfc\uff0c\u53ef\u4ee5\u5f97\u5230  \u4e5f\u5c31\u662fsample mean\u3002 \u8bb0x=1\u6709m\u4e2a\u6837\u672c\uff0c\u5219 </p> <p>Binomial distribution distribution of the number m of observations of x =1, given that the data set has size N </p> <p>\u5bf9\u4e8e\u72ec\u7acb\u4e8b\u4ef6\uff0cthe mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u671f\u671b\u548c\u65b9\u5dee\uff1a </p>"},{"location":"PRML/chap2/chap2/#211-the-beta-distribution","title":"2.1.1 The beta distribution","text":"<p>conjugacy If we choose a prior to be proportional to powers of \u00b5 and (1 \u2212 \u00b5), then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior.</p> <p>beta distribution   \u7531\u5206\u90e8\u79ef\u5206\u6cd5\uff1a  </p> <p>The posterior distribution of \u00b5 is now obtained by multiplying the beta prior(2.13) by the binomial likelihood function (2.9) and normalizing.  where l = N \u2212 m</p> <p>\u5bf9\u6bd4(2.13)\u7684\u5f62\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0posterior\u4e5f\u662f\u4e00\u4e2aBeta\u5206\u5e03\uff1a </p> <p>We see that the effect of observing a data set of m observations of x =1 and l observations of x =0 has been to increase the value of a by m, and the value of b by l, in going from the prior distribution to the posterior distribution \u6211\u4eec\u53ef\u4ee5\u628a\u89c2\u6d4b\u6570\u636e\u7684\u8fc7\u7a0b\u770b\u4f5c\u662f\u4e00\u4e2a\u201c\u4e0d\u65ad\u4fee\u6539Beta\u5206\u5e03\u7684\u53c2\u6570\u7684\u8fc7\u7a0b\u201d\uff0c\u7531\u6b64\u4ece\u5148\u9a8c\u8f6c\u5316\u4e3a\u540e\u9a8c</p> <p>This allows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x =1 and x =0, respectively. \u8fd9\u53e5\u8bdd\u5f88\u96be\u7406\u89e3\uff0c\u6211\u7684\u7406\u89e3\u662f\uff1a\u89c2\u6d4b\u6570\u636e\u7684\u8fc7\u7a0b\u662f\u4e00\u4e2a\u4e0d\u65ad\u589e\u52a0a\u548cb\u7684\u8fc7\u7a0b\uff0c\u4e5f\u662f\u4e00\u4e2a\u4e0d\u65ad\u4fee\u6b63\u5148\u9a8c\u7684\u8fc7\u7a0b\u3002 \u800c\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e4b\u524d\uff0c\u5f53\u6211\u4eec\u89c2\u6d4b0\u4e2a\u6570\u636e\u65f6\uff0c\u6211\u4eec\u5bf9\u5148\u9a8c\u7684\u5047\u8bbe\u4e5f\u53ef\u4ee5\u770b\u4f5c\u89c2\u5bdf\u4e86\u67d0\u51e0\u4e2a\u6570\u636e\u7684\u7ed3\u679c\uff08fictitious prior observation\uff09\u3002\u800c\u603b\u4f53\u7684\u503c\u5c31\u53eb\u505aeffective number of observations\u3002\u4e5f\u5c31\u662f\u622a\u6b62\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u89c2\u6d4b\u5230\u7684\u6570\u636e\u4e2a\u6570\uff08\u5305\u62ec\u6211\u4eec\u6700\u521d\u201c\u8499\u5bf9\u7684\u201d\uff09</p> <p>\u9884\u6d4b\u65f6\uff1a  \u7531\u4e8e\u4e4b\u524d\u63d0\u5230\u7684beta distribution\u7684mean\u7684\u516c\u5f0f\uff0c </p> <p>which has a simple interpretation as the total fraction of observations (both real observations and fictitious prior observations) that correspond to x =1</p> <p>\u7531\u6b64\u53ef\u4ee5\u5f97\u51faMLE\u4e0eMAP\u7684\u5173\u7cfb\uff1a \u5f53m, l\u8d8b\u4e8e\u65e0\u7a77\u65f6\uff0cMAP\u7b49\u4e8eMLE \u800c\u5bf9\u4e8e\u6709\u9650\u7684\u6570\u636e\uff0cthe posterior mean for \u00b5 always lies between the prior mean and the maximum likelihood estimate for \u00b5 corresponding to the relative frequencies of events given by (2.7).</p> <p> \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5728Beta distribution\u4e2d\uff0c\u968f\u7740\u89c2\u5bdf\u6570\u7684\u589e\u52a0\uff0c\u5206\u5e03\u53d8\u5f97\u8d8a\u6765\u8d8asharply peaked\u3002 In fact, we might wonder whether it is a general property of Bayesian learning that, as we observe more and more data, the uncertainty represented by the posterior distribution will steadily decrease. \u90a3\u4e48\u8fd9\u662f\u4e0d\u662f\u4e00\u4e2a\u5171\u6709\u7684\u6027\u8d28\u5462 on average, the posterior variance of \u03b8 is smaller than the prior variance</p>"},{"location":"PRML/chap2/chap2/#22-multinomial-variables","title":"2.2. Multinomial Variables","text":"<p>a generalization of the Bernoulli distribution</p> <p>\u5982\u679cx\u662fone hot vector\uff0c\u4e14the probability of \\(x_{k} =1\\) is \\(\\mu_{k}\\) </p> <p>\u5176\u4e2d\\(\\Sigma_{k}\\space\\mu_{k}=1\\)\u4e14\\(\\mu_{k}\\ge0\\)</p> <p></p> <p>\u968f\u540e\u53ef\u4ee5\u5199\u51falikelihood\u7684\u5f62\u5f0f\uff1a </p> <p>\u7c7b\u6bd4\u4e8ebinomial\uff0c\u8fd9\u91cc\u7684likelihood\u4e5f\u53ea\u4e0ek\u4e2asum\u6709\u5173\uff1a  \u4e5f\u662fsufficient statistics</p> <p>\u7531\u62c9\u683c\u6717\u65e5\u53ef\u4ee5\u6c42\u51fa\\(\\mu_{ML}\\),\u4e5f\u5c31\u662fMLE\u7684\u89e3 </p> <p></p> <p>multinomial distribution</p> <p>joint distribution of the quantities m1,...,mK, conditioned on the parameters \u00b5 and on the total number N of observations</p> <p> </p>"},{"location":"PRML/chap2/chap2/#221-the-dirichlet-distribution","title":"2.2.1 The Dirichlet distribution","text":"<p>\u540e\u9a8c\u7684\u5f62\u5f0f\uff1a </p> <p>\u53ef\u89c1\u540e\u9a8c\u4e5f\u662f\u4e00\u4e2aDirichlet \u7531\u6b64\u901a\u8fc7\u89c2\u5bdf\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u540e\u9a8cnormalization term\u7684\u53c2\u6570\uff1a  \u5176\u4e2dwhere we have denoted m =(m1,...,mK)T.</p> <p>Beta\u4e0eDirichlet\u7684\u5173\u7cfb\uff1a As for the case of the binomial distribution with its beta prior, we can interpret the parameters \\(\\alpha_{k}\\) of the Dirichlet prior as an effective number of observations of \\(x_{k}=1\\).</p>"},{"location":"PRML/chap2/chap2/#23-the-gaussian-distribution","title":"2.3. The Gaussian Distribution","text":"<p>geometrical form</p> <p>gaussian\u4e2d\u4e0e\\(\\mathrm{x}\\)\u6709\u5173\u7684\u53ea\u6709  \u4f4d\u4e8e\u6307\u6570\u90e8\u5206\u3002\u8fd9\u4e00\u90e8\u5206\u79f0\u4e3a\u03bc\u4e0ex\u7684Mahalanobis distance\u3002 \u5f53\u03a3\u4e3aidentity matrix\u65f6\uff0creduce to Euclidean distance\u3002</p>"},{"location":"PRML/chap3/chap3/","title":"Chap3","text":""},{"location":"PRML/chap3/chap3/#31-linear-basis-function-models","title":"3.1. Linear Basis Function Models","text":"<p>\u7ebf\u6027\u56de\u5f52\u5c31\u662f\u666e\u901a\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u6709\u5f88\u5927\u7684\u9650\u5236\uff0c\u56e0\u6b64\u8fd9\u91cc\u62d3\u5c55\u4e3alinear combinations of fixed nonlinear functions of the input variables   where \\(\u03c6_j(x)\\) are known as basis functions</p> <p>\u51e0\u4e2abasis\uff1a Gaussian basis\uff1a  (although it should be noted that they are not required to have a probabilistic interpretation)</p> <p>sigmoidal basis:  Equivalently, we can use the \u2018tanh\u2019 function because this is related to the logistic sigmoid by tanh(a)=2\u03c3(a) \u2212 1</p> <p>(\u5728\u8fd9\u7ae0\uff0c\u5e76\u4e0d\u5177\u4f53\u7279\u6307\u67d0\u79cdbasis\uff0c\u56e0\u6b64\u7b80\u8bb0\u03c6(x)= x)</p>"},{"location":"PRML/chap3/chap3/#311-maximum-likelihood-and-least-squares","title":"3.1.1 Maximum likelihood and least squares","text":"<p>Note that the Gaussian noise assumption implies that the conditional distribution of t given x is unimodal, which may be inappropriate for some applications. An extension to mixtures of conditional Gaussian distributions, which permit multimodal conditional distributions, will be discussed in Section 14.5.1.</p> <p>\u628a\u4e0a\u8fb9\u7684\u5f0f\u5b50\u6539\u5199\u6210\u5411\u91cf\u5f62\u5f0f\uff1aLikelihood function:  Note that in supervised learning problems such as regression (and classification), we are not seeking to model the distribution of the input variables. \u56e0\u6b64X\u6c38\u8fdc\u5728condition\u8fd9\u8fb9\uff0c\u56e0\u6b64\u7701\u7565  </p> <p>\u8fd9\u65f6\u68af\u5ea6\u4e3a  \u89e3\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\uff0c\u53ef\u4ee5\u5f97\u5230\u6b63\u89c4\u65b9\u7a0b </p> <p>pseudo inverse\uff1a \u5f53\u77e9\u9635is square and invertible\uff0cpseudo inverse\u8f6c\u5316\u4e3ainverse </p>"},{"location":"PRML/chap3/chap3/#313-sequential-learning","title":"3.1.3 Sequential learning","text":"<p>if the data set is sufficiently large, it may be worthwhile to use sequential algorithms, also known as on-line algorithms, in which the data points are considered one at a time, and the model parameters updated after each such presentation. -&gt; stochastic gradient descent, also known as sequential gradient descent</p>"},{"location":"PRML/chap3/chap3/#314-regularized-least-squares","title":"3.1.4 Regularized least squares","text":"<p>L2\u5bf9\u5e94\u7684solution\uff1a </p> <p>\u6b63\u5219\u5316\u662f\u4e3a\u4e86\u7ea6\u675fw\uff0c\u56e0\u6b64\u53ef\u4ee5\u5199\u6210  \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528Lagrange\uff1a </p> <p>quadratic\uff0c\u7ea6\u675f\u4e3a\u4eff\u5c04\uff0c\u6ee1\u8db3KKT\uff0c\u56e0\u6b64\uff1a  \u53ef\u4ee5\u628a\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u753b\u5230w\u5404\u7ef4\u7ec4\u6210\u7684\u7a7a\u95f4\u4e2d\uff0c\u84dd\u8272\u7684\u4e3aerror\u7684\u7b49\u9ad8\u7ebf\uff0c\u5c31\u80fd\u5f97\u5230\u4e0b\u9762\u8fd9\u5f20\u719f\u6089\u7684\u56fe\uff1a </p>"},{"location":"PRML/chap3/chap3/#315-multiple-outputs","title":"3.1.5 Multiple outputs","text":""},{"location":"PRML/chap3/chap3/#32-the-bias-variance-decomposition","title":"3.2. The Bias-Variance Decomposition","text":"<p>h(x)\u662ft\u7684\u6761\u4ef6\u671f\u671b\uff0c\u8fd9\u4e2a\u57281.5.5\u91cd\u5df2\u7ecf\u63d0\u5230\u8fc7 Loss\u7684\u671f\u671b\u53ef\u4ee5\u5206\u89e3\u4e3a\u4ee5\u4e0b\u4e24\u9879\uff0c\u7b2c\u4e00\u9879y(x)\u4e0emodel\u6709\u5173\uff0c\u800c\u7b2c\u4e8c\u9879\u53ea\u53d6\u51b3\u4e8e\u6570\u636e\u81ea\u8eab\u7684noise </p> <p>\u4e0b\u9762\u6765\u63a2\u8ba8model\u81ea\u8eab\u7684uncertainty\uff1a * \u5982\u679c\u662fbayesian\u65b9\u6cd5\uff0cmodel\u7684\u4e0d\u786e\u5b9a\u6027\u7531posterior distribution over w\u51b3\u5b9a * \u5982\u679c\u662ffrequentist\u65b9\u6cd5\uff1a     A frequentist treatment, however, involves making a point estimate of w based on the data set D, and tries instead to interpret the uncertainty of this estimate through the following thought experiment. Suppose we had a large number of data sets each of size N and each drawn independently from the distribution p(t, x).      For any given data set D, we can run our learning algorithm and obtain a prediction function y(x; D). Different data sets from the ensemble will give different functions and consequently different values of the squared loss. The performance of a particular learning algorithm is then assessed by taking the average over this ensemble of data sets.</p> <p>\u5bf9\u4e8e\u4e0a\u9762\u5f0f\u5b50\u7684\u7b2c\u4e00\u9879\u8fdb\u884c\u914d\u51d1\uff0c\u518d\u5bf9\u4e8eD\u53d6\u671f\u671b\uff0c\u53ef\u4ee5\u5f97\u5230 </p> <p></p> <p></p>"},{"location":"PRML/chap3/chap3/#33-bayesian-linear-regression","title":"3.3. Bayesian Linear Regression","text":""},{"location":"PRML/chap3/chap3/#331-parameter-distribution","title":"3.3.1 Parameter distribution","text":"<p>For the moment, we shall treat the noise precision parameter \u03b2 as a known constant.  \u8fd9\u91cc\u9009\u62e9gausiian\u662f\u56e0\u4e3a\uff0c\u89c2\u5bdf  \u53ef\u4ee5\u53d1\u73b0p(t|w) defined by (3.10) is the exponential of a quadratic function of w \u56e0\u6b64\u5171\u8f6d\u7684\u5148\u9a8c\u5c31\u9009\u62e9Gaussian </p> <p>Due to the choice of a conjugate Gaussian prior distribution, the posterior will also be Gaussian</p> <p>\u8fd9\u91cc\u4f7f\u7528\u7b2c\u4e8c\u7ae0\u7684conditional of gaussian\u7684\u7ed3\u8bba\u53ef\u4ee5\u76f4\u63a5\u5199\u51faprior\u4e0elikelihood\u76f8\u4e58\u4e4b\u540e\u5f52\u4e00\u5316\u5f97\u5230\u7684posterior\u7684\u8868\u8fbe\u5f0f  </p> <p>\u56e0\u4e3agaussian\u7684\u6700\u503c\u5c31\u7b49\u4e8e\u5747\u503c\uff0c\u56e0\u6b64 \\(w_{MAP} = w_N\\) \u540c\u65f6\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\uff0c\u5f53prior\u7684\\(variance\\to\\infty\\)\uff0c\u540e\u9a8c\u7684\\(m_N\\)\u4f1a\u8f6c\u5316\u4e3aMLE\u7684\u7ed3\u679c\uff0c\u4e5f\u5c31\u662f\u4e4b\u524d\u63d0\u5230\u7684\u6b63\u89c4\u65b9\u7a0b </p> <p>\u672c\u7ae0\u4e0b\u6587\u4e2d\uff0c\u8ba8\u8bba\u7684\u662fzero-mean isotropic(\u5404\u5411\u540c\u6027) Gaussian governed by a single precision parameter \u03b1 </p> <p>log of posterior:  \u6700\u5927\u5316log posterior\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316sum-of-squares with regularization term \u03bb = \u03b1/\u03b2</p>"},{"location":"PRML/chap3/chap3/#332-predictive-distribution","title":"3.3.2 Predictive distribution","text":"<p>\u5229\u7528\u4e0a\u8fb9\u76842.115\u7684margin\u7ed3\u8bba\uff0c\u53ef\u4ee5\u76f4\u63a5\u5f97\u51faoutput\u7684\u5206\u5e03\uff1a </p> <p>\u5728variance\u7684\u8868\u8fbe\u5f0f\u4e2d\uff0c\u7b2c\u4e00\u9879\u662f\u56e0\u4e3a\u6570\u636e\u7684noise\uff0c\u7b2c\u4e8c\u9879\u662f\u56e0\u4e3aw\u7684uncertainty\u3002\u800cnoise\u548cw\u662f\u72ec\u7acb\u7684\uff0c\u56e0\u6b64\u662f\u76f8\u52a0\u7684\u5173\u7cfb</p> <p>\u7f3a\u9677\uff1a If we used localized basis functions such as Gaussians, then in regions away from the basis function centres, the contribution from the second term in the predictive variance (3.59) will go to zero, leaving only the noise contribution \u03b2\u22121. </p> <p>Thus, the model becomes very confident in its predictions when extrapolating outside the region occupied by the basis functions, which is generally an undesirable behaviour. This problem can be avoided by adopting an alternative Bayesian approach to regression known as a Gaussian process.</p>"},{"location":"PRML/chap3/chap3/#333-equivalent-kernel","title":"3.3.3 Equivalent kernel","text":"<p>the predictive mean can be written in the form </p> <p>\u8fd9\u4e2a\u5f0f\u5b50\u53ef\u4ee5\u5199\u6210kernel\u7684\u5f62\u5f0f\uff0c\u89e3\u91ca\u4e3a\u6240\u6709\u6570\u636e\u7684\u7ebf\u6027\u7ec4\u5408\uff1a  \u8fd9\u4e2a\u7684kernel\u53eb\u505a smoother matrix or the equivalent kernel Regression functions, such as this, which make predictions by taking linear combinations of the training set target values are known as linear smoothers.</p> <p>\u4ee5\u4e0a\u6211\u4eec\u5148\u627e\u4e86basis function\uff0c\u7136\u540e\u627e\u5230\u7b49\u4ef7\u7684kernel\u3002\u5176\u5b9e\u6211\u4eec\u8fd8\u53ef\u4ee5\u76f4\u63a5\u5b9a\u4e49\u4e00\u4e2alocal kernel\uff0c\u76f4\u63a5\u7528\u5df2\u77e5\u7684\u6240\u6709\u6570\u636e\u505a\u9884\u6d4b\u3002\u8fd9\u5c31\u662fGaussian processes</p> <p></p> <p>equivalent kernel\u548c\u5176\u4ed6\u6240\u6709kernel\u90fd\u6709\u4e00\u4e2a\u91cd\u8981\u7684\u6027\u8d28\uff0cit can be expressed in the form an inner product with respect to a vector \u03c8(x) of nonlinear functions </p>"},{"location":"PRML/chap3/chap3/#35-the-evidence-approximation","title":"3.5. The Evidence Approximation","text":"<p>\u5728fully Bayesian treatment\u4e2d\uff0c\u6211\u4eec\u8981\u4e3a\u8d85\u53c2\u6570\u03b1\u548c\u03b2\u8bbe\u5148\u9a8c\uff0cand make predictions by marginalizing with respect to these hyperparameters as well as with respect to the parameters w. \u4f46\u662f\u5b9e\u9645\u4e0a\u8fd9\u6837\u7684\u79ef\u5206\u662fintractable\u7684\u3002</p> <p>Here we discuss an approximation in which we set the hyperparameters to specific values determined by maximizing the marginal likelihood function obtained by first integrating over the parameters w \uff08\u6211\u4eec\u5148\u5bf9w\u79ef\u5206\uff0c\u7136\u540e\u7528MLE\u786e\u5b9ahyperparameters\uff09</p> <p>\u8fd9\u79cd\u65b9\u6cd5\u6709\u597d\u51e0\u4e2a\u540d\u5b57\uff1aempirical Bayes, type 2 maximum likelihood, generalized maximum likelihood, evidence approximation</p> <p>\u7cbe\u786e\u7684\u5199\u6cd5\u5e94\u8be5\u662f\u8fd9\u6837  \u4f46\u5982\u679cp(\u03b1, \u03b2|t)\u4e2d\uff0c\u03b1, \u03b2\u90fdsharply peaked\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u628a\u8fd9\u4e24\u4e2a\u53c2\u6570\u56fa\u5b9a\uff0c\u8fd1\u4f3c\u4e3a\u4e0b\u9762\u7684\u5f0f\u5b50 </p> <p>\u03b1, \u03b2\u7684\u540e\u9a8c\u7531\u4e0b\u9762\u8fd9\u4e2a\u5f0f\u5b50\u51b3\u5b9a\uff0c  If the prior is relatively flat, then in the evidence framework the values of \u03b1 and \u03b2 \u5c31\u53ef\u4ee5\u901a\u8fc7maxmize likelihood \u5f97\u5230\uff0c\u8fd9\u91cc\u7684MLE\u662f\u901a\u8fc7\u6240\u6709\u7684training data\u76f4\u63a5\u5f97\u5230\u03b1, \u03b2\uff0c\u800c\u4e0d\u7528cross-validation Recall that the ratio \u03b1/\u03b2 is analogous to a regularization parameter.</p> <p>Returning to the evidence framework, we note that there are two approaches that we can take to the maximization of the log evidence</p> <ul> <li>evaluate the evidence function analytically and then set its derivative equal to zero to obtain re-estimation equations for \u03b1 and \u03b2, which we shall do in Section 3.5.2</li> <li>use a technique called the expectation maximization (EM) algorithm</li> </ul> <p>The marginal likelihood function p(t|\u03b1, \u03b2) is obtained by integrating over the weight parameters w, so that  \u5176\u4e2d\uff0c\u4ee3\u5165   \u53ef\u4ee5\u5f97\u5230  </p>"},{"location":"PRML/chap8/chap8/","title":"8. GRAPHICAL MODELS","text":"<p>probabilistic graphical models</p> <p>In a probabilistic graphical model, each node represents a random variable (or group of random variables), and the links express probabilistic relationships between these variables.</p> <p>\u6709\u5411\u56fe\uff1aBayesian networks \u65e0\u5411\u56fe\uff1aMarkov random fields</p> <p>\u6709\u5411\u56fe expressing causal relationships between random variables \u65e0\u5411\u56fe expressing soft constraints between random variables.</p> <p>\u5728solving inference problems\u65f6\uff0c\u4e00\u822c\u4e3a\u4e86\u8ba1\u7b97\u65b9\u4fbf\uff0c\u8981\u628a\u6709\u5411\u56fe\u548c\u65e0\u5411\u56fe\u8f6c\u5316\u4e3afactor graph</p>"},{"location":"PRML/chap8/chap8/#81-bayesian-networks","title":"8.1. Bayesian Networks","text":"<p>\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u4efb\u610f\u4e00\u4e2a3\u4e2a\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\u90fd\u53ef\u4ee5\u5199\u6210\u8fd9\u79cd\u5f62\u5f0f\uff0c\u5e76\u753b\u51fabayesian network  </p> <p>\u9700\u8981\u6ce8\u610f\uff0c\u5f0f\u5b50\u7684\u5de6\u8fb9\u662f\u5bf9\u79f0\u7684\uff0c\u7136\u800c\u6700\u7ec8\u753b\u51fa\u6765\u7684\u56fe\u4e0d\u5bf9\u79f0\u3002\u5c55\u5f00\u7684\u987a\u5e8f\u4e0d\u540c\uff0c\u6700\u7ec8\u5f62\u6210\u7684\u56fe\u4e5f\u5c31\u4e0d\u4e00\u6837\u3002</p> <p>\u6269\u5c55\u5230K\u4e2a\u53d8\u91cf\uff1a\u5982\u679c\u6309\u8fd9\u6837\u5c55\u5f00\uff0c\u5219\u5f62\u6210\u7684\u56fe\u662ffully connected  </p> <p>\u7136\u800c\uff0cit is the absence of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents</p> <p>Each such conditional distribution will be conditioned only on the parents of the corresponding node in the graph. </p> <p></p> <p>\u56e0\u6b64\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u6709\u5411\u56fe\uff0c\u6211\u4eec\u4fbf\u80fd\u5199\u51fa\u5bf9\u5e94\u7684joint distribution\uff1a  \u8fd9\u4f53\u73b0\u4e86the factorization properties of the joint distribution for a directed graphical model</p> <p>\u6ce8\uff1a\u8fd9\u91cc\u7684\u56fe\u5fc5\u987bno directed cycles\uff0c\u4e5f\u5c31\u662fDAG</p>"},{"location":"PRML/chap8/chap8/#811-example-polynomial-regression","title":"8.1.1 Example: Polynomial regression","text":"<p>\u5728Polynomial regression\u4e2d\uff1a The random variables in this model are the vector of polynomial coefficients \\(\\mathrm{w}\\) and the observed data \\(t=(t_1,...,t_N)^T\\).  In addition, this model contains the input data \\(x =(x_1,...,x_N)^T\\), the noise variance \\(\u03c3^2\\), and the hyperparameter \\(\\alpha\\) representing the precision of the Gaussian prior over w, all of which are parameters of the model rather than random variables.</p> <p>\u8fd9\u6837\u4e00\u6765\uff0c\\(t\\)\u548c\\(\\mathrm{w}\\)\u7684joint distribution\u5c31\u53ef\u4ee5\u5199\u4f5c\uff1a  </p> <p>\u4e0a\u56fe\u4e2d\u7684N\u4e2at\u7ed3\u70b9\u4e0d\u65b9\u4fbf\uff0cWe introduce a graphical notation that allows such multiple nodes to be expressed more compactly, in which we draw a single representative node tn and then surround this with a box, called a plate, labelled with N indicating that there are N nodes of this kind. </p> <p>\u6709\u65f6\u6211\u4eec\u4f1a\u628a\u53c2\u6570\u4e5f\u5199\u5230\u8868\u8fbe\u5f0f\u4e2d\uff0c </p> <p>\u800c\u8fd9\u4e9b\u53c2\u6570\u4e0d\u662f\u968f\u673a\u53d8\u91cf\uff0c\u4e0d\u80fd\u753b\u6210\u7a7a\u5fc3\u5706\uff0crandom variables will be denoted by open circles, and deterministic parameters will be denoted by smaller solid circles. </p> <p>In a graphical model, we will denote such observed variables by shading the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which the variables {tn} are observed is shown in Figure 8.6. Note that the value of w is not observed, and so w is an example of a latent variable, also known as a hidden variable. Such variables play a crucial role in many probabilistic models and will form the focus of Chapters 9 and 12.</p> <p></p> <p>\uff08\u5728\u89c2\u6d4b\u5230\u67d0\u4e9b\u53d8\u91cf\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5199\u51fa\\(\\mathrm{w}\\)\u7684\u540e\u9a8c\uff1a\uff09 </p> <p>\u5176\u5b9e\u53c2\u6570\u7684\u540e\u9a8c\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u8981\u7528\u6a21\u578b\u505a\u51fa\u9884\u6d4b\u3002 \u8bb0\u65b0\u6765\u7684\u8f93\u5165\u4e3a\\(\\hat{x}\\), \u6211\u4eec\u60f3\u8981\u627e\u5230the corresponding probability distribution of \\(\\hat{t}\\) conditioned on the observed data</p> <p></p> <p>and the corresponding joint distribution of all of the random variables in this model, conditioned on the deterministic parameters, is then given by </p> <p></p>"},{"location":"PRML/chap8/chap8/#812-generative-models","title":"8.1.2 Generative models","text":"<p>ancestral sampling</p> <p>We shall suppose that the variables have been ordered such that there are no links from any node to any lower numbered node, in other words each node has a higher number than any of its parents. Our goal is to draw a sample \\(\\hat{x}_1,...,\\hat{x}_K\\) from the joint distribution.</p> <p> Note that at each stage, these parent values will always be available becauce they correspond to lower numbered nodes that have already been sampled</p> <p>To obtain a sample from some marginal distribution corresponding to a subset of the variables, we simply take the sampled values for the required nodes and ignore the sampled values for the remaining nodes. </p> <p>The primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler (typically exponential family) conditional distributions.</p> <p>Two cases are particularly worthy of note, namely when the parent and child node each correspond to discrete variables and when they each correspond to Gaussian variables, because in these two cases the relationship can be extended hierarchically to construct arbitrarily complex directed acyclic graphs.</p> <p>Discrete \\(\\to\\) Discrete</p> <p>Gaussian \\(\\to\\) Gaussian</p>"},{"location":"PRML/chap8/chap8/#813-discrete-variables","title":"8.1.3 Discrete variables","text":"<p>\u5355\u4e2a\u79bb\u6563\u53d8\u91cf\u7684\u6982\u7387\u5206\u5e03\u662f\u8fd9\u6837\u7684  \u4e24\u4e2a\u79bb\u6563\u53d8\u91cf\u7684\u5206\u5e03\uff1a\uff08\\(K^2-1\\)\u4e2a\u53c2\u6570\uff09 </p> <p></p> <p>From a graphical perspective, we have reduced the number of parameters by dropping links in the graph, at the expense of having a restricted class of distributions.</p> <p>An alternative way to reduce the number of independent parameters in a model is by sharing parameters (also known as tying of parameters).</p> <p>Another way of controlling the exponential growth in the number of parameters in models of discrete variables is to use parameterized models for the conditional distributions instead of complete tables of conditional probability values.   The motivation for the logistic sigmoid representation was discussed in Section 4.2.</p>"},{"location":"PRML/chap8/chap8/#814-linear-gaussian-models","title":"8.1.4 Linear-Gaussian models","text":"<p>Here we show how a multivariate Gaussian can be expressed as a directed graph corresponding to a linear-Gaussian model over the component variables. </p> <p>\u8fd9\u91cc\u8981\u7528\u5230\u7b2c\u4e8c\u7ae0\u7684\u77e5\u8bc6\uff1a\"if two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian\"</p> <p>\u8fd9\u91cc\u662f\u53cd\u8fc7\u6765\u7528\u7684\uff0c\u5982\u679c\u4e00\u4e2a\u8282\u70b9\u662f\u9ad8\u65af\uff0c\u800c\u5b83\u7684\u5747\u503c\u662f\u7236\u8282\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u90a3\u4e48\u5b83\u548c\u7236\u8282\u70b9\u7684 joint distribution \\(p(\\mathrm{x})\\) is a multivariate Gaussian.</p> <p></p> <p> </p> <p>\u8003\u8651\u4e24\u4e2a\u6781\u7aef\u60c5\u51b5\uff1a</p> <p>\u5f53graph\u4e2d\u6ca1\u6709\u8fde\u63a5\u65f6\uff0cthere are no parameters \\(w_{ij}\\) and so there are just D parameters \\(b_i\\) and D parameters \\(v_i\\) \u6b64\u65f6mean of p(x) is given by (b1,...,bD)T and the covariance matrix is diagonal of the form diag(v1,...,vD). \u7ed3\u679c\u5c31\u662fa set of D independent univariate Gaussian distributions</p> <p>\u5f53graph\u4e3a\u5168\u8fde\u63a5\u65f6\uff0cw\u7684\u77e9\u9635\u662f\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635\uff0c\u53c2\u6570\u4e2a\u6570\u4e3aD(D\u22121)/2. v\u6240\u5728\u7684\u534f\u65b9\u5dee\u77e9\u9635is a general symmetric covariance matrix</p> <p>\u5728\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u8fd0\u7528\u4e0a\u9762\u7684(8.15)\u548c(8.16)\uff0c\u53ef\u4ee5\u5199\u51fajoint distribution\u7684\u5206\u5e03\u4e3a\uff1a  </p> <p>(2.3.6)(Mark, \u8fd9\u6bb5\u6ca1\u770b) Note that we have already encountered a specific example of the linear-Gaussian relationship when we saw that the conjugate prior for the mean \u00b5 of a Gaussian variable x is itself a Gaussian distribution over \u00b5. The joint distribution over x and \u00b5 is therefore Gaussian. This corresponds to a simple two-node graph in which the node representing \u00b5 is the parent of the node representing x. The mean of the distribution over \u00b5 is a parameter controlling a prior, and so it can be viewed as a hyperparameter. Because the value of this hyperparameter may itself be unknown, we can again treat it from a Bayesian perspective by introducing a prior over the hyperparameter, sometimes called a hyperprior, which is again given by a Gaussian distribution. This type of construction can be extended in principle to any level and is an illustration of a hierarchical Bayesian model, of which we shall encounter further examples in later chapters.</p>"},{"location":"PRML/chap8/chap8/#82-conditional-independence","title":"8.2. Conditional Independence","text":"<p>\u5bf9\u4e8e\u53d8\u91cfa, b, c, \u5982\u679c\u6709  \u5c31\u79f0a is conditionally independent of b given c</p> <p>\u5f53\u8868\u793ajoint distribution\u65f6\uff0c\u8868\u8fbe\u5f0f\u7a0d\u6709\u4e0d\u540c\uff0c\u5e26\u5165\u4e0a\u9762\u7684\u5f0f\u5b50\u5c31\u884c\uff1a  \uff08\u672c\u8d28\u5176\u5b9e\u662fp(a, b)=p(a)p(b)\uff0c\u53ea\u4e0d\u8fc7\u591a\u4e86\u4e2acondition\uff09</p> <p>conditional independence \u8981\u6c42\u4e0a\u9762\u8fd9\u4e24\u4e2a\u5f0f\u5b50\u5bf9\u53d8\u91cf\u7684\u4efb\u610f\u53d6\u503c\u90fd\u8981\u6210\u7acb\uff0c\u800c\u4e0d\u662f\u67d0\u4e9b\u503c</p> <p>\u53ef\u4ee5\u7b80\u5199\u4e3a\uff1a </p> <p>conditional independence\u53ef\u4ee5\u4ecegraph\u4e2d\u76f4\u63a5\u8bfb\u51fa\uff0c\u4e0d\u9700\u8981\u4efb\u4f55\u8ba1\u7b97 The general framework for achieving this is called d-separation, where the \u2018d\u2019 stands for \u2018directed\u2019</p>"},{"location":"PRML/chap8/chap8/#821-three-example-graphs","title":"8.2.1 Three example graphs","text":""},{"location":"PRML/chap8/chap8/#case_1","title":"case_1","text":"<p> \u5bf9\u4e8e\u4e0a\u9762\u7684gragh\uff0c\u53ef\u4ee5\u5199\u51fa\u4e0b\u9762\u7684\u5206\u5e03\uff1a  * \u5982\u679c\u6ca1\u6709\u53d8\u91cf\u88ab\u89c2\u6d4b\uff0c\u4e3a\u4e86\u6c42p(a,b)\uff0c\u6211\u4eec\u4f1amarginalizing both sides of (8.23) with respect to c to give  \u8fd9\u4e2a\u5f97\u4e0d\u51fap(a)p(b)\uff0c\u56e0\u6b64\u5f97\u4e0d\u51faconditional independence\uff0c\u53ef\u4ee5\u8bb0\u4f5c </p> <ul> <li>\u5047\u8bbec\u88ab\u89c2\u6d4b\u4e86\uff0c\u6216\u8005\u8bf4we condition on the variable c*\uff0c \u7531\u4e8e\u672c\u8eabgraph\u90fd\u53ef\u4ee5\u5199\u6210  \u4e3a\u4e86\u6c42a, b\u7684joint\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u628a\u4e0a\u8fb9\u8fd9\u4e2a\u5f0f\u5b50\u540c\u9664p(c) </li> </ul> <p>(\u6362\u53e5\u8bdd\u8bf4\uff0c\u4ecegraph\u4e2d\u5f97\u4e0d\u5230p(a,b)=p(a)p(b)\uff0c\u4f46\u662f\u6709p(a,b|c)=p(a|c)p(b|c))</p> <p>\u603b\u7ed3\uff1a The node c is said to be tail-to-tail with respect to this path because the node is connected to the tails of the two arrows, when we condition on node c, the conditioned node \u2018blocks\u2019 the path from a to b and causes a and b to become (conditionally) independent. (\u8fd9\u91cctail-to-tail\u7684\u8fd9\u4e2ato\u5c31\u5f88\u8ff7\u60d1\uff0c\u6700\u597d\u8bb0\u6210\"and\"\uff0ctail&amp;tail, head&amp;head, head&amp;tail)</p>"},{"location":"PRML/chap8/chap8/#case_2","title":"case_2","text":"<p> \u540c\u6837\uff0c\u5148\u5199\u51fa </p> <ul> <li> <p>\u5f53\u6ca1\u6709condition c\u65f6\uff0ca\uff0cb\u6ca1\u6709\u72ec\u7acb  </p> </li> <li> <p>condition c\u4e4b\u540e\uff0c </p> </li> </ul> <p>\u603b\u7ed3\uff1a head-to-tail Such a path connects nodes a and b and renders them dependent. If we now observe c, then this observation \u2018blocks\u2019 the path from a to b and so we obtain the conditional independence property</p>"},{"location":"PRML/chap8/chap8/#case_3","title":"case_3","text":"<p> This graph has rather different properties from the two previous examples</p> <p>\u5148\u5199\u51fa </p> <ul> <li> <p>Consider first the case where none of the variables are observed. Marginalizing both sides of (8.28) over c we obtain  and so a and b are independent with no variables observed, in contrast to the two previous examples \u89c2\u6d4b\u524d\u72ec\u7acb </p> </li> <li> <p>Now suppose we condition on c,\u89c2\u6d4b\u4e4b\u540e\u4e0d\u72ec\u7acb </p> </li> </ul> <p>\u603b\u7ed3\uff1a head-to-head When node c is unobserved, it \u2018blocks\u2019 the path, and the variables a and b are independent. However, conditioning on c \u2018unblocks\u2019 the path and renders a and b dependent. \u53e6\u5916\u8fd8\u6709\u4e00\u4e2a\u7ed3\u8bba\uff1aa head-to-head path will become unblocked if either the node, or any ofits descendants, is observed\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u89c2\u6d4bc\u548c\u89c2\u6d4bc\u7684\u540e\u4ee3\u53ef\u4ee5\u8d77\u5230\u540c\u6837\u7684\u4f5c\u7528</p>"},{"location":"PRML/chap8/chap8/#822-d-separation","title":"8.2.2 D-separation","text":"<p>A, B, C\u662f\u4e09\u4e2anode\u7684\u96c6\u5408\uff0c\u6ca1\u6709\u4ea4\u96c6 </p> <p>note\uff1a parameters\u7528small filled circles\u8868\u793a\uff0c\u7406\u8bba\u4e0a\u76f8\u5f53\u4e8eobserved nodes. However, there are no marginal distributions associated with such nodes. Consequently they play no role in d-separation.</p> <p>We can view a graphical model (in this case a directed graph) as a filter in which a probability distribution p(x) is allowed through the filter if, and only if, it satisfies the directed factorization property (8.5)  We can alternatively use the graph to filter distributions according to whether they respect all of the conditional independencies implied by the d-separation properties of the graph. \u6839\u636ed-separation theorem\uff0c\u4e0a\u9762\u4e24\u79cd\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u662f\u4e00\u6837\u7684</p>"},{"location":"PRML/chap8/chap8/#markov-blanket-or-markov-boundary","title":"Markov blanket or Markov boundary","text":"<p>The set of nodes comprising the parents, the children and the co-parents is called the Markov blanket </p> <p>We can think of the Markov blanket of a node xi as being the minimal set of nodes that isolates xi from the rest of the graph.</p> <p>the conditional distribution of xi, conditioned on all the remaining variables in the graph, is dependent only on the variables in the Markov blanket.</p>"},{"location":"PRML/chap8/chap8/#83-markov-random-fields","title":"8.3. Markov Random Fields","text":""},{"location":"PRML/chap8/chap8/#831-conditional-independence-properties","title":"8.3.1 Conditional independence properties","text":"<p>We might ask whether it is possible to define an alternative graphical semantics for probability distributions such that conditional independence is determined by simple graph separation. This is indeed the case and corresponds to undirected graphical models. \u4e5f\u5c31\u662f\u8bf4undirected graph\u4e0d\u9700\u8981\u590d\u6742\u7684separation\uff0c\u53ea\u9700\u8981\u5728graph\u4e0a\u76f4\u63a5\u5206\u5272</p> <p>If all such paths pass through one or more nodes in set C, then all such paths are \u2018blocked\u2019 and so the conditional independence property holds.  However, if there is at least one such path that is not blocked, then the property does not necessarily hold, or more precisely there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation. </p> <p></p> <p>The Markov blanket for an undirected graph takes a particularly simple form, because a node will be conditionally independent of all other nodes conditioned only on the neighbouring nodes </p>"},{"location":"PRML/chap8/chap8/#832-factorization-properties","title":"8.3.2 Factorization properties","text":"<p>\u6211\u4eec\u60f3expressing the joint distribution p(x) as a product of functions defined over sets of variables that are local to the graph.</p> <p>If we consider two nodes xi and xj that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph. \u4e5f\u5c31\u662f\u8bf4\u76f8\u90bb\u8282\u70b9\u4e00\u5b9a\u76f8\u5173\uff0c\u4e5f\u5c31\u62c6\u4e0d\u5f00\u4e86\uff0c\u5199\u4e0d\u6210\u4e58\u79ef\u5f62\u5f0f </p> <p>clique\uff1athe set of nodes in a clique is fully connected</p> <p> \u8fd9\u91cc\u63d0\u5230\u4e86\u4e24\u4e2a\u51fd\u6570\uff0cpotential functions\u548cpartition function\uff0cpartition function\u53ea\u662f\u7528\u4e8enormalization\u7684</p> <p> Note that we do not restrict the choice of potential functions to those that have a specific probabilistic interpretation as marginal or conditional distributions.  \u4e0e\u6709\u5411\u56fe\u4e0d\u540c\uff0c\u6709\u5411\u56fe\u4e2d\uff0cp(x)\u62c6\u6210\u8bb8\u591a\u6761\u4ef6\u6982\u7387\u7684\u4e58\u79ef \u5728\u65e0\u5411\u56fe\u4e2d\uff0c\u4e0d\u8981\u6c42potential functions\u5fc5\u987b\u662fmarginal or conditional distributions</p> <p>The presence of this normalization constant is one of the major limitations of undirected graphs. If we have a model with M discrete nodes each having K states, \u5219partition function\u662f\\(K^M\\)\u9879\u6c42\u548c</p> <p>\u5982\u679c\u6211\u4eec\u60f3\u8981\u5f97\u5230connection between conditional independence and factorization for undirected graphs\uff0c\u5c31\u8981\u7ea6\u675f\\(\\Psi_C(x_C)\\) are strictly positive</p> <p>\u7531\u4e8epotential functions\u4e25\u683c\u5927\u4e8e0\uff0c\u56e0\u6b64\u53ef\u4ee5\u5199\u6210\u6307\u6570\u7684\u5f62\u5f0f  where E(xC) is called an energy function, and the exponential representation is called the Boltzmann distribution The joint distribution\u662fpotential\u7684\u4e58\u79ef\uff0cso the total energy is obtained by adding the energies of each of the maximal cliques.</p>"},{"location":"PRML/chap8/chap8/#834-relation-to-directed-graphs","title":"8.3.4 Relation to directed graphs","text":""},{"location":"PRML/chap8/chap8/#directed-undirected","title":"directed -&gt; undirected","text":"<p>\u5148\u8003\u8651\u6700\u7b80\u5355\u7684directed chain\uff0c\u5728chain\u4e2d\uff0cmaximal cliques are simply the pairs of neighbouring nodes \u8ba9\u8fd9\u4e24\u4e2a\u5f0f\u5b50\u5bf9\u5e94   \u5c31\u6709 </p> <p>\u4e0b\u9762\u8003\u8651general\u7684\u60c5\u51b5\uff1a \u8981\u60f3\u8f6c\u5316\uff0c\u5c31\u8981\u7528clique\u7684potential\u8868\u793aconditional distributions\u3002 In order for this to be valid, we must ensure that the set of variables that appears in each of the conditional distributions is a member of at least one clique of the undirected graph \u4e5f\u5c31\u662f\u8bf4\uff0c\u8981\u60f3\u8fbe\u6210\u8fd9\u4e2a\uff0c\u6709\u5411\u56fe\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\uff0c\u90fd\u8981\u81f3\u5c11\u51fa\u73b0\u5728\u67d0\u4e00\u4e2aminimal clique\u4e2d\uff0c\u4e0d\u7136\u5c31\u4f1a\u628a\u8fd9\u4e2a\u8282\u70b9\u6f0f\u6389</p> <p>\u5982\u679c\u6709\u5411\u56fe\u4e2d\u7684\u5b50\u8282\u70b9\u53ea\u6709\u4e00\u4e2aparent\uff0c\u90a3\u4e48\u53ea\u9700\u8981\u628a\u7bad\u5934\u76f4\u63a5\u53d8\u6210edge\uff0c\u7136\u540e\u628anode pair\u89c6\u4e3aminimal clique \u4f46\u662f\u5f53\u4e00\u4e2a\u5b50\u8282\u70b9\u6709\u591a\u4e2aparent\u65f6\uff0c\u4e5f\u5c31\u662f\"head to head\"\u65f6\uff0c\u4e0d\u80fd\u76f4\u63a5\u8f6c\u5316\u3002To ensure this, we add extra links between all pairs of parents of the node Anachronistically, this process of \u2018marrying the parents\u2019 has become known as moralization, and the resulting undirected graph, after dropping the arrows, is called the moral graph \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728moralization\u7684\u8fc7\u7a0b\u4e2d\uff0c\u54df\u53ef\u80fd\u4f1a\u4e22\u5230conditiona independence\uff0c\u6bd4\u5982\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\uff1a </p> <p>Steps: 1. add additional undirected links between all pairs of parents for each node in the graph and then drop the arrows on the original links to give the moral graph 2. initialize all of the clique potentials of the moral graph to 1 3. take each conditional distribution factor in the original directed graph and multiply it into one of the clique potentials 4. in all cases the partition function is given by Z=1</p>"},{"location":"PRML/chap8/chap8/#undirected-directed","title":"undirected -&gt; directed","text":"<p>Converting from an undirected to a directed representation is much less common and in general presents problems due to the normalization constraints</p>"},{"location":"PRML/chap8/chap8/#difference","title":"difference","text":"<p>It turns out that the two types of graph can express different conditional independence properties</p>"},{"location":"PRML/chap8/chap8/#84-inference-in-graphical-models","title":"8.4. Inference in Graphical Models","text":""},{"location":"PRML/chap8/chap8/#841-inference-on-a-chain","title":"8.4.1 Inference on a chain","text":"<p>\u6211\u4eec\u8fd9\u91cc\u53ea\u9700\u8981\u8ba8\u8bba\u65e0\u5411\u56fe\u3002\u56e0\u4e3a\u6709\u5411\u56fe\u53ef\u4ee5\u76f4\u63a5\u8f6c\u5316\u4e3a\u65e0\u5411\u56fe\u3002We have already seen that the directed chain can be transformed into an equivalent undirected chain. Because the directed graph does not have any nodes with more than one parent, this does not require the addition of any extra links, and the directed and undirected versions of this graph express exactly the same set of conditional independence statements.</p> <p></p> <p>\u4e0a\u8fb9\u7684chain\u53ef\u4ee5\u5199\u6210\u4e0b\u9762\u7684potential\uff0c\u540c\u65f6\u6211\u4eec\u5047\u8bbe\u53d8\u91cf\u90fd\u662f\u79bb\u6563\u7684\uff0c\u6bcf\u4e2a\u53d8\u91cf\u6709K\u4e2a\u72b6\u6001 </p> <p>Let us consider the inference problem of finding the marginal distribution \\(p(x_n)\\) for a specific node \\(x_n\\) that is part way along the chain.</p> <p>\u4e3a\u4e86\u907f\u514d\u6307\u6570\u7ea7\u7684\u6c42\u548c\u8fd0\u7b97\uff0c exploiting the conditional independence properties of the graphical model</p> <p>\u601d\u60f3\uff1a \u628a\u4ee3\u5165\uff0c\u7136\u540e\u89c2\u5bdf</p> <p>\u5148\u770bthe summation over\\(x_N\\), \u6211\u4eec\u77e5\u9053\\(x_N\\)\u53ea\u4e0e\\(x_{N-1}\\)\u76f8\u5173\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\\(x_N\\)\u4e0e\\(x_{N-1}\\)\u7684potential\u5f97\u5230\u4e8c\u8005\u7684joint\uff0c\u518d\u5728\\(x_N\\)\u4e0a\u6c42\u548c\uff0c\u5c31\u80fd\u5f97\u5230\\(x_{N-1}\\)\u7684\u5206\u5e03 </p> <p>\u4ee5\u6b64\u7c7b\u63a8\uff0cBecause each summation effectively removes a variable from the distribution, this can be viewed as the removal of a node from the graph.</p> <p>we can express the desired marginal in the form  \u8fd9\u4e2a\u5f0f\u5b50\u6ca1\u6709\u589e\u52a0\u4ec0\u4e48\u65b0\u4e1c\u897f\uff0c\u53ea\u662f\u628a(8.50)\u91cd\u65b0\u6392\u5e8f \u8fd9\u4e2a\u5f0f\u5b50\u7684\u590d\u6742\u5ea6\u4e3a\\(O(NK^2)\\)</p>"},{"location":"PRML/chap8/chap8/#passing-of-local-messages","title":"passing of local messages","text":"<p>We now give a powerful interpretation of this calculation in terms of the passing of local messages around on the graph.</p> <p>marginal \\(x_N\\) \u53ef\u4ee5\u5206\u89e3\u4e3a\u4e24\u9879\u4e4b\u79ef\u4ee5\u53canormalization term </p> <p>We shall interpret \\(\u00b5_\u03b1(x_n)\\) as a message passed forwards along the chain from node \\(x_{n\u22121}\\) to node \\(x_n\\).  Similarly, \\(\u00b5_\u03b2(x_n)\\) can be viewed as a message passed backwards along the chain to node \\(x_n\\) from node \\(x_{n+1}\\).  \u6ce8\u610f\\(\\mu\\)\u53ea\u4ee3\u8868\u76f8\u90bb\u4e00\u9879\u4f20\u8fc7\u6765\u7684\u4fe1\u606f Note that each of the messages comprises a set of K values, one for each choice of xn, and so the product of two messages should be interpreted as the point-wise multiplication of the elements of the two messages to give another set of K values.</p> <p>\u56e0\u6b64\u53ef\u4ee5\u5f97\u51fa\u9012\u5f52\u5f0f\uff1a   \u56e0\u6b64\u56e0\u4e3a\u5934\u548c\u5c3e\u6ca1\u6709\u03bc\uff0c\u56e0\u6b64\u53ef\u4ee5\u9012\u5f52\u8ba1\u7b97\uff0c\u5982\uff1a   \u6ce8\u610f\u811a\u6807\uff1aThe outgoing message \\(\u00b5_\u03b1(x_n)\\) in (8.55) is obtained by multiplying the incoming message \\(\u00b5_\u03b1(x_{n-1})\\) by the local potential involving the node variable and the outgoing variable and then summing over the node variable.</p> <p>\u8fd9\u4e2a\u56fe\u53eb\u505aMarkov chains\uff0cand the corresponding message passing equations represent an example of the ChapmanKolmogorov equations for Markov processes</p> <p>\u5982\u679c\u60f3\u5f97\u5230chain\u4e0a\u6bcf\u4e2a\u53d8\u91cf\u7684marginal\uff0c\u5c31\u53ef\u4ee5\u5148\u4ece\\(x_N\\)\u5f00\u59cb\u6c42\u51fa\u6240\u6709\u7684\\(\u00b5_\\beta(x_i)\\)\uff0c\u4ece\u4ece\\(x_1\\)\u5f00\u59cb\u6c42\u51fa\u6240\u6709\u7684\\(\u00b5_\\alpha(x_i)\\)\u3002\u628a\u6240\u6709\u7684\u7ed3\u679c\u5b58\u8d77\u6765\uff0c\u7136\u540e\u518d\u7b97marginal\u3002</p>"},{"location":"PRML/chap8/chap8/#note-observation","title":"Note: observation","text":"<p>\u5982\u679c\u6709\u67d0\u4e9b\u53d8\u91cf\u88ab\u89c2\u6d4b\uff1aIf some of the nodes in the graph are observed, then the corresponding variables are simply clamped to their observed values and there is no summation. \u6ce8\u610f\u8fd9\u4e2a\u6307\u793a\u51fd\u6570\\(I\\)\u662f\u548csummation\u540c\u65f6\u51fa\u73b0\u7684\uff0c\u4e3e\u4e2a\u4f8b\u5b50\uff1a\u6211\u4eec\u8981\u7b97\\(\\sum_{x_1}\\sum_{x2}f(x_1,x_2)\\)\uff0c\u7136\u540e\u6211\u4eec\u89c2\u6d4b\u4e86\\(x_2\\)\u4e3a\\(\\hat{x_2}\\) \u76f4\u63a5\u4ee3\u5165\u7684\u8bdd\u4e3a\\(\\sum_{x_1}f(x_1,\\hat{x_2})\\), \u5982\u679c\u5199\u6210summation+I\u7684\u8bdd\uff0c\u5c31\u662f\\(\\sum_{x_1}\\sum_{x2}f(x_1,x_2)\\cdot I(x_2,\\hat{x_2})\\), \u6b64\u65f6\u628asummation\\(\\sum_{x2}\\)\u5c55\u5f00\uff0c\u6b64\u65f6\u53ea\u6709\\(x_2=\\hat{x_2}\\)\u7684\u9879\u76ee\u4f1a\u4e581\uff0c\u5176\u4ed6\u9879\u4f1a\u4e580.\u7136\u540e\u518d\u628a\u6bcf\u4e00\u9879\u76f8\u52a0\uff0c\u6700\u540e\u5f97\u5230\u7684\u7ed3\u679c\u5c31\u662f\\(\\sum_{x_1}f(x_1,\\hat{x_2})\\)\uff0c\u548c\u76f4\u63a5\u4ee3\u5165\u7684\u7ed3\u679c\u76f8\u540c</p> <p></p> <p>\u76f8\u90bb\u4e24\u4e2a\u53d8\u91cf\u7684joint distribution\uff1a\\(p(x_{n-1}, x_n)\\) This is similar to the evaluation of the marginal for a single node, except that there are now two variables that are not summed out.  </p>"},{"location":"PRML/chap8/chap8/#842-trees","title":"8.4.2 Trees","text":"<ul> <li> <p>In the case of an undirected graph, a tree is defined as: a graph in which there is one, and only one, path between any pair of nodes. Such graphs therefore do not have loops. </p> </li> <li> <p>In the case of directed graphs, a tree is defined such that: there is a single node, called the root, which has no parents, and all other nodes have one parent</p> </li> </ul> <p></p> <p>the moralization step will not add any links as all nodes have at most one parent, and as a consequence the corresponding moralized graph will be an undirected tree</p> <p>If there are nodes in a directed graph that have more than one parent, but there is still only one path (ignoring the direction of the arrows) between any two nodes,  then the graph is a called a polytree,</p> <p>Such a graph will have more than one node with the property of having no parents, and furthermore, the corresponding moralized undirected graph will have loops.</p>"},{"location":"PRML/chap8/chap8/#843-factor-graphs","title":"8.4.3 Factor graphs","text":"<p>sum-product algorithm \u9002\u7528\u4e8eundirected and directed trees and to polytrees \u6211\u4eec\u4e5f\u53ef\u4ee5\u628a\u5b83\u4eec\u7edf\u4e00\u6210\u4e00\u79cd\u7ed3\u6784\uff1afactor graph</p> <p>\u6709\u5411\u56fe\u548c\u65e0\u5411\u56fe\u90fd\u53ef\u4ee5\u628a\u6240\u6709\u53d8\u91cf\u7684\u603bjoint\u5206\u89e3\u4e3afunction of subset\u7684\u4e58\u79ef\u3002 Factor graphs make this decomposition explicit by introducing additional nodes for the factors themselves in addition to the nodes representing the variables.</p> <p></p> <p>\u8fd9\u90e8\u5206\u6211\u4eec\u7528\\(\\mathrm{x_s}\\)\u8868\u793a\u53d8\u91cf\u7684\u5b50\u96c6\uff0c\u7528\\(x_i\\)\u8868\u793a\u5355\u4e00\u53d8\u91cf\uff08\u548c\u4ee5\u524d\u4e0d\u540c\uff0c\u4ee5\u524d\\(x_i\\)\u4e5f\u53ef\u4ee5\u8868\u793a\u53d8\u91cf\u7684\u96c6\u5408\uff09</p> <p>\u5bf9\u4e8e\u6709\u5411\u56fe\uff0c\\(f_s(\\mathrm{x_s})\\) are local conditional distributions.  \u5bf9\u4e8e\u65e0\u5411\u56fe\uff0c\\(f_s(\\mathrm{x_s})\\) are potential functions over the maximal cliques (the normalizing coefficient 1/Z can be viewed as a factor defined over the empty set of variables)</p> <p>\u5177\u4f53\u753b\u6cd5\uff1a * there is a node (depicted as usual by a circle) for every variable in the distribution, as was the case for directed and undirected graphs.  * There are also additional nodes (depicted by small squares) for each factor fs(xs) in the joint distribution.  * Finally, there are undirected links connecting each factor node to all of the variables nodes on which that factor depends.</p> <p>\u6bd4\u5982\u4e0b\u9762\u7684\u5206\u5e03\u53ef\u4ee5\u753b\u6210\u8fd9\u6837\uff1a  </p> <p>\u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\\(f_1\\)\u548c\\(f_2\\)\u5176\u5b9e\u53ef\u4ee5\u5408\u5e76\u6210potential\uff0c\\(f_3\\)\u548c\\(f_4\\)\u4e5f\u53ef\u4ee5\u3002\u4f46\u662f\u5728factor graph\u4e2dkeeps such factors explicit and so is able to convey more detailed information about the underlying factorization.</p> <p>Factor graphs are said to be bipartite(\u4e8c\u5206\u56fe) because they consist of two distinct kinds of nodes, and all links go between nodes of opposite type. </p> <p>\u5bf9\u4e8e\u65e0\u5411\u56fe\uff0c\u6211\u4eec\u5bf9\u6bcf\u4e2amaximum clique\u5efa\u7acb\u4e00\u4e2afactor node\uff0c\u5e76\u628af\u5c31\u8bbe\u4e3aclique potentials\u3002\u540c\u4e00\u4e2a\u56fe\u53ef\u80fd\u6709\u4e0d\u540c\u7684factor graph\uff1a </p> <p>\u5bf9\u4e8e\u6709\u5411\u56fe\uff0ccreate factor nodes corresponding to the conditional distributions, and then finally add the appropriate links.\u540c\u4e00\u4e2a\u56fe\u53ef\u80fd\u6709\u4e0d\u540c\u7684factor graph </p> <p>\u5982\u679c\u6211\u4eec\u5bf9directed tree\u6216undirected tree \u505amoralize\uff0c\u7ed3\u679c\u4ecd\u7136\u662ftree(in other words, the factor graph will have no loops, and there will be one and only one path connecting any two nodes)</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5bf9\u4e8edirected polytree\uff0c\u6211\u4eec\u5728\u8f6c\u5316\u4e3aundirected graph\u65f6\uff0c\u662f\u6709loop\u7684\u3002\u4f46\u662f\u5728moralization\u4e2d, conversion to a factor graph again results in a tree.   In fact, local cycles in a directed graph due to links connecting parents of a node can be removed on conversion to a factor graph by defining the appropriate factor function. \u6b64\u5916\uff0c\u6709\u5411\u56fe\u4e2d\"head to head\"\u4ea7\u751f\u7684local cycles\uff0c\u5982\u679c\u9009\u62e9\u5408\u9002\u7684factor function\uff0c\u5c31\u80fd\u5728factor graph\u4e2d\u53bb\u6389\uff1a </p>"},{"location":"PRML/chap8/chap8/#844-the-sum-product-algorithm","title":"8.4.4 The sum-product algorithm","text":"<p>\"evaluating local marginals over nodes or subsets of nodes\"</p> <p>\u5728\u672c\u7ae0\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u53d8\u91cf\u90fd\u662f\u79bb\u6563\u7684\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u7528sum\u6765\u505amarginalize\u3002\u4f46\u4e8b\u5b9e\u4e0asum-product algorithm\u5bf9\u4e8elinear-Gaussian models\u4e5f\u9002\u7528</p> <p>\u5728directed graph\u4e2d\u8fd8\u6709\u4e00\u4e2a\u7528\u4e8eexact inference\u7684\u7b97\u6cd5\uff0cbelief propagation\uff0c\u53ef\u4ee5\u770b\u4f5csum-product algorithm\u7684\u4e00\u4e2aspecial case\u3002 Here we shall consider only the sum-product algorithm because it is simpler to derive and to apply, as well as being more general.</p> <p>\u6211\u4eec\u5047\u8bbe\u539f\u672c\u7684graph\u662fan undirected tree or a directed tree or polytree\uff0cso that the corresponding factor graph has a tree structure</p> <p>Our goal\uff1a 1. to obtain an efficient, exact inference algorithm for finding marginals 2. in situations where several marginals are required to allow computations to be shared efficiently</p>"},{"location":"PRML/chap8/chap8/#finding-the-marginal-px-for-particular-variable-node-x","title":"finding the marginal p(x) for particular variable node x","text":"<p>\u76ee\u524d\uff0c\u6211\u4eec\u5047\u8bbe\u6240\u6709\u53d8\u91cf\u90fd\u662fhidden\u7684\u3002</p> <p>By definition, the marginal is obtained by summing the joint distribution over all variables except x so that </p> <p>Idea: to substitute for p(x) using the factor graph expression (8.59)   and then interchange summations and products in order to obtain an efficient algorithm</p> <p>\u56de\u60f3\u5230bipartite\u7684\u6027\u8d28\uff0c\u4e0ex\u76f8\u8fde\u7684\u4e00\u5b9a\u662ff\u8282\u70b9 \u518d\u56de\u60f3\u5230tree\u7684\u6027\u8d28\uff0c\u4efb\u610f\u4e24\u4e2a\u8282\u70b9\u4e4b\u95f4\u53ea\u6709\u4e00\u4e2apath</p> <p>\u56e0\u6b64\u628a\u4e0ex\u76f8\u8fde\u7684\u8282\u70b9\u4ecex\u4e0ef\u7684\u8fde\u63a5\u5904\u65ad\u5f00\uff0c\u5206\u6210\u82e5\u5e72\u4e2agroup\uff0c\u5404\u4e2agroup\u5e76\u4e0d\u76f8\u8fde </p> <p>\u56de\u60f3factor graph\u7684\u5b9a\u4e49\uff0cjoint\u53ef\u4ee5\u5199\u6210product of functions of subset </p> <p></p> <p>ne(x) denotes the set of factor nodes that are neighbours of x, and Xs denotes the set of all variables in the subtree connected to the variable node x via the factor node fs, and Fs(x,Xs) represents the product of all the factors in the group associated with factor fs.</p> <p>\u5404\u4e2agroup\u5e76\u4e0d\u76f8\u8fde\uff0c\u56e0\u6b64\u53ef\u4ee5\u5199\u6210\u5404\u4e2agroup\u7684\u4e58\u79ef</p> <p>\u4ee3\u5165\u5230p(x)\u7684\u8868\u8fbe\u5f0f\u4e2d\uff0c\u5229\u7528\u4e58\u6cd5\u5206\u914d\u5f8b\u4ea4\u6362\u4e58\u6cd5\u4e0e\u52a0\u6cd5  functions of each group \u7684\u4e58\u79ef\uff0c\u5bf9\u4e8e\u6240\u6709\u5176\u4ed6\u53d8\u91cf\u6c42sum -&gt; \u5bf9\u4e8e\u6bcf\u4e2afunction of each group\uff0c\u5bf9\u4e8e\u6240\u6709\u5176\u4ed6\u53d8\u91cf\u6c42sum\uff0c\u7136\u540e\u518d\u4e58\u8d77\u6765 (\u800c\u5728\u56e0\u4e3agroup\u5f7c\u6b64\u4e0d\u76f8\u8fde\uff0c\u5728\\(group_i\\)\u4e0a\uff0c\u5bf9\u4e8e\u6240\u6709\u53d8\u91cfsum\u7b49\u4e8e\u5bf9\\(group_i\\)\u5185\u90e8\u7684\u53d8\u91cfsum\uff0c\u4e5f\u5c31\u662f\\(X_s\\)\uff0c\u6240\u4ee5\u4e0a\u56fe\u4e2d\u7684summation\u53ef\u4ee5\u5199\u6210\\(X_s\\))</p> <p>\u5c06group\u5185\u90e8\u7684summation\u5b9a\u4e49\u4e3amessages from the factor nodes \\(f_s\\) to the variable node x  We see that the required marginal p(x) is given by the product of all the incoming messages arriving at node x. marginal p(x)\u5c31\u662f\u6240\u6709\u4f20\u5230node x\u7684message\u7684\u4e58\u79ef</p> <p>\\(F_s(x,X_s)\\) is described by a factor (sub-)graph and so can itself be factorized. In particular, we can write  where, for convenience, we have denoted the variables associated with factor \\(f_s\\),in addition to x,by \\(x_1,...,x_M\\)\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u5199\u6210\\(\\mathrm{x}_s\\) \uff08\u56e0\u4e3a\\(F_s(x,X_s)\\)\u4e5f\u662f\u4e00\u4e2afactor (sub-)graph\uff0c\u56e0\u6b64\u4e5f\u53ef\u4ee5\u5199\u6210product of functions of subset\u7684\u5f62\u5f0f\uff0c\u4e0a\u9762\u7684\u5f0f\u5b50\u53ea\u662f\u4e00\u79cd\u6bd4\u8f83\u6709\u7528\u7684\u5f62\u5f0f\uff09</p> <p></p> <p> \u6ce8\u610f\u8fd9\u91cc\u7684\\(f_s(...)\\)\u5c31\u662f\u5404\u4e2af\u8282\u70b9\u6240\u5bf9\u5e94\u7684\u51fd\u6570 \u89c2\u5bdf\u5230\u4e0a\u9762\u7684\u8fd9\u4e2a  \u53c8\u662f\u4e00\u4e2asummation on group\uff0c\u56e0\u6b64\u8fd8\u662f\u53ef\u4ee5\u5199\u6210message\u7684\u5f62\u5f0f\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u6b21\u7531(\u56fe8.47)\u53ef\u4ee5\u770b\u51fa\uff0cmessage\u662f\u7531x node\u6d41\u5411f node\u7684\u3002  \u8fd8\u662f\u7531\u4e8etree\u7684\u6027\u8d28\uff0c(\u56fe8.47)\u4e2d\u7684\u6bcf\u4e2agroup\u4ecd\u7136\u662f\u4e92\u4e0d\u76f8\u8fde\u7684</p> <p>\u7136\u540e\u518d\u5bf9\\(G_m(x_m,X_{sm})\\)\u5206\u89e3\uff0c  where the product is taken over all neighbours of node xm except for node \\(f_s\\) </p> <p>\u540c\u65f6\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\\(F_l(x_m,X_{ml})\\)\u548c\u6700\u521d\u6c42p(x)\u7684\u56e0\u5b50\u662f\u4e00\u6837\u7684 Note that each of the factors \\(F_l(x_m,X_{ml})\\) represents a subtree of the original graph of precisely the same kind as introduced in (8.62). </p> <p>Substituting (8.68) into (8.67), we then obtain  where we have used the definition (8.64) of the messages passed from factor nodes to variable nodes.</p> <p>Thus to evaluate the message sent by a variable node to an adjacent factor node along the connecting link, we simply take the product of the incoming messages along all of the other links.</p> <p>Note: * any variable node that has only two neighbours performs no computation but simply passes messages through unchanged * a variable node can send a message to a factor node once it has received incoming messages from all other neighbouring factor nodes</p> <p>Each of these messages can be computed recursively in terms of other messages. In order to start this recursion, we can view the node x as the root of the tree and begin at the leaf nodes.</p> <p>\u73b0\u5728\u5c31\u6765\u8003\u8651leaf nodes\u7684\u8ba1\u7b97\uff0c * if a leaf node is a variable node, then the message that it sends along its one and only link is given by          \u56e0\u4e3a\u6b64\u65f6F\u4ee3\u8868\u5728\u4e00\u4e2aempty\u7684group\u4e0a\u9762summation\uff0c\u7ed3\u679c\u4e3a1      * if the leaf node is a factor node, we see from (8.66) that the message sent should take the form          \u56e0\u4e3a\u6b64\u65f6G\u4e5f\u662f\u5728\u4e00\u4e2aempty\u7684group\u4e0a\u9762summation\uff0c\u7ed3\u679c\u4e3a1\uff0c\u800c\u524d\u8fb9\u5bf9\u5404\u79cdx\u8fdb\u884csummation\u4e4b\u540e\uff0c\u5c31\u53d8\u6210\u4e86f(x)     </p> <p></p> <p>\u603b\u7ed3\uff1a 1. viewing the variable node x as the root of the factor graph and initiating messages at the leaves of the graph using (8.70) and (8.71)       1. The message passing steps (8.66) and (8.69) are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours       1. Once the root node has received messages from all of its neighbours, the required marginal can be evaluated using (8.63).     </p>"},{"location":"PRML/chap8/chap8/#find-the-marginals-for-every-variable-node","title":"find the marginals for every variable node","text":"<p>We can obtain a much more efficient procedure by \u2018overlaying\u2019 these multiple message passing algorithms to obtain the general sum-product algorithm as follows</p> <ol> <li>\u4efb\u610f\u9009\u5b9a\u4e00\u4e2anode\u4f5c\u4e3aroot</li> <li>\u4eceleaf\u628amessage\u4f20\u5230root</li> <li>root\u53ef\u4ee5\u4ece\u6240\u6709\u7684\u90bb\u5c45\u90a3\u91cc\u6536\u5230message\uff0c\u4e5f\u5c31\u80fd\u628a\u4fe1\u606f\u518d\u53d1\u7ed9\u6bcf\u4e2a\u90bb\u5c45</li> </ol> <p>By now, a message will have passed in both directions across every link in the graph</p>"},{"location":"PRML/chap8/chap8/#find-the-marginal-distributions-pmathrmx_s-associated-with-the-sets-of-variables-belonging-to-each-of-the-factors","title":"find the marginal distributions \\(p(\\mathrm{x_s})\\) associated with the sets of variables belonging to each of the factors","text":"<p>it is easy to see that the marginal associated with a factor is given by the product of messages arriving at the factor node and the local factor at that node  in complete analogy with the marginals at the variable nodes</p> <p>\u56de\u60f3(8.66)\uff0c\u8fd9\u4e24\u8005\u90fd\u662f\u8981\u6c42\\(f_s\\)\u8282\u70b9\u6240\u4ee3\u8868\u7684\"message\".\u53ea\u4e0d\u8fc7(8.66)\u4e25\u683c\u610f\u4e49\u4e0a\u7684message\u9700\u8981\u5bf9\\(\\mathrm{x_s}\\)summation\uff0c\u800c\u6211\u4eec\u60f3\u6c42\\(p(\\mathrm{x_s})\\)\uff0c\u5f53\u7136\u5c31\u4e0d\u7528\u6c42\u548c\u4e86 </p> <p>If the factors are parameterized functions and we wish to learn the values of the parameters using the EM algorithm, then these marginals are precisely the quantities we will need to calculate in the E step, as we shall see in detail when we discuss the hidden Markov model in Chapter 13.</p>"},{"location":"PRML/chap8/chap8/#a-different-view","title":"a different view","text":"<p>\u56e0\u4e3a\u4ecevariable node\u6307\u5411factor node\uff0c\u5c31\u53ea\u662f\u5355\u7eaf\u7684\u76f8\u4e58 \u56e0\u6b64\u53ef\u4ee5\u5ffd\u89c6\u6389\u6240\u6709\u7684variable node\uff0c\u5c06max-product\u53ea\u770b\u4f5c\u662f\u4e00\u4e2afactor node\u4e4b\u95f4\u4f20\u9012message\u7684\u8fc7\u7a0b The sum-product algorithm can be viewed purely in terms of messages sent out by factor nodes to other factor nodes.</p> <p></p>"},{"location":"PRML/chap8/chap8/#normalization","title":"normalization","text":"<p>\u5982\u679cfactor graph\u7531\u6709\u5411\u56fe\u8f6c\u6362\u800c\u6765\uff0cthe joint distribution is already correctly normalized \u5982\u679cfactor graph\u7531\u65e0\u5411\u56fe\u8f6c\u6362\u800c\u6765\uff0cin general there will be an unknown normalization coefficient 1/Z. </p> <p>As with the simple chain example of Figure 8.38, this is easily handled by working with an unnormalized version \\(\\widetilde{p}(x)\\) of the joint distribution, where \\(p(x) = \\widetilde{p}(x)/Z\\)  We first run the sum-product algorithm to find the corresponding unnormalized marginals \\(p(x_i)\\). The coefficient \\(1/Z\\) is then easily obtained by normalizing any one of these marginals, and this is computationally efficient because the normalization is done over a single variable rather than over the entire set of variables as would be required to normalize \\(p(x)\\) directly. \u4e5f\u5c31\u662f\uff0c\u5bf9\u4e8e\u65e0\u5411\u56fe\uff0c\u5728marginalize\u4e4b\u524d\u7684\u6240\u6709\u8ba1\u7b97\u4e2d\uff0cjoint\u90fd\u53ef\u4ee5\u4e0d\u7528normalize\u3002\u7b49\u5230\u6700\u7ec8\u6c42\u51famargin\u4e4b\u540e\uff0c\u5bf9\u4e8e\u8fd9\u4e00\u4e2a\u53d8\u91cf\u6c42\u548c\u5f97\u51faZ\u5c31\u53ef\u4ee5\u4e86</p>"},{"location":"PRML/chap8/chap8/#example","title":"example","text":"<p> \u8fd9\u91cc\u8fd9\u4e2agraph\u5e76\u4e0d\u4e00\u5b9a\u662f\u4ece\u6709\u5411\u56fe\u8fd8\u662f\u65e0\u5411\u56fe\u5f97\u6765\uff0c\u56e0\u6b64\u4e0d\u4e00\u5b9anormalize \u8fd9\u4e2agraph\u7684unnormalized joint distribution\u4e3a\uff1a </p> <p> </p> <p>\u6b64\u65f6\uff0c\u6bcf\u4e2alink\u4e0a\u9762\u4e24\u4e2a\u65b9\u5411\u7684message\u5c31\u90fd\u7b97\u51fa\u6765\u4e86 \u6700\u540e\uff0c\u6839\u636e  \u6c42\u51fa\\(x_2\\)\u7684unnormalized margin\uff1a  \u5728\u6b64\u4e4b\u4e0a\u5bf9\\(x_2\\)\u6c42\u548c\uff0c\u5c31\u80fd\u6c42\u51fanormalization term Z</p>"},{"location":"PRML/chap8/chap8/#observed-variable","title":"observed variable","text":"<p>In most practical applications, a subset of the variables will be observed, and we wish to calculate posterior distributions conditioned on these observations.</p> <p>\u8fd9\u6bb5\u8981\u7c7b\u6bd4\u4e8e8.4.1\u4e2d\u5bf9\u4e8echain\u7684observe\u770b\uff0c  \u6ce8\u610f\u6b64\u65f6\u5f97\u51fa\u7684\u7ed3\u679c\u662funnormalized\u7684</p>"},{"location":"PRML/chap8/chap8/#845-the-max-sum-algorithm","title":"8.4.5 The max-sum algorithm","text":"<p>sum-product algorithm\u7528\u4e8etake a joint distribution \\(p(\\mathrm{x})\\) expressed as a factor graph and efficiently find marginals over the component variables \u4e00\u822c\u6211\u4eec\u8fd8\u6709\u4e24\u4e2a\u5e38\u89c1\u7684\u4efb\u52a1\uff1a * find a setting of the variables that has the largest probability * find the value of that probability</p> <p>\u8fd9\u4e24\u4e2a\u5de5\u4f5c\u53ef\u4ee5\u7528max-sum, which can be viewed as an application of dynamic programming in the context of graphical models</p> <p>\u6700\u7b80\u5355\u7684\u601d\u8def\u5c31\u662f\uff0c\u5bf9\u6bcf\u4e2a\u53d8\u91cfx\u90fd\u8dd1\u4e00\u904dmax-product\uff0c\u7136\u540e\u5728\u5404\u81ea\u7684marginal\u4e0a\u8fb9\u627e\u5230\u6700\u5927\u503c \\(x_i^*\\). \u4f46\u662fthis would give the set of values that are individually the most probable.</p> <p>In practice, we typically wish to find the set of values that jointly have the largest probability, in other words the vector \\(x_{max}\\) that maximizes the joint distribution, so that </p> <p>\u4e5f\u5c31\u662f\u6211\u4eec\u8981\u627e\u4e00\u7ec4\u503c\uff0c\u8ba9joint \\(p(\\mathrm{x})\\)\u8fbe\u5230 max\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e00\u7ef4\u5ea6\u627e\u4e00\u4e2amax\u7136\u540e\u62fc\u8d77\u6765</p>"},{"location":"PRML/chap8/chap8/#find-the-maximum-of-the-joint-distribution-by-propagating-messages-from-the-leaves-to-an-arbitrarily-chosen-root-node","title":"find the maximum of the joint distribution (by propagating messages from the leaves to an arbitrarily chosen root node)","text":"<p>\u5047\u8bbe\u4e00\u5171\u6709M\u4e2a\u53d8\u91cf\uff0c\u53ef\u4ee5\u628amax\u5199\u6210\u5c55\u5f00\u7684\u5f62\u5f0f\uff1a </p> <p>and then substitute for \\(p(\\mathrm{x})\\) using its expansion in terms of a product of factors\u3002\u7136\u540e\u53ef\u4ee5\u5bf9\\(p(\\mathrm{x})\\)\u8fdb\u884c\u5206\u89e3</p> <p>\u5728max-product\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4e58\u6cd5\u5206\u914d\u5f8b\u4ea4\u6362\u4e86\u4e58\u6cd5\u4e0e\u6c42\u548c \u800c\u5728\u8fd9\u91ccmax-sum\u4e2d\uff0c\u6211\u4eec\u5229\u7528max\u7684\u6027\u8d28\u4ea4\u6362\u4e58\u6cd5\u4e0emax </p> <p>\u5148\u8003\u8651chain\u4e0a\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u662ffactor graph\uff0c\u53ef\u4ee5\u62c6\u5206  </p> <p>\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4ea4\u6362\u4e4b\u540e\u7684\u5f0f\u5b50is easily interpreted in terms of messages passed from node xN backwards along the chain to node x1. \uff08\u5728\u5f15\u5165factor graph\u4e4b\u524d\u7684\u90a3\u90e8\u5206\u8ba8\u8bba\u8fc7\uff0cchain\u7684message\u5c31\u662fpotential\uff09</p> <p>\u4e0b\u9762\u4ecechain\u63a8\u5e7f\u5230tree\uff0c\u628a  \u4ee3\u5165\u5230max\u7684\u6027\u8d28\u4e2d The structure of this calculation is identical to that of the sum-product algorithm, and so we can simply translate those results into the present context.</p> <p>In particular, suppose that we designate a particular variable node as the \u2018root\u2019 of the graph.  Then we start a set of messages propagating inwards from the leaves of the tree towards the root, with each node sending its message towards the root once it has received all incoming messages from its other neighbours.  The final maximization is performed over the product of all messages arriving at the root node, and gives the maximum value for \\(p(\\mathrm{x})\\)  \u89c2\u5bdfsum-product\u5f97\u5230\u7684\u5f0f\u5b50\uff0c\u8fd9\u4e2a\u5f0f\u5b50\u662f\u7531\u4e58\u6cd5\u5206\u914d\u5f8b\u5f97\u5230\u7684\u3002max-product\u7684\u63a8\u5bfc\u5728\u4e0b\u9762\u3002 This could be called the max-product algorithm and is identical to the sum-product algorithm except that summations are replaced by maximizations.</p> <p>Q\uff1a\u90a3\u8fd9\u6837\u5c82\u4e0d\u662f\u5e94\u8be5\u628a\u4e0a\u9762\u7684\u5f0f\u5b50\u7684\u6c42\u548c\u6362\u6210max\u4e48\uff0c\u4e0d\u5e94\u8be5\u662fmax-product\u4e48\uff0c\u4e3a\u4ec0\u4e48\u662fmax-sum\uff1f \u901a\u5e38\u5bf9p(x)\u6c42ln\uff0c\u9632\u6b62underflow\u3002In practice, products of many small probabilities can lead to numerical underflow problems, and so it is convenient to work with the logarithm of the joint distribution. \u56e0\u4e3alnx\u5355\u8c03\uff0c\u6240\u4ee5max\u548cln\u53ef\u4ee5\u4ea4\u6362  \u6b64\u65f6max\u7684\u6027\u8d28\u8fd8\u5728\uff0c\u53ea\u4e0d\u8fc7\u4ece\u4e58\u6cd5\u53d8\u6210\u52a0\u6cd5 </p> <p>Thus taking the logarithm simply has the effect of replacing the products in the max-product algorithm with sums, and so we obtain the max-sum algorithm</p> <p>\u7c7b\u6bd4sum-product\u7684\u7ed3\u8bba\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5199\u51famax-sum\u7684\u5f0f\u5b50\uff1a   while at the root node the maximum probability can then be computed, by analogy with (8.63), using </p> <p>\u7531\u6b64\u5c31\u5f97\u5230\u4e86joint\u7684max</p>"},{"location":"PRML/chap8/chap8/#finding-the-configuration-of-the-variables-for-which-the-joint-distribution-attains-this-maximum-value","title":"finding the configuration of the variables for which the joint distribution attains this maximum value","text":"<p>\u6211\u4eec\u60f3\u8981\u5f97\u5230\u4e0e\u4e0a\u9762\u7684\\(p^{max}\\)\u5bf9\u5e94\u7684\\(x^{max}\\)  At this point, we might be tempted simply to continue with the message passing algorithm and send messages from the root back out to the leaves, using (8.93) and (8.94), then apply (8.98) to all of the remaining variable nodes. However, because we are now maximizing rather than summing, it is possible that there may be multiple configurations of x all of which give rise to the maximum value for p(x).In such cases, this strategy can fail because it is possible for the individual variable values obtained by maximizing the product of messages at each node to belong to different maximizing configurations, giving an overall configuration that no longer corresponds to a maximum.</p> <p>The problem can be resolved by adopting a rather different kind of message passing from the root node to the leaves</p> <p>let us return once again to the simple chain example of N variables x1,...,xN each having K states Suppose we take node xN to be the root node. Then in the first phase, we propagate messages from the leaf node x1 to the root node using   \u7136\u540e\u5c31\u80fd\u5f97\u5230The most probable value for xN </p> <p>\u73b0\u5728\u6211\u4eec\u60f3\u5f97\u5230the states of the previous variables that correspond to the same maximizing configuration.</p> <p>This can be done by keeping track of which values of the variables gave rise to the maximum state of each variable, in other words by storing quantities given by </p> <p>\u4e0b\u9762\u6765\u89e3\u91ca\u4e00\u4e0b\u8fd9\u4e2a\u5f0f\u5b50\uff1a  Note that this is not a probabilistic graphical model because the nodes represent individual states of variables For each state of a given variable, there is a unique state of the previous variable that maximizes the probability  Once we know the most probable value of the final node xN, we can then simply follow the link back to find the most probable state of node xN\u22121 and so on back to the initial node x1</p> <p>This corresponds to propagating a message back down the chain using  and is known as back-tracking.</p> <p>\u5982\u679c\u6211\u4eec\u6b63\u5411max-sum\u4e4b\u540e\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u53d8\u91cf\u5957\u7528\u8fd9\u4e2a\u5f0f\u5b50\uff1a  \u6700\u540e\u9009\u51fa\u7684\u7ec4\u5408\u53ef\u80fd\u5206\u5e03\u5728(\u56fe8.53)\u7684\u51e0\u6761\u4e0d\u540c\u7684\u8def\u5f84\u4e2d\uff0c\u6700\u7ec8\u5c31\u5bfc\u81f4\u4e0d\u662fglobal maximum</p> <p>extension to a general tree-structured factor graph\uff1a \u6362\u53e5\u8bdd\u8bf4\uff0c\u5728\u6b63\u5411message\u4f20\u64ad\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u7b97\u8fd9\u4e2a\u4e1c\u897f\uff0c  \u5728\u5bf9M\u4e2a\u53d8\u91cf\u9010\u4e2amax\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8bb0\u4e0b\u6bcf\u4e2a\u53d8\u91cf\u6ee1\u8db3\u8fd9\u4e2a\u5f0f\u5b50\u7684\u503c  \u7136\u540e\u6211\u4eec\u5728\u6b63\u5411\u4f20\u64ad\u7ed3\u675f\u4e4b\u540e\u901a\u8fc7\u8fd9\u4e2a\u5f0f\u5b50\u5f97\u5230\u4e86\\(p^{max}\\)  \u6b64\u65f6\u6211\u4eec\u5b58\u4e0b\u6765\u7684\u90a3\u4e9b\u503c\u5c31\u81ea\u52a8\u5bf9\u5e94\\(p^{max}_1,...,p^{max}_M\\)</p> <p>An important application of this technique is for finding the most probable sequence of hidden states in a hidden Markov model, in which case it is known as the Viterbi algorithm</p>"},{"location":"PRML/chap8/chap8/#observation","title":"observation","text":"<p>The observed variables are clamped to their observed values, and the maximization is performed over the remaining hidden variables. This can be shown formally by including identity functions for the observed variables into the factor functions, as we did for the sum-product algorithm.</p>"},{"location":"PRML/chap8/chap8/#846-exact-inference-in-general-graphs","title":"8.4.6 Exact inference in general graphs","text":"<p>sum-product\u548cmax-sum\u9002\u7528\u4e8etree\uff0c\u7136\u800c\u5b9e\u8df5\u4e2dwe have to deal with graphs having loops \u5c06tree\u4e0a\u9762\u7684inference\u63a8\u5e7f\u5230\u4efb\u610f\u62d3\u6251\u7ed3\u6784\u7684\u7b97\u6cd5\u79f0\u4e3ajunction tree algorithm \u5927\u6982\u6b65\u9aa4\uff1a 1. \u628a\u6709\u5411\u56fe\u8f6c\u5316\u4e3a\u65e0\u5411\u56fe\uff0c\u65e0\u5411\u56fe\u4e0d\u7528\u53d8 2. Next the graph is triangulated, which involves finding chord-less cycles containing four or more nodes and adding extra links to eliminate such chord-less cycles          \u5728\u4e0a\u9762\u8fd9\u4e2a\u56fe\u4e2d\uff0cA-D-B-C-A\u4e3achordless cycle\uff0c\u6b64\u65f6\u8981\u5728AB\u6216CD\u4efb\u610f\u52a0\u4e00\u6761\u8fb9     \u6ce8\u610fpotential\u8fd8\u662f\u539f\u6765\u7684\u51fd\u6570\uff0c\u53ea\u4e0d\u8fc7\u662f\u6309\u65b0\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u89e3 3. \u7528\u8fd9\u4e2atriangulated undirected graph\u5efa\u7acba new tree-structured undirected graph called a join tree     join tree\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4ee3\u8868\u65e0\u5411\u56fe\u4e2d\u7684\u4e00\u4e2amaximal clique\uff0c\u6709\u516c\u5171\u53d8\u91cf\u7684maximal clique\u4e4b\u95f4\u6709link     * \u8fd9\u4e2a\u8fde\u63a5\u5404\u4e2aclique\u5f62\u6210\u6811\u7684\u8fc7\u7a0b\uff0c\u8981\u6ee1\u8db3\u4e00\u5b9a\u7684\u6761\u4ef6\uff0c\u4f7f\u5f97\u6700\u540e\u5f62\u6210\u7684\u6811\u662fmaximal spanning tree     * \u5176\u4e2dlink\u7684weight\u4e3a\u4e24\u4e2aclique\u5171\u6709\u53d8\u91cf\u7684\u4e2a\u6570\uff0ctree\u7684weight\u662f\u6811\u4e0a\u6240\u6709link\u7684weight\u4e4b\u548c     * If the tree is condensed, so that any clique that is a subset of another clique is absorbed into the larger clique, this gives a junction tree.      * As a consequence of the triangulation step, the resulting tree satisfies the running intersection property, which means that if a variable is contained in two cliques, then it must also be contained in every clique on the path that connects them. 4. a two-stage message passing algorithm, essentially equivalent to the sum-product algorithm, can now be applied to this junction tree in order to find marginals and conditionals</p> <p>\u867d\u7136\u524d\u9762\u5f88\u590d\u6742\uff0c\u4f46\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6838\u5fc3\u4ecd\u7136\u662f\u4ea4\u6362\u4e58\u6cd5\u548c\u52a0\u6cd5\uff0c\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u5148summation\u51fa\u5c40\u90e8\u7684message\uff0c\u518d\u628amessage\u4f20\u9012\u76f8\u4e58\u3002\u800c\u4e0d\u662f\u76f4\u63a5\u5bf9joint \u8fdb\u884csummation\u3002 at its heart is the simple idea that we have used already of exploiting the factorization properties of the distribution to allow sums and products to be interchanged so that partial summations can be performed, thereby avoiding having to work directly with the joint distribution</p> <p>\u5c40\u9650\u6027\uff1a Unfortunately, the algorithm must work with the joint distributions within each node (each of which corresponds to a clique of the triangulated graph) and so the computational cost of the algorithm is determined by the number of variables in the largest clique\uff0c\u5e76\u4e14\u4f1a\u6307\u6570\u589e\u957f</p>"},{"location":"PRML/chap8/chap8/#847-loopy-belief-propagation","title":"8.4.7 Loopy belief propagation","text":"<p>\u5728approximate inference\u4e2d\uff0c\u9700\u8981\u7528\u5230chapter10\u4e2d\u7684variational methods\u548cchapter11\u7684Monte Carlo methods\u3002 \u8fd9\u91cc\u5148\u7b80\u5355\u4ecb\u7ecd\u4e00\u79cd\u7528\u4e8egraph with loops \u7684approximate\u65b9\u6cd5 \u601d\u60f3\uff1a The idea is simply to apply the sum-product algorithm even though there is no guarantee that it will yield good results. \u56e0\u4e3agraph\u73b0\u5728\u6709cycles\u4e86\uff0cinformation can flow many times around the graph. For some models, the algorithm will converge, whereas for others it will not. (Mark\uff0c\u8fd9\u91cc\u8df3\u4e86\u4e00\u6bb5)</p>"},{"location":"PRML/chap8/chap8/#848-learning-the-graph-structure","title":"8.4.8 Learning the graph structure","text":"<p>\u4e4b\u524d\u6211\u4eec\u90fd\u662f\u5047\u8bbegraph\u662f\u5df2\u77e5\u7684\uff0c \u7406\u8bba\u4e0a\u6211\u4eec\u53ef\u4ee5\u4e3agraph\u7684\u7ed3\u6784\u8bbe\u4e00\u4e2aprior\uff0c\u7136\u540e\u5229\u7528posterior\u6765\u8fdb\u884c\u9884\u6d4b\uff1a  \u4e5f\u5c31\u662fmake predictions by averaging with respect to this distribution \u7136\u800c\u8fd9\u6837\u8ba1\u7b97\u91cf\u592a\u5927\uff0c\u4e00\u822c\u90fd\u4f1a\u91c7\u7528heuristics\u6765\u7b5b\u9009graph\u7684\u7ed3\u6784</p>"},{"location":"PRML/chap9/chap9/","title":"9. Mixture Models and EM","text":"<ul> <li>As well as providing a framework for building more complex probability distributions, mixture models can also be used to cluster data</li> <li>A general technique for finding maximum likelihood estimators in latent variable models is the expectation-maximization (EM) algorithm. </li> <li>\u5728\u5f88\u591a\u5e94\u7528\u4e2d\uff0cGaussian mixture models\u7684\u53c2\u6570\u90fd\u662f\u7528EM\u5f97\u5230\u7684\uff0c\u7136\u800cMLE\u662f\u6709\u7f3a\u9677\u7684\u3002chapter10\u4f1a\u4ecb\u7ecdVariational inference. Variational inference\u7684\u8ba1\u7b97\u91cf\u4e0d\u4f1a\u6bd4EM\u591a\u5f88\u591a\uff0c\u89e3\u51b3\u4e86MLE\u7684\u95ee\u9898\uff0c\u800c\u4e14allowing the number of components in the mixture to be inferred automatically from the data</li> </ul>"},{"location":"PRML/chap9/chap9/#91-k-means-clustering","title":"9.1. K-means Clustering","text":"<p>\u76ee\u6807\u662f\u628aN\u4e2aD\u7ef4\u7684observed data x\u5206\u4e3aK\u4e2acluster\uff0c\u8fd9\u91cc\u5047\u5b9aK\u5df2\u77e5\u3002</p> <p>\u6211\u4eec\u5f15\u5165K\u4e2aD\u7ef4\u5411\u91cf\\(\\mu_k\\)\u7528\u4e8e\u63cf\u8ff0\u6bcf\u4e2acluster\u7684\u4e2d\u5fc3 \u6211\u4eec\u60f3\u8981\u627e\u5230\u4e00\u79cd\u5bf9data\u7684\u5206\u914d\uff0c\u4f7f\u5f97the sum of the squares of the distances of each data point to its closest vector \\(\\mu_k\\),is a minimum</p> <p>\u5bf9\u6bcf\u4e00\u4e2ax\uff0c\u90fd\u7528\u4e00\u4e2aK\u7ef4onehot\u8868\u793a\u8fd9\u4e2adata\u6240\u5c5e\u7684cluster </p> <p>\u7136\u540e\u5c31\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2aobjective function\uff0c\u6216\u8005\u53ebdistortion measure\uff1a  Our goal is to find values for the \\(\\{r_{nk}\\}\\) and the \\(\\{\\mu_{k}\\}\\) so as to minimize J.</p> <p>\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2aiterative procedure\uff0c\u6bcf\u4e2aiteration\u5206\u4e3a\u4e24\u6b65\uff0c\u5206\u522b\u5bf9\\(r_{nk}\\) \u548c \\(\\mu_{k}\\)\u4f18\u5316</p> <p>First we choose some initial values for the \\(\\mu_{k}\\).  Then in the first phase we minimize J with respect to the \\(r_{nk}\\), keeping the \\(\\mu_{k}\\) fixed.  In the second phase we minimize J with respect to the \\(\\mu_{k}\\), keeping \\(r_{nk}\\) fixed.  This two-stage optimization is then repeated until convergence. </p> <p>\u4e4b\u540e\u6211\u4eec\u4f1a\u770b\u5230\u5bf9\\(r_{nk}\\)\u548c\\(\\mu_{k}\\)\u5206\u522b\u5bf9\u5e94E step\u548cM step</p> <p>Consider first the determination of the \\(r_{nk}\\) we simply assign the \\(n^{th}\\) data point to the closest cluster centre. More formally, this can be expressed as </p> <p>Now consider the optimization of the \\(\\mu_{k}\\) with the \\(r_{nk}\\) held fixed. \u76f4\u63a5\u5bf9\u03bc\u6c42\u5bfc\u53ef\u5f97  \\(\\mu_{k}\\) equal to the mean of all of the data points xn assigned to cluster k</p> <p>Because each phase reduces the value of the objective function J, convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of J.</p> <p>\u628asquared Euclidean distance\u63a8\u5e7f\u5230general dissimilarity\uff0c\u5c31\u5f97\u5230\u4e86K-medoids algorithm  \u4e3a\u4e86\u7b80\u5316\u6a21\u578b\uff0c\u4f1a\u628a\\(\\mu\\)\u8bbe\u4e3a\u6bcf\u4e2acluster\u4e2d\u7684\u67d0\u4e2adata</p> <p>\u6ce8\u610f\u5728K-Means\u4e2d\u662f\u786c\u5206\u7c7b\uff0c\u6bcf\u7b14data\u53ea\u80fd\u5206\u914d\u7ed9\u552f\u4e00\u7684\u4e00\u4e2acluster</p>"},{"location":"PRML/chap9/chap9/#92-mixtures-of-gaussians","title":"9.2. Mixtures of Gaussians","text":"<p>We now turn to a formulation of Gaussian mixtures in terms of discrete latent variables.</p> <p>Gaussian mixture distribution\u53ef\u4ee5\u5199\u6210linear superposition of Gaussians </p> <p>\u4e0b\u9762\u6211\u4eec\u4ecelatent variable\u7684\u89d2\u5ea6\u5f15\u51fa\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\uff1a \u5f15\u5165\u4e00\u4e2a1-of-K\u7684\u53d8\u91cf\\(\\mathrm{z}\\), in which a particular element zk is equal to 1 and all other elements are equal to 0</p> <p>\u5b9a\u4e49\u4e00\u4e2ax\u4e0ez\u7684joint\uff0cin terms of a marginal distribution p(z) and a conditional distribution p(x|z) \u8fd9\u6837\u5c31\u53ef\u4ee5\u5f97\u5230graph\uff1a </p> <p>\u7136\u540e\u6211\u4eec\u628az\u7684marginal\u7528\u4e00\u7ec4\u7cfb\u6570\\(\\pi_k\\)\u8868\u793a   \u6ce8\u610f\u8fd9\u91cc\\(\\pi_k\\)\u8981\u6ee1\u8db3\u6982\u7387\u7684\u6027\u8d28\uff08\u975e\u8d1f\uff0c\u548c\u4e3a\u4e00\uff09</p> <p>\u540c\u65f6\uff0c\u6211\u4eec\u628ax\u5728z=1\u7684conditional\u8bbe\u4e3agaussian\uff1a  \u5408\u8d77\u6765\u53ef\u4ee5\u5199\u6210 </p> <p>The joint distribution is given by p(z)p(x|z),  \u5728joint\u4e0a\u5bf9z\u8fdb\u884csummation\u53ef\u4ee5\u5f97\u5230marginal p(x): </p> <p>\u4e4d\u4e00\u770b\u5f15\u5165z\u597d\u50cf\u6ca1\u4ec0\u4e48\u610f\u4e49\uff0cHowever, we are now able to work with the joint distribution p(x, z) instead of the marginal distribution p(x), and this will lead to significant simplifications, most notably through the introduction of the expectation-maximization (EM) algorithm</p> <p>\u73b0\u5728\u518d\u5b9a\u4e49\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u91cf\uff1a the conditional probability of z given x, \\(\\gamma(z_k)\\) \\(\\gamma(z_k)\\)\u53ef\u4ee5\u76f4\u63a5\u7528bayes theorem\u6c42  We shall view \u03c0k as the prior probability of zk =1, and the quantity \u03b3(zk) as the corresponding posterior probability once we have observed x As we shall see later, \u03b3(zk) can also be viewed as the responsibility that component k takes for \u2018explaining\u2019 the observation x.</p> <p>\u5728mixture gaussian\u4e0agenerate random samples\u7684\u65b9\u6cd5\uff1a \u4e4b\u524d\u63d0\u5230\u7684ancestral sampling\uff1a \u5148\u5728marginal p(z)\u4e0a\u751f\u6210\u4e00\u4e2avalue of z\uff0c\u8bb0\u4f5c\\(\\hat{z}\\)\uff0c \u7136\u540e\u7528\u8fd9\u4e2a\\(\\hat{z}\\)\uff0c\u5728conditional\\(p(x|\\hat{z})\\)\u4e0agenerate\u4e00\u4e2ax</p> <p> \u6839\u636eprior \\(\\pi_k\\)\u53ef\u4ee5\u5f97\u5230\u56fea\uff0c\u6839\u636eposterior \\(\\gamma(z_k)\\)\u53ef\u4ee5\u5f97\u5230\u56fec</p>"},{"location":"PRML/chap9/chap9/#921-maximum-likelihood","title":"9.2.1 Maximum likelihood","text":"<p>Suppose we have a data set of observations {x1,..., xN}, and we wish to model this data using a mixture of Gaussians \u5bf9\u5e94\u7684latent variable\u53ef\u4ee5\u5199\u6210\u4e00\u4e2aNxK\u7684\u77e9\u9635Z  \u5047\u8bbei.i.d\uff0c\u5219\u53ef\u4ee5\u5199\u51falog likelihood\uff1a </p> <p>\u4e0b\u9762\u6765\u63a2\u8ba8\u5bf9mixture gaussian\u8fdb\u884cMLE\u6240\u5b58\u5728\u7684\u95ee\u9898\uff1a For simplicity, consider a Gaussian mixture whose components have covariance matrices given by \\(\\Sigma_k = \\sigma^2_k\\textbf{I}\\)</p> <p>Suppose that one of the components of the mixture model, let us say the jth component, has its mean \u00b5j exactly equal to one of the data points so that \u00b5j = xn for some value of n. This data point will then contribute a term in the likelihood function of the form</p> <p> \\(\\sigma_j\\to0\\)\u65f6\uff0c\u8fd9\u4e00\u70b9\u5bf9likelihood\u7684\u8d21\u732e\u8d8b\u4e8e\u65e0\u7a77\uff0clikelihood\u8d8b\u4e8e\u65e0\u7a77</p> <p>\u56e0\u6b64MLE\u4e0d\u9002\u7528\u4e8eMixture Gaussian\uff0c\u56e0\u4e3asuch singularities will always be present and will occur whenever one of the Gaussian components \u2018collapses\u2019 onto a specific data point.</p> <p>Recall that this problem did not arise in the case of a single Gaussian distribution. To understand the difference, note that if a single Gaussian collapses onto a data point it will contribute multiplicative factors to the likelihood function arising from the other data points and these factors will go to zero exponentially fast, giving an overall likelihood that goes to zero rather than infinity.  However, once we have (at least) two components in the mixture, one of the components can have a finite variance and therefore assign finite probability to all of the data points while the other component can shrink onto one specific data point and thereby contribute an ever increasing additive value to the log likelihood</p> <p>\u8fd8\u6709\u4e00\u4e2a\u95ee\u9898\u5c31\u662fidentifiability\u3002for any given (nondegenerate) point in the space of parameter values there will be a further K!\u22121 additional points all of which give rise to exactly the same distribution. \u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662fK\u7ec4\u53c2\u6570\u548cK\u4e2acomponent\uff0cK\u4e2a\u841d\u535cK\u4e2a\u5751\uff0c\u987a\u5e8f\u65e0\u6240\u8c13\u7ed3\u679c\uff0c\u4f46\u662f\u5374\u5bf9\u5e94\u7740K\uff01\u4e2asolution</p> <p>Mixture gaussian\u6bd4\u5355\u4e2agaussian\u66f4\u96be\u8ba1\u7b97\u7684\u539f\u56e0\uff0c\u5176\u5b9e\u5c31\u662f(9.14)\u7684log likelihood\u4e2d\uff0cln\u5185\u6709\u6c42\u548c\uff0c\u8fd9\u4e2a\u662f\u4e0d\u80fd\u505a\u7b80\u5316\u7684\u3002\u56e0\u6b64\u6c42\u5bfc\u4e4b\u540e\u4e0d\u80fd\u76f4\u63a5\u5f97\u5230\u89e3</p>"},{"location":"PRML/chap9/chap9/#922-em-for-gaussian-mixtures","title":"9.2.2 EM for Gaussian mixtures","text":"<p>An elegant and powerful method for finding maximum likelihood solutions for models with latent variables is called the expectation-maximization algorithm, or EM algorithm </p> <p>\u5c06log likelihood\u5bf9\\(\\mu_k\\)\u6c42\u5bfc   \u53ef\u4ee5\u770b\u5230\u53f3\u8fb9\u7684\u5176\u4e2d\u4e00\u9879\u5c31\u662f\\(\\gamma(z_{nk})\\)\uff0c\u4e5f\u5c31\u662f\\(p(z_k=1|x)\\)\u8fd9\u4e2a\u540e\u9a8c</p> <p>multiplying by \\(\\Sigma_k\\)(which we assume to be nonsingular)\u53ef\u4ee5\u5f97\u5230  We can interpret \\(N_k\\) as the effective number of points assigned to cluster k.</p> <p>\u53ef\u4ee5\u770b\u5230\uff0c\\(\\mu_k\\)\u662f\u7b2ck\u4e2agaussian\u7684data\u7684weighted sum\uff0cweight\u5219\u662fposterior probability \u03b3(znk) that component k was responsible for generating xn</p> <p>\u63a5\u4e0b\u6765\u7528\u76f8\u540c\u7684\u601d\u8def\u5bf9\\(Sigma_k\\)\u6c42\u5bfc\uff0cmaking use of the result for the maximum likelihood solution for the covariance matrix of a single Gaussian, we obtain   again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component.</p> <p>\u6700\u540e\u6211\u4eec\u8981\u5bf9\\(\\pi_k\\)\u505aMLE\uff0c\u8981\u7528\u5230Lagrange\uff0c\u56e0\u4e3a\u5bf9\u4e8e\\(\\pi_k\\)\u662f\u6709\u7ea6\u675f\u7684\uff1a </p> <p></p> <p>\u56e0\u6b64\u7b2ck\u4e2agaussian\u7684\u7cfb\u6570\u5c31\u662fthe average responsibility which that component takes for explaining the data points</p> <p>\u9700\u8981\u5f3a\u8c03\u7684\u662f\uff0c\u4e0a\u9762\u7684\u4e09\u4e2a\u5f0f\u5b50\u5e76\u4e0d\u662fmodel\u7684\u4e00\u4e2aclosed-form solution\uff0c \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u7528iterative scheme\u6765\u6c42\u89e3\uff0c 1. \u9996\u5148\u9009\u5b9ameans, covariances, and mixing coefficients\u7684\u521d\u59cb\u503c 2. alternate between the following two updates that we shall call the E step and the M step     1. In the expectation step, or E step, we use the current values for the parameters to evaluate the posterior probabilities, or responsibilities, given by (9.13)          2. We then use these probabilities in the maximization step, or M step, to re-estimate the means, covariances, and mixing coefficients using the results (9.17), (9.19), and (9.22)</p> <p>\u6ce8\u610f\u5728M step\u4e2d\u6211\u4eec\u5148\u6c42\\(\\mu\\)\uff0c\u518d\u7528\u8fd9\u4e2a\\(\\mu\\)\u6c42\\(\\Sigma\\)</p> <p>EM\u7684iteration\u6570\u4e0e\u6bcf\u4e2a\u5faa\u73af\u4e2d\u7684\u8ba1\u7b97\u91cf\u90fd\u8fdc\u5927\u4e8eKMeans\uff0c\u56e0\u6b64\u5e38\u5e38\u5148\u7528Kmeans\u627e\u521d\u59cb\u503c\uff0c\u518d\u7528gaussian mixture\u8fdb\u4e00\u6b65\u6c42\uff1aThe covariance matrices can conveniently be initialized to the sample covariances of the clusters found by the K-means algorithm, and the mixing coefficients can be set to the fractions of data points assigned to the respective clusters.  \u5f3a\u8c03\uff1alog likelihood function\u6709\u591a\u4e2alocal maxima, EM\u5e76\u4e0d\u4fdd\u8bc1\u627e\u5230global maxima</p>"},{"location":"PRML/chap9/chap9/#93-an-alternative-view-of-em","title":"9.3. An Alternative View of EM","text":"<p>The goal of the EM algorithm is to find maximum likelihood solutions for models having latent variables</p> <p>We denote the set of all observed data by \\(\\textbf{X}\\), in which the \\(n^{th}\\) row represents \\(\\textbf{x}^T_n\\), and similarly we denote the set of all latent variables by \\(\\textbf{Z}\\), with a corresponding row \\(\\textbf{z}^T_n\\). The set of all model parameters is denoted by \\(\\theta\\), and so the log likelihood function is given by  \u5bf9\u4e8econtinuous latent variables\uff0c\u53ea\u9700\u8981\u628a\u6c42\u548c\u6362\u6210\u79ef\u5206</p> <p>\u6ce8\u610f\u5230summation\u51fa\u73b0\u5728ln\u5185\uff0cEven if the joint distribution p(X, Z|\u03b8) belongs to the exponential family, the marginal distribution p(X|\u03b8) typically does not as a result of this summation. \u6c42\u548c\u4f7f\u5f97ln\u4e0d\u80fd\u76f4\u63a5\u4f5c\u7528\u4e8ejoint\uff0c\u5bfc\u81f4MLE\u7684\u89e3\u5f88\u590d\u6742</p> <p>\u5047\u8bbe\u5bf9\u4e8e\u6bcf\u4e2aobservation in X\uff0c\u6211\u4eec\u90fd\u77e5\u9053\u5bf9\u5e94\u7684Z\uff0cWe shall call {X, Z} the complete data set, and we shall refer to the actual observed data X as incomplete\u3002  \uff08a\u662fcomplete\u7684\uff0cb\u662fincomplete\u7684\uff09 \u6211\u4eec\u53ef\u77e5\uff0c\u5bf9\u4e8ecomplete data set\u7684log likelihood\u5c31\u662f\u7b80\u5355\u7684ln p(X,Z|\u03b8), \u6211\u4eec\u5047\u8bbe\u8fd9\u4e2alog likelihood\u7684maximization\u662f\u597d\u6c42\u7684\u3002</p> <p>\u7136\u800c\u5728\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709complete data set\uff0c\u53ea\u6709incomplete data set X Our state of knowledge of the values of the latent variables in Z is given only by the posterior distribution p(Z|X, \u03b8)</p> <p>Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable, which corresponds (as we shall see) to the E step of the EM algorithm.  \u4e0a\u9762\u8fd9\u53e5\u5f88\u5173\u952e\uff0c\u6211\u4eec\u662f\u5728\u8868\u793acomplete-data log likelihood\u7684\u671f\u671b\uff0c\u5176\u4e2d\u6982\u7387\u662fposterior p(Z|X, \u03b8)\uff0cvalue\u662f ln p(X, Z|\u03b8)\u3002\u4e5f\u5c31\u662f\u7528incomplete\u7684X\u7684log likelihood\u5728p(Z|X, \u03b8)\u4e0a\u7684\u6982\u7387\u6c42\u671f\u671b\u3002\u8fd9\u4e2a\u671f\u671b\u4e0d\u662fX\u7684\u671f\u671b\uff0c\u800c\u662f\u4eba\u4e3a\u5b9a\u4e49\u7684\u4e00\u4e2a\u503c\uff0c\u662f\u5bf9complete-data log likelihood\u7684\u8fd1\u4f3c\u3002\u800c\u8fd9\u8ba1\u7b97\u8fd9\u4e2a\u671f\u671b\u7684\u6b65\u9aa4\u5bf9\u5e94E-step\uff0c\u800c\u6c42\u51fa\u4f7f\u8fd9\u4e2a\u671f\u671b\u6700\u5927\u7684\u53c2\u6570\u7684\u8fc7\u7a0b\u5bf9\u5e94M-step\u3002 \u5982\u679c\u5f53\u524d\u53c2\u6570\u5bf9\u5e94\\(\\theta^{old}\\)\uff0c\u90a3\u4e48\u5148\u540e\u7ecf\u5386\u4e86\u4e00\u4e2aEstep\u548cMstep\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\\(\\theta^{new}\\)</p> <p>The use of the expectation may seem somewhat arbitrary. However, we shall see the motivation for this choice when we give a deeper treatment of EM in Section 9.4.</p> <p>\u603b\u7ed3\uff1a * In the E step, we use the current parameter values \\(\\theta^{old}\\) to find the posterior distribution of the latent variables given by \\(p(Z|X, \\theta^{old})\\).     We then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value \\(\\theta\\). This expectation, denoted \\(\\mathcal{Q} (\\theta,\\theta^{old})\\), isgiven by      * In the M step, we determine the revised parameter estimate \\(\\theta^{new}\\) by maximizing this function     </p> <p>Note: \u6ce8\u610f\u73b0\u5728\u5728\\(\\mathcal{Q} (\\theta,\\theta^{old})\\) \u4e2d\uff0cln\u76f4\u63a5\u4f5c\u7528\u5728joint\u4e0a\uff0cln\u4e2d\u6ca1\u6709summation\u4e86\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba1\u7b97\u4e86</p> <p> </p> <p>The EM algorithm can also be used to find MAP (maximum posterior) solutions for models in which a prior p(\u03b8) is defined over the parameters.  In this case the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by \\(\\mathcal{Q} (\\theta,\\theta^{old}) + ln p(\\theta)\\).</p>"},{"location":"PRML/chap9/chap9/#931-gaussian-mixtures-revisited","title":"9.3.1 Gaussian mixtures revisited","text":"<p>EM\u7684motivation\uff1a(incomplete\u4e0d\u77e5\u9053Z\u5bfc\u81f4\u4e86ln(summation), \u6ca1\u6cd5\u7b97. \u800c\u4e0b\u9762\u6765\u5c55\u793a\u5982\u679c\u662fcomplete\u7684\u8bdd\uff0c\u5c31\u5f88\u597d\u7b97) \u73b0\u5728\u5047\u8bbe\u6211\u4eec\u4e0d\u5149\u77e5\u9053X\uff0c\u8fd8\u77e5\u9053Z\uff0c\u4e5f\u5c31\u662f\u77e5\u9053\u662f\u54ea\u4e2acomponent\u4ea7\u751f\u4e86X\u3002\uff08\u6ce8\u610f\u8fd9\u4e2aZ\u662fhard\u7684onehot\uff0c\u4e0eposterior\u7684\\(\\gamma(z_{nk})\\)\u533a\u5206\u5f00\uff09  \u6b64\u65f6likelihood\u4e3a\uff1a  \u6b64\u65f6\u53ef\u4ee5\u770b\u5230\uff0cln\u5185\u90e8\u6ca1\u6709summation\uff0c\u800cln\u91cc\u7684gaussian\u672c\u8eab\u5c31\u662fexponential family\u4e2d\u7684\uff0c\u56e0\u6b64\u5f88\u597d\u7b97</p> <p>\uff08\u4e0b\u9762\u4ece\u9690\u53d8\u91cf\u7684\u89d2\u5ea6\u628aGMM\u518d\u63a8\u4e00\u4e0b\uff09 \u7136\u800c\u5b9e\u9645\u4e2d\u5e76\u6ca1\u6709\u9690\u53d8\u91cf\u7684\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u628a\u6570\u636eX\u5728Z\u7684\u540e\u9a8c\u4e0a\u7684\u671f\u671b\u770b\u4f5c\u662fcomplete-data log likelihood</p> <p>  \u518d\u52a0\u4e0abayes\u5b9a\u7406\u53ef\u4ee5\u5f97\u5230  </p> <p>\u4e0e\u4e4b\u524d\u7684\u786c\u7b97p(X)  \u76f8\u6bd4\uff0c\u53ef\u4ee5\u770b\u5230ln\u4e0e\u6c42\u548c\u4ea4\u6362\u4e86\u4f4d\u7f6e\uff0c\u56e0\u6b64\u597d\u7b97</p> <ul> <li>the maximization with respect to the means and covariances:     \u56e0\u4e3a\\(z_{nk}\\)\u662fonehot\uff0c\u56e0\u6b64(9.36)\u53ea\u662fK\u4e2aindependent\u7684gaussian\u76f8\u52a0\uff0c\u56e0\u6b64\u7ed3\u679c\u5c31\u662fK\u4e2agaussian\u5404\u81ea\u7b97\u5404\u81ea\u7684\u53c2\u6570</li> <li>the maximization with respect to the mixing coefficients\uff1a     \u56e0\u4e3a\u6709sum\u4e3a1\u7684\u7ea6\u675f\uff0c\u56e0\u6b64\u8fd8\u662f\u8981\u7528lagrange\uff0c\u5f97\u5230          \u56e0\u6b64mixing coefficients are equal to the fractions of data points assigned to the corresponding components</li> </ul> <p>\u90a3\u4e48\u95ee\u9898\u6765\u4e86\uff0c\\(z_{nk}\\)\u6211\u4eec\u4e0d\u77e5\u9053\uff0c\u56e0\u6b64\u548c\u524d\u9762\u4e00\u6837\uff0c\u6211\u4eec\u7528bayes\uff1a\uff08\u5176\u5b9e\u5f0f\u5b50\u4e5f\u548c\u524d\u9762(9.35)\u662f\u4e00\u4e2a\u9053\u7406\uff0c\u56e0\u4e3aposterior\u6b63\u6bd4\u4e8ejoint\uff09  \u89c2\u5bdf\u8fd9\u4e2a\u5f0f\u5b50\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u3002N\u4e2a\\(z_n\\)\u662findependent\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5   (Mark\uff0c\u4e0a\u9762\u8fd9\u4e2a\u6c42\u671f\u671b\u6ca1\u548b\u770b\u61c2) The expected value of the complete-data log likelihood function is therefore given by </p>"},{"location":"PRML/chap9/chap9/#932-relation-to-k-means","title":"9.3.2 Relation to K-means","text":"<p>Kmeans\u662fhard\u7684\uff0cGMM\u662fsoft\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u628akmeans\u770b\u4f5c\u662fGMM\u7684\u4e00\u4e2alimit</p> <p>\u5047\u8bbeGMM\u7684\u6bcf\u4e2acomponent\u7684gaussian\uff0ccovariance  matrix\u90fd\u662f\\(\\epsilon I\\), where \\(\\epsilon\\) is a variance parameter that is shared by all of the components  \u6211\u4eec\u5047\u8bbe\uff0c\\(\\epsilon\\)\u4e0d\u9700\u8981\u5728EM\u4e2d\u6c42\u89e3\uff0c\u7136\u540e\u6211\u4eec\u4ee3\u5165EM-GMM\u7684\u7ed3\u8bba\uff1a \u53ef\u4ee5\u5f97\u5230\u5f53\\(\\epsilon \\to 0\\)\u65f6\uff0cem\u7684\u89e3\u5c31\u662fkmeans\u7684\u89e3</p>"},{"location":"PRML/chap9/chap9/#94-the-em-algorithm-in-general","title":"9.4. The EM Algorithm in General","text":"<p>(proof that the EM algorithm indeed maximize the likelihood function)</p> <p>\u6211\u4eec\u7684\u76ee\u6807\u662fmaximize </p> <p>\u5047\u8bbep(X|\u03b8)\u96be\u7b97\uff0c\u4f46\u662fcomplete-data likelihood function p(X, Z|\u03b8)\u5f88\u7b80\u5355 \u6211\u4eec\u5f15\u5165q(Z)\u4f5c\u4e3aZ\u7684\u5206\u5e03\uff0c\u7136\u540e\u53ef\u4ee5\u770b\u5230\u5bf9\u4e8e\u4efb\u610f\u7684q(Z)\uff0c\u90fd\u6709  \u56e0\u4e3a\u6709\uff1a </p> <p>\u8fd9\u91cc\u7684KL\u6563\u5ea6\u53ef\u4ee5\u7406\u89e3\u4e3a\uff1a\u7528p(Z|X,\u03b8)\u8fd1\u4f3cq(Z)\u5e26\u6765\u7684\u71b5\u589e L(q,\u03b8)\u5219\u662f\u4e00\u4e2aq\u7684functional\uff0c\u4e5f\u662f\u4e00\u4e2a\u03b8\u7684function</p> <p>\u56e0\u4e3aKL(q||p)&gt;=0,\u6240\u4ee5L(q, \u03b8)&lt;=ln p(X|\u03b8) \u4e5f\u5c31\u662f\u8bf4L(q, \u03b8)\u662fln p(X|\u03b8)\u7684lower bound  \u5728E-step\u4e2d\uff0c\u6211\u4eec\u56fa\u5b9a\u4f4f\\(\\theta^{old}\\)\uff0c\u5173\u4e8eq(Z)maxmize L(q, \u03b8).\u56e0\u4e3ap(X|\u03b8)\u548cq(Z)\u6ca1\u5173\u7cfb\uff0c\u6240\u4ee5L(q, \u03b8)\u80fd\u4e00\u76f4\u53d6\u5230\u4e0a\u9650\uff0c\u4e5f\u5c31\u662fKL\u6563\u5ea6\u4e3a0\uff0cq(Z)=p(Z|X,\u03b8)\u7684\u60c5\u51b5\u3002In this case, the lower bound will equal the log likelihood </p> <p>\u5728M-step\u4e2d\uff0c\u6211\u4eec\u56fa\u5b9a\u4f4fq(Z)\uff0c\u5173\u4e8e\u03b8\u5bf9L(q, \u03b8)\u6700\u5927\u5316\uff0c\u4ece\u800c\u5f97\u5230\\(\\theta^{new}\\) \u6b64\u65f6lower bound L\u4f1a\u53d8\u5927\uff0c\u4e5f\u5c31\u5bfc\u81f4\u6574\u4e2alog likelihood\u53d8\u5927 \u6ce8\u610f\u6b64\u65f6q(Z)\u4e0d\u518d\u7b49\u4e8ep(Z|X,\u03b8)\uff0c\u56e0\u4e3a\u03b8\u5df2\u7ecf\u66f4\u65b0\u4e86\uff0c\u6240\u4ee5KL\u5927\u4e8e0(\u8fd9\u91cc\u8ba8\u8bba\u7684\u90fd\u662f\u8fd8\u6ca1\u6709\u6536\u655b\u7684\u60c5\u51b5)\u3002The increase in the log likelihood function is therefore greater than the increase in the lower bound  \u6211\u4eec\u53ef\u4ee5\u628a\\(q(Z)=p(Z|X,\\theta^{old})\\)\u4ee3\u5165L\u7684\u5b9a\u4e49\u5f0f\uff0c\u5f97\u5230  (\u56e0\u4e3a\u4e4b\u524d\u5728GMM\u4e2d\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u8fc7\u8fd1\u4f3c\u7684complete log likelihood\u4e3a)  \u518d\u89c2\u5bdf\uff0c\u5728\u4e0a\u9762M-step\u4e2d\uff0c\u03b8\u53ea\u51fa\u73b0\u5728ln\u5185\u90e8\uff0c\u56e0\u6b64\u5728joint p(X, Z|\u03b8)\u4e3aexponential family\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u5c31\u4f1a\u6bd4\u76f4\u63a5\u7b97p(X|\u03b8)\u7b80\u5355</p> <p>\u7528EM\u505aMAP\uff1ap(\u03b8|X)\uff0c\u5176\u4e2d\u8bbe\u5b9aprior p(\u03b8) \u548c\u4e4b\u524d\u4e00\u6837\uff0c\u6211\u4eec\u5148\u628a\u6211\u4eec\u7684\u76ee\u6807\u8f6c\u6362\u6210joint </p> <p>\u7136\u540e\u548c\u4e4b\u524d\u4e00\u6837\uff0c\u6211\u4eec\u540c\u6837\u628aKL\u6563\u5ea6\u62c6\u51fa\u6765\uff0c\u56e0\u4e3a\u6211\u4eec\u5df2\u77e5\u5b83\u662f\u975e\u8d1f\u7684\uff1a  where ln p(X) is a constant. \u6b64\u65f6\u5bf9\u8fd9\u4e2a\u5f0f\u5b50\u8fdb\u884cEM\uff0c \u5728E-step\u4e2d\uff0c\u5bf9q\u4f18\u5316\u7684\u5f0f\u5b50\u4e0e\u4e4b\u524d\u4e00\u6837 \u5728M-step\u4e2d\uff0c\u5bf9\u03b8\u4f18\u5316\u7684\u8fc7\u7a0b\u4e0d\u53d8\uff0c\u53ea\u662f\u5728\u5f0f\u5b50\u4e2d\u591a\u4e86\u4e00\u9879lnp(\u03b8)</p>"}]}